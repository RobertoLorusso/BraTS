{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub1h-Y7DIpYf"
      },
      "source": [
        "## README ❗\n",
        "\n",
        "Set a manual_seed for reproducibility.\n",
        "\n",
        "References:\n",
        "\n",
        "- See [Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwFJcAuef07T"
      },
      "source": [
        "## Environment setup 🏛\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45BU03HjgAgT",
        "outputId": "c17b5dbc-41e8-4d31-81d6-759ce381dfb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: monai in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from monai) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->monai) (1.3.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement shutil (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for shutil\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.0.post0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Looking in links: https://data.dgl.ai/wheels/cu121/repo.html\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.10/dist-packages (2.0.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.11.17)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchdata>=0.5.0->dgl) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Looking in links: https://data.dgl.ai/wheels-test/repo.html\n",
            "Requirement already satisfied: dglgo in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: typer>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (0.9.0)\n",
            "Requirement already satisfied: isort>=5.10.1 in /usr/local/lib/python3.10/dist-packages (from dglgo) (5.13.2)\n",
            "Requirement already satisfied: autopep8>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (2.0.4)\n",
            "Requirement already satisfied: numpydoc>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.10.14)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.20 in /usr/local/lib/python3.10/dist-packages (from dglgo) (0.18.5)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from dglgo) (6.0.1)\n",
            "Requirement already satisfied: ogb>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.3.6)\n",
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.10/dist-packages (from dglgo) (2022.9.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.2.2)\n",
            "Requirement already satisfied: pycodestyle>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from autopep8>=1.6.0->dglgo) (2.11.1)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from autopep8>=1.6.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: sphinx>=5 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (5.0.2)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (3.1.3)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (0.9.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (4.66.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (2.0.7)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (0.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.0->dglgo) (4.5.0)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml>=0.17.20->dglgo) (0.2.8)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (3.2.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.4.0->dglgo) (8.1.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi->dglgo) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.1.4)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (67.7.2)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (0.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2023.3.post1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.7)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.16.1)\n",
            "Requirement already satisfied: docutils<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (0.18.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb>=1.3.3->dglgo) (1.3.0)\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "! pip install python-dotenv\n",
        "! pip install monai\n",
        "! pip install shutil\n",
        "! pip install mlflow --quiet\n",
        "! pip install pyngrok --quiet\n",
        "! pip install torchmetrics\n",
        "#! pip install dgl\n",
        "! pip install  dgl -f https://data.dgl.ai/wheels/cu121/repo.html\n",
        "! pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "sys.path.append('/content/drive/MyDrive/Lorusso/BraTS/')\n",
        "\n",
        "from src.preprocess import evaluation\n",
        "from src.preprocess.image_processing import *\n",
        "from src.preprocess.nifti_io import *\n",
        "from src.preprocess.graphgen import *\n",
        "from src.preprocess.graph_io import *\n",
        "\n",
        "from mlflow.models.signature import infer_signature\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from pyngrok import ngrok\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dotenv import load_dotenv\n",
        "import concurrent.futures\n",
        "import tarfile\n",
        "import nibabel as nib\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "from distutils.dir_util import copy_tree\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import SequentialSampler\n",
        "from torchvision import utils\n",
        "\n",
        "from monai.networks.nets import AutoEncoder\n",
        "from monai.losses import DiceCELoss, DiceFocalLoss, DiceLoss, FocalLoss\n",
        "\n",
        "from dgl import from_networkx as to_dgl_graph\n",
        "from dgl import batch as dgl_batch\n",
        "\n",
        "#torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MkZwkHg4Wn_e"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "if [ ! -f '/content/drive/MyDrive/Lorusso/BraTS/.env' ]; then\n",
        "    echo \"Creating .env file...\"\n",
        "    echo \"INPUT_DATA_DIR='/content/drive/MyDrive/Lorusso/BraTS/data/raw/brats'\" > '/content/drive/MyDrive/Lorusso/BraTS/.env'\n",
        "    echo \"PROCESSED_DATA_DIR='/content/drive/MyDrive/Lorusso/BraTS/data/processed'\" >> '/content/drive/MyDrive/Lorusso/BraTS/.env'\n",
        "    echo \"LOG_PATH='/content/drive/MyDrive/Lorusso/BraTS/logs'\" >> '/content/drive/MyDrive/Lorusso/BraTS/.env'\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0u_tX1q-hh-L"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "if [ ! -f '/content/drive/MyDrive/Lorusso/BraTS/data/raw/BraTS2021_Training_Data.tar'  ]; then\n",
        "    echo \"Downloading BraTS dataset at /content/drive/MyDrive/Lorusso/BraTS/data/raw ...\"\n",
        "    mkdir /root/.kaggle/\n",
        "    cp '/content/drive/MyDrive/Lorusso/kaggle.json' /root/.kaggle\n",
        "    chmod 600 '/root/.kaggle/kaggle.json'\n",
        "    cd '/content/drive/MyDrive/Lorusso/BraTS/data/raw' && kaggle datasets download -d dschettler8845/brats-2021-task1\n",
        "    ls '/content/drive/MyDrive/Lorusso/BraTS/data/raw'\n",
        "    unzip '/content/drive/MyDrive/Lorusso/BraTS/data/raw/brats-2021-task1.zip' -d '/content/drive/MyDrive/Lorusso/BraTS/data/raw/'\n",
        "    rm -r '/content/drive/MyDrive/Lorusso/BraTS/data/raw/brats-2021-task1.zip'\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2dm-FkQK1izU"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "path='/content/drive/MyDrive/Lorusso/BraTS/data/raw/brats'\n",
        "dst='/content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled/'\n",
        "\n",
        "\n",
        "if [ ! -d '/content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled' ]; then\n",
        "\n",
        "    mkdir '/content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled'\n",
        "\n",
        "    rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/train\n",
        "    rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/test\n",
        "    rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/val\n",
        "\n",
        "    for el in $(ls $path | head -n 24);\n",
        "        do\n",
        "            echo \"$path/$el -> $dst\"\n",
        "            cp -R \"$path/$el\" $dst\n",
        "        done\n",
        "fi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE7OwGBES7sa"
      },
      "source": [
        "## MLFlow server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4-kysoZS-i4",
        "outputId": "24ab5ef3-4082-4ad4-b614-29ed452eeb75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow Tracking UI: https://69d3-34-82-82-241.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "mlflow.set_tracking_uri('file:///content/drive/MyDrive/Lorusso/BraTS/mlruns')\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"2Yliuv8VnNyKNcljxgEv6NpZgz8_6ZDBYmEcebUeoX93eGJAE\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)\n",
        "\n",
        "# run tracking UI in the background\n",
        "get_ipython().system_raw(\"mlflow ui --backend-store-uri file:///content/drive/MyDrive/Lorusso/BraTS/mlruns --port 5000 & \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7ygOSsiEAWs"
      },
      "source": [
        "## Utils 🛠"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Lrs_wCJEAJpS"
      },
      "outputs": [],
      "source": [
        "class Logger:\n",
        "    def __init__(self,filename):\n",
        "        self.filename = filename\n",
        "        try:  #try to open file in 'r' (read) mode, if file does not exists the statement will throw an IOexception\n",
        "            log_file = open(self.filename, \"r\")\n",
        "            log_file.close()\n",
        "        except Exception as e: #catch the Exception raised from the block above and create the missing file in the specified path\n",
        "            log_file = open(self.filename, \"w\")\n",
        "            log_file.close()\n",
        "\n",
        "    def log_msg(self,*args):\n",
        "        try:\n",
        "            with open(self.filename, \"a+\") as log_file:\n",
        "                for el in args:\n",
        "                    if(type(el) is list or type(el) is tuple):\n",
        "                        for subel in el:\n",
        "                            log_file.write(subel)\n",
        "                            log_file.write('\\n')\n",
        "                    else:\n",
        "                        log_file.write(el)\n",
        "                        log_file.write('\\n')\n",
        "        except Exception as e:\n",
        "            print('Exception in Logger.log_msg')\n",
        "            print(e)\n",
        "\n",
        "    def read_msg(self):\n",
        "        content = []\n",
        "        try:\n",
        "            with open(self.filename) as file:\n",
        "                for el in file:\n",
        "                    content.append(el)\n",
        "        except Exception as e:\n",
        "            print('Exception in Logger.read_msg')\n",
        "            print(e)\n",
        "        finally:\n",
        "            return content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def untar_brats(tar_path, extract_path):\n",
        "    tar = tarfile.open(tar_path)\n",
        "    tar.extractall(extract_path)\n",
        "    tar.close()\n",
        "\n",
        "\n",
        "def seg_to_challenge( src = '/content/drive/MyDrive/Lorusso/BraTS/data/processed_15000_0.5_0/BraTS2023-ValidationData',\n",
        "                     dst =  '/content/drive/MyDrive/Lorusso/BraTS/data/segmentation/BraTS2023-ValidationData/'):\n",
        "    for mri_id in os.listdir(src):\n",
        "        for el in os.listdir(os.path.join(src, mri_id)):\n",
        "            if(not (el.endswith('_input.nii.gz') or el.endswith('.json') or el.endswith('_supervoxels.nii.gz') or el.endswith('.pkl')) ):\n",
        "                shutil.copy2(os.path.join(src,mri_id,el), dst)\n",
        "                os.rename(os.path.join(dst,el), f\"{dst}{os.sep}{mri_id}.nii.gz\")\n",
        "\n",
        "def rm_run(experiment_name,run_id):\n",
        "    current_experiment=dict(mlflow.get_experiment_by_name(experiment_name))\n",
        "    experiment_id = current_experiment['experiment_id']\n",
        "    for el in run_id:\n",
        "        rm_path = '/content/drive/MyDrive/Lorusso/BraTS/mlruns/'+experiment_id+'/'+el\n",
        "        shutil.rmtree(rm_path)\n",
        "\n",
        "\n",
        "def plot_reconstruction(im_orig, im_rec, ax:int = 0, slice_index:int = 100):\n",
        "\n",
        "    f, ax_array = plt.subplots(1,2, figsize=(10,10))\n",
        "    ax_array[0].imshow(np.take(im_orig, indices = slice_index, axis = ax), cmap='gray')\n",
        "    ax_array[1].imshow(np.take( im_rec , indices=slice_index, axis = ax), cmap='gray')\n",
        "\n",
        "def plot_brain_sections(img,labels, ax = 1,slice_index = 90):\n",
        "\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    assert(img.shape[:3] == labels.shape)\n",
        "    img = img.T\n",
        "    labels = labels.T\n",
        "    d1,d2,d3 = np.shape(lab)\n",
        "    dims = [d1,d2,d3]\n",
        "    dims.pop(ax)\n",
        "    color_segmentation = np.zeros((dims[0],dims[1],3))\n",
        "\n",
        "    gray_segmentation = np.take(img,slice_index,axis = ax)\n",
        "    color_segmentation[gray_segmentation == 1] = [255,0,0] # Red (necrotic tumor core)\n",
        "    color_segmentation[gray_segmentation == 2] = [0,255,0] # Green (peritumoral edematous/invaded tissue)\n",
        "    color_segmentation[gray_segmentation == 3] = [0,0,255] # Blue (enhancing tumor)\n",
        "\n",
        "    t1 = img[0]\n",
        "    flair = img[1]\n",
        "    t2 = img[2]\n",
        "    t1ce = img[3]\n",
        "\n",
        "    image = t1+t2+flair+t1ce\n",
        "\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.subplot(1,4,1)\n",
        "    plt.imshow(np.take(image,slice_index,axis = 0),cmap='gray')\n",
        "\n",
        "    plt.subplot(1,4,2)\n",
        "    plt.imshow(np.take(image,slice_index,axis = 1),cmap='gray')\n",
        "\n",
        "    plt.subplot(1,4,3)\n",
        "    plt.imshow(np.take(image,slice_index,axis = 2),cmap='gray')\n",
        "\n",
        "    plt.subplot(1,4,4)\n",
        "    plt.imshow(color_segmentation,cmap='gray')\n",
        "    plt.xlabel('Segmentation')\n",
        "\n",
        "\n",
        "def visTensor(tensor, ch=0, allkernels=False, nrow=8, padding=1):\n",
        "    n,c,w,h = tensor.shape\n",
        "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
        "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
        "    rows = np.min((tensor.shape[0] // nrow + 1, 64))\n",
        "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
        "    plt.figure( figsize=(nrow,rows) )\n",
        "    plt.imshow(grid.cpu().permute((1, 2, 0)))\n",
        "\n",
        "\n",
        "def count_labels(triple_list):\n",
        "    label_counts = {}\n",
        "    labels = [triple[3] for triple in triple_list]\n",
        "\n",
        "    # Concatenate all the arrays into one\n",
        "    all_labels = np.concatenate(labels)\n",
        "\n",
        "    # Count the occurrences of each label\n",
        "    counter = Counter(all_labels)\n",
        "\n",
        "    # Create a dict with the counts for labels 1 to 4\n",
        "    counts_dict = {i: counter[i] for i in counter.keys()}\n",
        "    return counts_dict\n",
        "\n",
        "\n",
        "def class_weights_tensor(label_weights):\n",
        "    num_classes = max(label_weights.keys())+1\n",
        "    weight_tensor = torch.zeros(num_classes, dtype=torch.float32)\n",
        "    # Sort the dictionary by keys (labels)\n",
        "    sorted_label_weights = sorted(label_weights.items(), key=lambda x: x[0])\n",
        "    for label, weight in sorted_label_weights:\n",
        "        weight_tensor[label] = weight  # Subtract 1 from label if your labels start from 1\n",
        "    return weight_tensor\n",
        "\n",
        "\n",
        "def compute_average_weights(graphs):\n",
        "    label_counts = count_labels(graphs)\n",
        "    total_count = sum(label_counts.values())\n",
        "    class_weights = {label: total_count / count for label, count in label_counts.items()}\n",
        "    weight_tensor = class_weights_tensor(class_weights)\n",
        "    return weight_tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKEY_j_lCzCa"
      },
      "source": [
        "## Dataset class  💾\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-Pa_KNzpRi0d"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Preprocessing script to convert from data provided by BraTS to data used by our model. Should be the first thing you run.\n",
        "Fulfills the following four functions:\n",
        "1. Normalize and standardize each image of each MRI modality\n",
        "2. Combine multiple MRI modalitities into one image array\n",
        "3. Swap labels from BraTS order (0,2,1,4) to more intuitive order (0,1,2,3)\n",
        "4. Convert image into a graph\n",
        "    Using Simple Linear Iterative Clustering algorithm\n",
        "    Parameters passed on command line\n",
        "\n",
        "If no labels are present (e.g. at test time, in deployment) can also build graph without labels.\n",
        "\n",
        "Saves the following in the specified output directory for each sample\n",
        "MRI_ID/\n",
        "    _input.nii.gz (processed and combined modalities for a sample as nifti file)\n",
        "    _label.nii.gz\n",
        "    _nxgraph.json (networkx graph containing both graph topography and features and labels for each node)\n",
        "    _supervoxels.nii.gz (supervoxel partitioning produced by SLIC)\n",
        "    _crop.npy (optionally the crop of the processed data relative to the original data) (crops out empty image planes)\n",
        "'''\n",
        "\n",
        "LABEL_MAP = {4: 3, 2: 1, 1: 2}\n",
        "\n",
        "\n",
        "def swap_labels_from_brats(label_data):\n",
        "    uniques = np.unique(label_data)\n",
        "    for u in uniques:\n",
        "        if u not in [0, 1, 2, 4]:\n",
        "            raise RuntimeError('unexpected label')\n",
        "    new_label_data = np.zeros_like(label_data, dtype=np.int16)\n",
        "    new_label_data[label_data == 4] = LABEL_MAP[4]\n",
        "    new_label_data[label_data == 2] = LABEL_MAP[2]\n",
        "    new_label_data[label_data == 1] = LABEL_MAP[1]\n",
        "    return new_label_data\n",
        "\n",
        "def swap_labels_to_brats(label_data):\n",
        "    uniques = np.unique(label_data)\n",
        "    for u in uniques:\n",
        "        if u not in [0, 1, 2, 3]:\n",
        "            raise RuntimeError('unexpected label')\n",
        "    new_label_data = np.zeros_like(label_data, dtype=np.int16)\n",
        "    new_label_data[label_data == LABEL_MAP[4]] = 4\n",
        "    new_label_data[label_data == LABEL_MAP[2]] = 2\n",
        "    new_label_data[label_data == LABEL_MAP[1]] = 1\n",
        "    return new_label_data\n",
        "\n",
        "\n",
        "def swap_labels_to_brats_2023(label_data):\n",
        "    uniques = np.unique(label_data)\n",
        "    for u in uniques:\n",
        "        if u not in [0, 1, 2, 3]:\n",
        "            raise RuntimeError('unexpected label')\n",
        "    new_label_data = np.zeros_like(label_data, dtype=np.int16)\n",
        "    new_label_data[label_data == LABEL_MAP[4]] = 3\n",
        "    new_label_data[label_data == LABEL_MAP[2]] = 2\n",
        "    new_label_data[label_data == LABEL_MAP[1]] = 1\n",
        "    return new_label_data\n",
        "\n",
        "class DataPreprocessor(Dataset):\n",
        "    def __init__(self, dotenv_path = \"/content/drive/MyDrive/Lorusso/BraTS/.env\", transform:bool = True, INPUT_PATH = None,\n",
        "                 num_nodes = 15000, boxiness_coef = 0.5, num_neighbors = 0, **kwargs):\n",
        "\n",
        "        load_dotenv(dotenv_path)\n",
        "        # Data mean and variance\n",
        "        data_stats = ([0.4645, 0.6625, 0.4064, 0.3648],\n",
        "                      [0.1593, 0.1703, 0.1216, 0.1627])\n",
        "        self.N_THREADS = 8\n",
        "        self.num_nodes = num_nodes\n",
        "        self.boxiness_coef = boxiness_coef\n",
        "        self.num_neighbors = num_neighbors\n",
        "\n",
        "        if(INPUT_PATH is not None and os.path.exists(INPUT_PATH)):\n",
        "            self.data_dir = INPUT_PATH\n",
        "        else:\n",
        "            self.data_dir = os.getenv('INPUT_DATA_DIR')\n",
        "\n",
        "        self.output_dir = os.getenv('PROCESSED_DATA_DIR')\n",
        "        self.graph_dir = f\"{self.output_dir}_{self.num_nodes}_{self.boxiness_coef}_{self.num_neighbors}{os.sep}{os.path.basename(self.data_dir)}\"\n",
        "        self.logger = Logger(filename=os.path.join(os.getenv('LOG_PATH'), os.path.basename(self.data_dir)+'_logs.txt'))\n",
        "\n",
        "        self.mri_prefix = 'BraTS2021_'\n",
        "        self.modality_extensions = [\"_flair.nii.gz\", \"_t1.nii.gz\", \"_t1ce.nii.gz\", \"_t2.nii.gz\"]\n",
        "        self.label_extension = \"_seg.nii.gz\"\n",
        "        self.dataset_mean = np.array(data_stats[0], dtype=np.float32)\n",
        "        self.dataset_std = np.array(data_stats[1], dtype=np.float32)\n",
        "        self.transform = transform\n",
        "        self.force_conversion = False\n",
        "\n",
        "        # Set or overwrite additional attributes\n",
        "        for el in kwargs.keys():\n",
        "            setattr(self,str(el),kwargs[el])\n",
        "\n",
        "        self.include_labels = self.label_extension is not None\n",
        "        self.all_ids, self.id_to_fp = self.get_all_mris_in_dataset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_ids)\n",
        "\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        # DO NOT APPLY PADDING!! See this issue:\n",
        "        # https://github.com/RobertoLorusso/BraTS/issues/10#issue-2065431255\n",
        "        #imstack, labels = self.padding(imstack, labels)\n",
        "\n",
        "        imstack = self.read_in_patient_sample(idx)\n",
        "        if(self.include_labels):\n",
        "            labels = self.read_in_labels(idx)\n",
        "        else:\n",
        "            labels = None\n",
        "        crop_idxs = None\n",
        "\n",
        "        if (self.transform):\n",
        "            imstack,labels,crop_idxs = self.get_standardized_image(imstack, labels)\n",
        "        return imstack, labels, crop_idxs\n",
        "\n",
        "\n",
        "    def read_in_patient_sample(self, mri_id):\n",
        "\n",
        "        scan_dir = self.data_dir + os.sep + mri_id\n",
        "        modality_exts = self.modality_extensions\n",
        "        num_modalities=len(modality_exts)\n",
        "        modality_imgs = []\n",
        "        for root, _, files in os.walk(scan_dir):\n",
        "            for ext in modality_exts:\n",
        "                for filename in files:\n",
        "                    if filename.endswith(ext):\n",
        "                        filepath = os.path.join(root, filename)\n",
        "                        mod_img = nib.load(filepath)\n",
        "                        #data is actually stored as int16\n",
        "                        img_data = np.array(mod_img.dataobj,dtype=np.float32)\n",
        "                        modality_imgs.append(img_data)\n",
        "        #check that all the modalities were present in the folder\n",
        "        assert(len(modality_imgs)==num_modalities)\n",
        "\n",
        "        patient_sample = np.stack(modality_imgs,3) if num_modalities>1 else modality_imgs[0]\n",
        "        return patient_sample\n",
        "\n",
        "\n",
        "    def read_in_labels(self,mri_id):\n",
        "        scan_dir = self.data_dir + os.sep + mri_id\n",
        "        for filename in os.listdir(scan_dir):\n",
        "            if filename.endswith(self.label_extension):\n",
        "                label_nib = nib.load(scan_dir+os.sep+filename)\n",
        "                #potentially also return affine if they are different between images (which they are not for brats)\n",
        "                return np.array(label_nib.dataobj,dtype=np.int16)\n",
        "        raise FileNotFoundError(f\"Label image not found in folder: {scan_dir}\")\n",
        "\n",
        "\n",
        "    def get_all_mris_in_dataset(self):\n",
        "        mri_folders = glob.glob(f\"{self.data_dir}**/{self.mri_prefix}*/\",\n",
        "                                recursive=True)\n",
        "        mri_folders = self.remove_incomplete_mris(mri_folders)\n",
        "        scan_dic = {os.path.split(fp)[0].split(\"/\")[-1]: fp for fp in mri_folders}\n",
        "        if(len(mri_folders) == 0):\n",
        "            print(\"No MRI found at \" + self.data_dir)\n",
        "        return list(scan_dic.keys()), scan_dic\n",
        "\n",
        "\n",
        "    def remove_incomplete_mris(self, mri_folders):\n",
        "        # if there are any you want to ignore just add them to this list\n",
        "        removed_mris = []\n",
        "        return [fp for fp in mri_folders if fp.split(\"/\")[-2] not in removed_mris]\n",
        "\n",
        "    def padding(self,image, labels):\n",
        "        n_channels = np.shape(image)[0]\n",
        "        max_val = max(np.shape(image))\n",
        "        pad_list = np.zeros([n_channels,max_val,max_val,max_val],dtype=np.float32)\n",
        "\n",
        "        for channel in range(0, n_channels): # pad every channel\n",
        "            pad_list[channel] = np.pad(image[channel],[(42,43),(0,0),(0,0)],'constant')\n",
        "        labels = np.pad(labels, [(42,43),(0,0),(0,0)],'constant')\n",
        "\n",
        "        return pad_list, labels\n",
        "\n",
        "\n",
        "    def get_standardized_image(self, image_data, label_data):\n",
        "\n",
        "        #standardized_labels = self.swap_labels_from_brats(label_data)\n",
        "\n",
        "        crop_idxs = self.determine_brain_crop(image_data)\n",
        "        cropped_data = image_data[crop_idxs]\n",
        "        if(self.include_labels):\n",
        "            cropped_labels = label_data[crop_idxs]\n",
        "            standardized_labels= swap_labels_from_brats(cropped_labels)\n",
        "        else:\n",
        "            standardized_labels = None\n",
        "\n",
        "        normalized_data = self.normalize_img_quantile(cropped_data)\n",
        "        standardized_data = self.standardize_img(normalized_data)\n",
        "        return standardized_data,standardized_labels,crop_idxs\n",
        "\n",
        "\n",
        "    def determine_brain_crop(self,multi_modal_data):\n",
        "        if(len(multi_modal_data.shape)==4):\n",
        "            max_intensity_vals = np.amax(multi_modal_data,axis=3)\n",
        "        elif(len(multi_modal_data.shape)==3):\n",
        "            max_intensity_vals = multi_modal_data\n",
        "        else:\n",
        "            raise Exception(f\"Expected input shape of either nxmxr or nxmxrxC. Instead got {multi_modal_data.shape}\")\n",
        "        mask = max_intensity_vals>0.01\n",
        "        ix = np.ix_(mask.any(axis=(1,2)),mask.any(axis=(0,2)),mask.any(axis=(0,1)))\n",
        "\n",
        "        return ix\n",
        "\n",
        "    def normalize_img(self, img_array):\n",
        "        new_image = np.zeros(img_array.shape, dtype=np.float32)\n",
        "        n_channel = img_array.shape[3] # channel-first images\n",
        "\n",
        "        for channel in range(0, n_channel): # normalize every channel\n",
        "\n",
        "            maxval, minval= np.max(img_array[channel]), np.min(img_array[channel])\n",
        "            new_image[channel] = (img_array[channel] - minval)/(maxval-minval)\n",
        "        return new_image\n",
        "\n",
        "\n",
        "    def normalize_img_quantile(self, img_array):\n",
        "        # Exclude the channel axis, for channel-first images is zero\n",
        "        quantile = np.quantile(img_array, 0.995, axis = (0,1,2) ).astype(np.float32)\n",
        "        img_array = img_array/quantile\n",
        "        return img_array\n",
        "\n",
        "\n",
        "\n",
        "    def standardize_img(self,img_array):\n",
        "        centered = img_array-self.dataset_mean\n",
        "        standardized = centered/self.dataset_std\n",
        "        return standardized\n",
        "\n",
        "\n",
        "    def get_status_ids(self):\n",
        "\n",
        "        common_ids = set()\n",
        "        converting_ids = set()\n",
        "        finished_ids = set()\n",
        "\n",
        "        finished = []\n",
        "        pending = []\n",
        "\n",
        "        try:\n",
        "            regex = r'(Converting|Finished) ' + self.mri_prefix + '(\\d+)([-_]\\d+)?'\n",
        "\n",
        "            content = self.logger.read_msg()\n",
        "\n",
        "            # Define the regular expression pattern\n",
        "            pattern = re.compile(regex)\n",
        "            # Find all occurrences in the list\n",
        "            matches = [match.groups() for s in content if (match := pattern.match(s))]\n",
        "\n",
        "            for action, brats_id, suffix in matches:\n",
        "                if action == 'Converting':\n",
        "                    if(suffix is not None):\n",
        "                        converting_ids.add(self.mri_prefix + brats_id + suffix)\n",
        "                    else:\n",
        "                        converting_ids.add(self.mri_prefix + brats_id)\n",
        "                elif action == 'Finished':\n",
        "                    if(suffix is not None):\n",
        "                        finished_ids.add(self.mri_prefix + brats_id + suffix)\n",
        "                    else:\n",
        "                        finished_ids.add(self.mri_prefix + brats_id)\n",
        "            # Find finished and pending conversions\n",
        "            common_ids = converting_ids.intersection(finished_ids)\n",
        "            finished = [el for el in common_ids]\n",
        "            pending = [el for el in converting_ids.difference(common_ids)]\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Exception in DataPreprocessor.get_status_ids')\n",
        "            print(e)\n",
        "\n",
        "        return {'Finished':finished, 'Pending':pending}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def remove_pending_graphs(self):\n",
        "\n",
        "        pending = self.get_status_ids()['Pending']\n",
        "\n",
        "        for mri_id in pending:\n",
        "            remove_path = f\"{self.graph_dir}{os.sep}{mri_id}\"\n",
        "            try:\n",
        "                print('Removing pending graph: ' + remove_path)\n",
        "                shutil.rmtree(remove_path)\n",
        "            except Exception as e:\n",
        "                print('Exception in DataPreprocessor.remove_pending_graphs:')\n",
        "                print(e)\n",
        "\n",
        "\n",
        "\n",
        "    def split_dataset(self, fixed = (1001, 125, 125),seed = 42):\n",
        "\n",
        "        random.seed(seed)\n",
        "        pos = random.sample(range(0,len(self.all_ids)), len(self.all_ids))\n",
        "\n",
        "        if fixed:\n",
        "            if(np.sum(fixed) != len(self.all_ids)):\n",
        "                print(\"Error: fixed ratio does not sum up to one.\\nSwitching to default (1001,125,125))\")\n",
        "                fixed = (1001,125,125)\n",
        "\n",
        "            train_length = fixed[0]\n",
        "            val_length = fixed[1]\n",
        "            test_length = fixed[2]\n",
        "\n",
        "\n",
        "        split_dict = {\n",
        "            'train': [self.all_ids[i] for i in pos[:train_length]],\n",
        "            'val': [self.all_ids[i] for i in pos[train_length :train_length + val_length]],\n",
        "            'test': [self.all_ids[i] for i in pos[train_length + val_length:]]\n",
        "        }\n",
        "\n",
        "        for k in split_dict.keys():\n",
        "            parent = '/'.join(self.data_dir.split('/')[:-1])\n",
        "            dst = os.path.join(parent,k)\n",
        "\n",
        "            try:\n",
        "              # create train,val,test dirs\n",
        "              if(not os.path.exists(dst)):\n",
        "                os.mkdir(dst)\n",
        "\n",
        "              # copy splitted data inside folders\n",
        "              for id in split_dict[k]:\n",
        "                if(not os.path.exists(os.path.join(dst,id))):\n",
        "                   os.mkdir(os.path.join(dst,id))\n",
        "                copy_tree(self.id_to_fp[id],os.path.join(dst,id))\n",
        "\n",
        "            except Exception as e:\n",
        "              print(f\"Exception thrown in class {self.__class__.__name__ }, method split_dataset\")\n",
        "              print(e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def image_to_graph(self, mri_id):\n",
        "\n",
        "\n",
        "        save_path = f\"{self.graph_dir}{os.sep}{mri_id}\"\n",
        "        finished = self.get_status_ids()['Finished']\n",
        "\n",
        "        if(mri_id not in finished):\n",
        "            self.logger.log_msg('Converting ' + str(mri_id))\n",
        "        print('Converting ' + str(mri_id))\n",
        "\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "\n",
        "        if(not os.path.exists(f\"{save_path}{os.sep}{mri_id}_input.nii.gz\") or self.force_conversion == True):\n",
        "            imstack, labels, crop_idxs = self.__getitem__(mri_id)\n",
        "        else:\n",
        "            imstack, labels = self.get_image(mri_id)\n",
        "\n",
        "        # Load supervoxels, if already exist, and save computational time avoiding the runnning of slic\n",
        "        sv_partitioning = None\n",
        "        if(os.path.exists(f\"{save_path}{os.sep}{mri_id}_supervoxels.nii.gz\")):\n",
        "            sv_partitioning = self.get_supervoxel_partitioning(mri_id)\n",
        "            print('Loading supervoxels for mri' + str(mri_id))\n",
        "\n",
        "        nx_graph,node_feats,region_img = img2graph(imstack,labels,sv_partitioning,self.num_nodes,self.boxiness_coef,self.num_neighbors)\n",
        "\n",
        "        save_networkx_graph(nx_graph, f\"{save_path}{os.sep}{mri_id}_nxgraph.json\")\n",
        "        save_as_nifti(imstack,f\"{save_path}{os.sep}{mri_id}_input.nii.gz\")\n",
        "        if(self.include_labels):\n",
        "            save_as_nifti(labels,f\"{save_path}{os.sep}{mri_id}_label.nii.gz\")\n",
        "        save_as_nifti(region_img,f\"{save_path}{os.sep}{mri_id}_supervoxels.nii.gz\")\n",
        "\n",
        "        with open(f\"{save_path}{os.sep}{mri_id}_crop.pkl\", \"wb\") as f:\n",
        "            pickle.dump(crop_idxs, f)\n",
        "\n",
        "        return mri_id\n",
        "\n",
        "    def get_voxel_labels(self,mri_id):\n",
        "        fp=f\"{self.graph_dir}{os.sep}{mri_id}{os.sep}{mri_id}_label.nii.gz\"\n",
        "        return read_nifti(fp,np.int16)\n",
        "\n",
        "    def get_image(self,mri_id):\n",
        "        fp = f\"{self.graph_dir}{os.sep}{mri_id}{os.sep}{mri_id}_input.nii.gz\"\n",
        "        img = read_nifti(fp,np.float32)\n",
        "        return img,self.get_voxel_labels(mri_id)\n",
        "\n",
        "\n",
        "    def get_supervoxel_partitioning(self,mri_id):\n",
        "        fp=f\"{self.graph_dir}{os.sep}{mri_id}{os.sep}{mri_id}_supervoxels.nii.gz\"\n",
        "        return read_nifti(fp,np.int16)\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        self.remove_pending_graphs()\n",
        "\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.N_THREADS) as executor:\n",
        "            futures = [executor.submit(self.image_to_graph, mri_id) for mri_id in self.all_ids]\n",
        "            print(\"Set up Threads, starting execution\")\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                try:\n",
        "                    mri_id = future.result()\n",
        "                except Exception as exc:\n",
        "                    print(\"Exception caught in DataPreprocessor.run\")\n",
        "                    print(f\"{exc}\")\n",
        "                else:\n",
        "                    if(mri_id not in self.get_status_ids()['Finished']):\n",
        "                        # Log message\n",
        "                        self.logger.log_msg('Finished ' + str(mri_id))\n",
        "                    print(\"Finished \"+ str(mri_id))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SeqSampler(SequentialSampler):\n",
        "    \"\"\"Samples elements sequentially, always in the same order.\n",
        "\n",
        "    Args:\n",
        "        data_source (Dataset): dataset to sample from\n",
        "    \"\"\"\n",
        "    def __init__(self, data_source:Dataset):\n",
        "        self.data_source = data_source\n",
        "        self.indexDict = [id for id in data_source.all_ids]\n",
        "    def __iter__(self):\n",
        "        return iter(self.indexDict)\n",
        "    def __len__(self):\n",
        "        return len(self.indexDict)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBt4O3t3uD3V"
      },
      "source": [
        "## ImageGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DEs9SyOJuHlI"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "A Dataset similar to a torch dataset which iterates over all samples in a directory and returns the contents as numpy arrays.\n",
        "Expects to receive a filepath to the output of the preprocess script which should have the following:\n",
        "1.processed image (nifti)\n",
        "2.label image (nifti)\n",
        "3.networkx graph (json)\n",
        "4.supervoxel partitioning (nifti)\n",
        "5. (optionally) a .npy file containing the crop of the processed image relative to the original image\n",
        "\n",
        "\n",
        "#Input#\n",
        "dataset_root_dir: filepath to preprocessed dataset (generated by running preprocess script)\n",
        "mri_start_string: a prefix that every image folder starts with (can be empty string)\n",
        "read_image: whether to read in and return preprocessed images for each sample (only necessary for CNN model)\n",
        "read_graph: whether to return graphs for each sample (for training GNN)\n",
        "read_label: whether to read in labels. Will be returned in vector form (one label per node if )\n",
        "\n",
        "#Output#\n",
        "\n",
        "If graph:\n",
        "Returns a DGL Graph, features for each node, and (optionally) labels for each node\n",
        "If image:\n",
        "Returns a numpy image array and (optionally) a numpy label array\n",
        "\n",
        "'''\n",
        "\n",
        "def minibatch_graphs(samples):\n",
        "    mri_ids,graphs,features, labels = map(list, zip(*samples))\n",
        "    #print(\"Batch Mri Ids:\",mri_ids)\n",
        "    batched_graph = dgl_batch(graphs)\n",
        "    return mri_ids,batched_graph, torch.FloatTensor(np.concatenate(features)), torch.LongTensor(np.concatenate(labels))\n",
        "\n",
        "\n",
        "class ImageGraphDataset(Dataset):\n",
        "    def __init__(self, dataset_root_dir,mri_start_string,read_image=True,read_graph=True,read_label=True, glob_features = None):\n",
        "        self.dataset_root_dir=dataset_root_dir\n",
        "        self.all_ids = self.get_all_mris_in_dataset(dataset_root_dir,mri_start_string)\n",
        "        self.read_image=read_image\n",
        "        self.read_graph=read_graph\n",
        "        self.read_label = read_label\n",
        "        self.glob_features = glob_features\n",
        "        assert(self.read_graph or self.read_image)\n",
        "\n",
        "    def get_all_mris_in_dataset(self,dataset_root_dir,mri_start_string):\n",
        "        mri_folders = glob.glob(f\"{dataset_root_dir}**/{mri_start_string}*/\",recursive=True)\n",
        "        mri_ids = [fp.split(os.sep)[-2] for fp in mri_folders]\n",
        "        print(f\"Found {len(mri_folders)} MRIs\")\n",
        "        return mri_ids\n",
        "\n",
        "    def get_one(self,mri_id):\n",
        "        if(self.read_graph and not self.read_image):\n",
        "            return (mri_id, *self.get_graph(mri_id))\n",
        "        elif(self.read_image  and not self.read_graph):\n",
        "            return (mri_id, *self.get_image(mri_id))\n",
        "        elif(self.read_image and self.read_graph):\n",
        "            return (mri_id, *self.get_graph(mri_id), *self.get_image(mri_id))\n",
        "        else:\n",
        "            print(\"Invalid combination of flags\")\n",
        "\n",
        "\n",
        "    def add_features(self,features):\n",
        "        feats_len = len(features)\n",
        "        new_features = np.ones([feats_len + len(self.glob_features)])\n",
        "        new_features[:feats_len] = np.copy(features)\n",
        "        new_features[feats_len:] = np.copy(self.glob_features)\n",
        "\n",
        "        return np.array(new_features,dtype=np.float32)\n",
        "\n",
        "    '''\n",
        "    Reads in the saved networkx graph, converts it to a DGLGraph, normalizes the graph (not actually sure how useful this is),\n",
        "    and returns the DGLGraph as well as a vector of node features and optionally labels.\n",
        "    '''\n",
        "    def get_graph(self,mri_id):\n",
        "        nx_graph = load_networkx_graph(f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_nxgraph.json\")\n",
        "        #features = np.array([nx_graph.nodes[n]['features'] for n in nx_graph.nodes])\n",
        "        features = []\n",
        "        for n in nx_graph.nodes:\n",
        "            node_features = np.array(nx_graph.nodes[n]['features'])\n",
        "            if(self.glob_features is not None):\n",
        "                node_features = self.add_features(node_features)\n",
        "            features.append(node_features)\n",
        "        features = np.array(features)\n",
        "\n",
        "        if(self.read_label):\n",
        "            labels = np.array([nx_graph.nodes[n]['label'] for n in nx_graph.nodes])\n",
        "\n",
        "        G = to_dgl_graph(nx_graph)\n",
        "        n_edges = G.number_of_edges()\n",
        "        # normalization\n",
        "        degs = G.in_degrees().float()\n",
        "        norm = torch.pow(degs, -0.5)\n",
        "        norm[torch.isinf(norm)] = 0\n",
        "        G.ndata['norm'] = norm.unsqueeze(1)\n",
        "\n",
        "        #G.ndata['feat'] = features\n",
        "        if(self.read_label):\n",
        "            #G.ndata['label'] = labels\n",
        "            return G, features, labels\n",
        "        return G, features\n",
        "\n",
        "    def get_voxel_labels(self,mri_id):\n",
        "        fp=f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_label.nii.gz\"\n",
        "        return read_nifti(fp,np.int16)\n",
        "\n",
        "    def get_image(self,mri_id):\n",
        "        fp = f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_input.nii.gz\"\n",
        "        img = read_nifti(fp,np.float32)\n",
        "        if(self.read_label):\n",
        "            return img,self.get_voxel_labels(mri_id)\n",
        "        else:\n",
        "            return (img,)\n",
        "\n",
        "    def get_supervoxel_partitioning(self,mri_id):\n",
        "        fp=f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_supervoxels.nii.gz\"\n",
        "        return read_nifti(fp,np.int16)\n",
        "\n",
        "    def get_crop(self,mri_id):\n",
        "        fp=f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_crop.pkl\"\n",
        "        return tuple(np.load(fp,allow_pickle=True))\n",
        "\n",
        "    def save_voxel_preds(self, mri_id, predicted_nodes):\n",
        "        supervoxel_partitioning = self.get_supervoxel_partitioning(mri_id)\n",
        "        raw_data_crop = self.get_crop(mri_id)\n",
        "        predicted_voxels = project_nodes_to_img(supervoxel_partitioning,predicted_nodes)\n",
        "        predicted_voxels = uncrop_to_brats_size(raw_data_crop,predicted_voxels)\n",
        "        predicted_voxels = swap_labels_to_brats(predicted_voxels)\n",
        "        save_as_nifti(predicted_voxels,f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_seg.nii.gz\")\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        for mri_id in self.all_ids:\n",
        "            yield self.get_one(mri_id)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        mri_id = self.all_ids[index]\n",
        "        return self.get_one(mri_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_ids)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgINpL8BK3hL"
      },
      "source": [
        "## Models 📪\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "78WIjwSZRkGN"
      },
      "outputs": [],
      "source": [
        "\n",
        "from dgl.nn.pytorch import GATConv, GraphConv\n",
        "from dgl.nn.pytorch.conv import ChebConv\n",
        "from dgl.nn.pytorch.conv import SAGEConv, GINConv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "'''\n",
        "Contains the actual neural network architectures.\n",
        "Supports GraphSAGE with either the pool,mean,gcn, or lstm aggregator as well as GAT.\n",
        "The input, output, and intermediate layer sizes can all be specified.\n",
        "Typically will call init_graph_net and pass along the desired model and hyperparameters.\n",
        "\n",
        "Also contains the CNN Refinement net which is a very simple 2 layer 3D convolutional neural network.\n",
        "As input, it expects 8 channels, which are the concatenated 4 input modalities and 4 output logits of the GNN predictions.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "class GraphSage(nn.Module):\n",
        "    def __init__(self,in_feats,layer_sizes,n_classes,aggregator_type,dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        # input layer\n",
        "        self.layers.append(SAGEConv(in_feats, layer_sizes[0], aggregator_type, feat_drop=dropout, activation=F.relu))\n",
        "        # hidden layers\n",
        "        for i in range(1,len(layer_sizes)):\n",
        "            self.layers.append(SAGEConv(layer_sizes[i-1], layer_sizes[i], aggregator_type, feat_drop=dropout, activation=F.relu))\n",
        "        # output layer\n",
        "        self.layers.append(SAGEConv(layer_sizes[-1], n_classes, aggregator_type, feat_drop=0, activation=None))\n",
        "\n",
        "    def forward(self,graph,features):\n",
        "        h = features\n",
        "        for layer in self.layers:\n",
        "            h = layer(graph, h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self,in_feats,layer_sizes,n_classes,heads,residuals,\n",
        "                activation=F.elu,feat_drop=0,attn_drop=0,negative_slope=0.2):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        # input projection (no residual)\n",
        "        self.layers.append(GATConv(\n",
        "            in_feats, layer_sizes[0], heads[0],\n",
        "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
        "        # hidden layers\n",
        "        for i in range(1, len(layer_sizes)):\n",
        "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "            self.layers.append(GATConv(\n",
        "                layer_sizes[i-1] * heads[i-1], layer_sizes[i], heads[i],\n",
        "                feat_drop, attn_drop, negative_slope, residuals[i], self.activation))\n",
        "        # output projection\n",
        "        self.layers.append(GATConv(\n",
        "            layer_sizes[-1] * heads[-1], n_classes, 1,\n",
        "            feat_drop, attn_drop, negative_slope, False, None))\n",
        "\n",
        "    def forward(self,g, inputs):\n",
        "        h = inputs\n",
        "        for l in range(len(self.layers)-1):\n",
        "            h = self.layers[l](g, h).flatten(1)\n",
        "        # output projection\n",
        "        logits = self.layers[-1](g, h).mean(1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class GIN(nn.Module):\n",
        "    def __init__(self, in_feats, layer_sizes, n_classes, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # input layer\n",
        "        self.layers.append(GINConv(apply_func=nn.Linear(in_feats, layer_sizes[0]), aggregator_type='sum'))\n",
        "        # hidden layers\n",
        "        for i in range(1, len(layer_sizes)):\n",
        "            self.layers.append(GINConv(apply_func=nn.Linear(layer_sizes[i-1], layer_sizes[i]), aggregator_type='sum'))\n",
        "        # output layer\n",
        "        self.layers.append(GINConv(apply_func=nn.Linear(layer_sizes[-1], n_classes), aggregator_type='sum'))\n",
        "\n",
        "    def forward(self, g, feat):\n",
        "        h = feat\n",
        "        for layer in self.layers[:-1]:\n",
        "            h = layer(g, h)\n",
        "            h = F.relu(h)\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = self.layers[-1](g, h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class ChebNet(nn.Module):\n",
        "    def __init__(self, in_feats, layer_sizes, n_classes, k, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Input layer\n",
        "        self.layers.append(ChebConv(in_feats, layer_sizes[0], k))\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(layer_sizes)):\n",
        "            self.layers.append(ChebConv(layer_sizes[i-1], layer_sizes[i], k))\n",
        "\n",
        "        # Output layer\n",
        "        self.layers.append(ChebConv(layer_sizes[-1], n_classes, k))\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            h = layer(g, h)\n",
        "            if i != len(self.layers) - 1: # No activation and dropout on the output layer\n",
        "                h = F.leaky_relu(h, 0.5)\n",
        "                h = self.dropout(h)\n",
        "        return h\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u3q4OhjYyGq"
      },
      "source": [
        "## Model Wrapper 📨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "biiQzha9JP8G"
      },
      "outputs": [],
      "source": [
        "class ModelWrapper():\n",
        "    \"\"\"\n",
        "    Allows train, evaluation, prediction and I/O operations on generic PyTorch models\n",
        "    The model is saved at every epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, optimizer: nn.Module, loss_fn: nn.Module,\n",
        "                 num_epochs: int, supervised: bool = True, dict_params:dict = {}, eval_metrics = None, isgnn:bool = True, LOAD_MODEL: bool = False,\n",
        "                 model_path: str  = '/content/drive/MyDrive/Lorusso/models', run_id = None):\n",
        "\n",
        "        self.device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "        self.model = model\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model = self.model.to(torch.float)\n",
        "        self.num_epochs = num_epochs\n",
        "        self.loss_fn = loss_fn\n",
        "        self.loss_fn = self.loss_fn.to(self.device)\n",
        "        self.eval_metrics = eval_metrics\n",
        "        self.optimizer = optimizer\n",
        "        self.model_path = model_path\n",
        "        self.save_path = self.model_path + '/' + self.model.__class__.__name__ + '/model.pt'\n",
        "\n",
        "        self.run_id = run_id\n",
        "        self.LOAD_MODEL = LOAD_MODEL\n",
        "        self.isgnn = isgnn\n",
        "        self.supervised = supervised\n",
        "        self.training_loss = []\n",
        "        self.validation_loss = []\n",
        "        self.dict_metrics = {}\n",
        "        self.elapsed_epochs = 0\n",
        "        self.elapsed_seconds = 0\n",
        "\n",
        "        self.dict_params = dict_params\n",
        "        self.update_params({'loss_fn':self.loss_fn.__class__.__name__})\n",
        "        self.update_params({'optimizer':self.optimizer.__class__.__name__})\n",
        "        self.update_params({'learning_rate':self.optimizer.state_dict()['param_groups'][0]['lr']})\n",
        "        self.update_params({'weight_decay':self.optimizer.state_dict()['param_groups'][0]['weight_decay']})\n",
        "\n",
        "        if(self.LOAD_MODEL):\n",
        "          self.load_checkpoint()\n",
        "\n",
        "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma = 0.98)\n",
        "\n",
        "\n",
        "        # Create directory for model loading\n",
        "        #try:\n",
        "        #  if(not os.path.exists(self.model_path + '/' + self.model.__class__.__name__)):\n",
        "        #    os.mkdir(self.model_path + '/' + self.model.__class__.__name__)\n",
        "        #except Exception as e:\n",
        "        #  print(f\"Exception thrown in class {self.__class__.__name__ }, method __init__\")\n",
        "        #  print(e)\n",
        "        #  print('\\n')\n",
        "\n",
        "\n",
        "    def update_params(self, new_params):\n",
        "        try:\n",
        "            self.dict_params.update(new_params)\n",
        "        except Exception as e:\n",
        "            print('Exception raised in WrapperModel.update_params')\n",
        "            print(e)\n",
        "\n",
        "    def log_checkpoint(self, info: dict):\n",
        "        mlflow.pytorch.log_state_dict(info, artifact_path='checkpoint')\n",
        "        #torch.save(info, self.save_path)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        \"\"\" Loads the last checkpoint for the given model \"\"\"\n",
        "        try:\n",
        "            if(self.run_id is None):\n",
        "                run_id = mlflow.search_runs(experiment_names=['BraTS_'+type(self.model).__name__],\n",
        "                                        order_by=[\"start_time DESC\"]).iloc[0].run_id\n",
        "            else:\n",
        "                run_id = self.run_id\n",
        "\n",
        "            checkpoint = mlflow.pytorch.load_state_dict('runs:/'+run_id+'/checkpoint',\n",
        "                                                        map_location=torch.device(self.device))\n",
        "\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.elapsed_epochs = len(checkpoint['training_loss'])\n",
        "            self.training_loss = checkpoint['training_loss']\n",
        "            self.validation_loss = checkpoint['validation_loss']\n",
        "            self.dict_metrics = checkpoint['dict_metrics']\n",
        "            self.elapsed_seconds = checkpoint['elapsed_seconds']\n",
        "            #print(self.elapsed_epochs)\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Exception thrown in class {self.model.__class__.__name__ }, method load_checkpoint\")\n",
        "            print(e)\n",
        "            print('\\n')\n",
        "            if(mlflow.active_run()):\n",
        "                mlflow.end_run()\n",
        "\n",
        "\n",
        "    #Calculates a slew of different metrics that might be interesting such as the number of nodes of each label and voxel and node Dice scores\n",
        "    def calculate_all_metrics_for_brain(self,mri_id,dataset,node_preds,node_labels):\n",
        "        label_counts = np.concatenate([evaluation.count_node_labels(node_preds),evaluation.count_node_labels(node_labels)])\n",
        "        node_dices = evaluation.calculate_node_dices(node_preds,node_labels)\n",
        "        #read in voxel_labels and supervoxel mapping to compute the image metrics\n",
        "        sv_partitioning = dataset.get_supervoxel_partitioning(mri_id)\n",
        "        true_voxels = dataset.get_voxel_labels(mri_id)\n",
        "        pred_voxels = project_nodes_to_img(sv_partitioning,node_preds)\n",
        "        voxel_metrics = evaluation.calculate_brats_metrics(pred_voxels,true_voxels)\n",
        "        return np.concatenate([node_dices,voxel_metrics])\n",
        "\n",
        "\n",
        "    def train(self, train_loader, val_loader = None,  experiment_prefix = ''):\n",
        "\n",
        "        try:\n",
        "            # Set MLFlow experiment\n",
        "            if(experiment_prefix):\n",
        "                mlflow.set_experiment(experiment_prefix + self.model.__class__.__name__)\n",
        "            else:\n",
        "                mlflow.set_experiment('BraTS_'+self.model.__class__.__name__)\n",
        "\n",
        "            # Start a new run if the model wasn't loaded\n",
        "            if(not mlflow.active_run()):\n",
        "                # Track metrics in the current run\n",
        "                mlflow.start_run()\n",
        "            elif(self.LOAD_MODEL == False and mlflow.active_run()):\n",
        "                mlflow.end_run()\n",
        "                mlflow.start_run()\n",
        "\n",
        "            for i in range(len(self.training_loss)):\n",
        "                mlflow.log_metric('train_loss', self.training_loss[i], step=i)\n",
        "\n",
        "            for i in range(len(self.validation_loss)):\n",
        "                mlflow.log_metric('val_loss', self.validation_loss[i], step=i)\n",
        "\n",
        "            self.update_params({'batch_size':train_loader.batch_size})\n",
        "            mlflow.log_params(self.dict_params)\n",
        "\n",
        "            training_loss = self.training_loss\n",
        "            validation_loss = self.validation_loss\n",
        "\n",
        "            self.tot_epochs = self.elapsed_epochs + self.num_epochs+1\n",
        "            self.tot_time = time.time()\n",
        "\n",
        "            # Train\n",
        "            for epoch in range(self.elapsed_epochs+1, self.tot_epochs):\n",
        "                start = time.time() # track time\n",
        "\n",
        "                # Complete pending evaluation\n",
        "                if(len(training_loss) > len(validation_loss)):\n",
        "\n",
        "                    # COMPLETE EVALUATION OF PREVIOUS EPOCH\n",
        "                    # NB: epoch = epoch - 1\n",
        "                    if(val_loader is not None):\n",
        "\n",
        "                        val_batch_loss = self.__eval(val_loader, (epoch-1) )\n",
        "                        validation_loss.append(np.array(val_batch_loss).mean())\n",
        "\n",
        "                        # Log additional metrics\n",
        "                        self.dict_metrics = self.__eval_metrics(val_loader)\n",
        "                        for k in self.dict_metrics.keys():\n",
        "                            mlflow.log_metric(str(k), self.dict_metrics[k], step=epoch-1)\n",
        "\n",
        "                        mlflow.log_metric('val_loss',validation_loss[-1], step=epoch-1)\n",
        "\n",
        "                        print(f\"Epoch: {epoch-1}/{self.tot_epochs-1}, Mean Validation Loss: {validation_loss[-1]:.4f}, Elapsed time: {time.time() - start:.0f} sec \\n\")\n",
        "\n",
        "                      # Update training time\n",
        "                        epoch_time = int(time.time() - self.tot_time) + self.elapsed_seconds\n",
        "\n",
        "                      # Create checkpoint\n",
        "                        val_dict = {\n",
        "                                  'model_state_dict': self.model.state_dict(),\n",
        "                                  'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                                  'training_loss': training_loss,\n",
        "                                  'validation_loss': validation_loss,\n",
        "                                  'dict_metrics': self.dict_metrics,\n",
        "                                  'elapsed_seconds': epoch_time\n",
        "                                  }\n",
        "                        self.log_checkpoint(val_dict)\n",
        "                    else:\n",
        "                        # Kind of exception, needed to keep the vectors of the same size\n",
        "                        validation_loss.append(np.mean(validation_loss))\n",
        "\n",
        "                        #Log metric\n",
        "                        mlflow.log_metric('val_loss',validation_loss[-1], step=epoch-1)\n",
        "\n",
        "                #### TRAIN ######\n",
        "                train_batch_loss = self.__train(train_loader, epoch)\n",
        "                training_loss.append(np.array(train_batch_loss).mean())\n",
        "\n",
        "                # Log metrics\n",
        "                mlflow.log_metric('train_loss',training_loss[-1], step=epoch) # MLFLOW tracking\n",
        "                print(f\"Epoch: {epoch}/{self.tot_epochs-1}, Mean Training Loss: {training_loss[-1]:.4f}, Epoch elapsed time: {time.time() - start:.0f} sec\")\n",
        "\n",
        "                #Save model every elapsed epoch\n",
        "                self.elapsed_epochs = epoch\n",
        "                epoch_time = int(time.time() - self.tot_time) + self.elapsed_seconds\n",
        "\n",
        "                train_dict = {\n",
        "                          'model_state_dict': self.model.state_dict(),\n",
        "                          'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                          'training_loss': training_loss,\n",
        "                          'validation_loss': validation_loss,\n",
        "                          'dict_metrics':self.dict_metrics,\n",
        "                          'elapsed_seconds': epoch_time\n",
        "                          }\n",
        "                self.log_checkpoint(train_dict)\n",
        "\n",
        "                #### EVALUATE ######\n",
        "                if(val_loader is not None):\n",
        "\n",
        "                    val_batch_loss = self.__eval(val_loader, epoch)\n",
        "                    validation_loss.append(np.array(val_batch_loss).mean())\n",
        "\n",
        "                    # Log additional metrics\n",
        "                    self.dict_metrics = self.__eval_metrics(val_loader)\n",
        "                    for k in self.dict_metrics.keys():\n",
        "                        mlflow.log_metric(str(k), self.dict_metrics[k], step=epoch)\n",
        "\n",
        "                    # Log metric\n",
        "                    mlflow.log_metric('val_loss',validation_loss[-1], step=epoch)\n",
        "                    print(f\"Epoch: {epoch}/{self.tot_epochs-1}, Mean Validation Loss: {validation_loss[-1]:.4f}, Elapsed time: {time.time() - start:.0f} sec\\n\")\n",
        "\n",
        "                    #Checkpoint\n",
        "                    epoch_time = int(time.time() - self.tot_time) + self.elapsed_seconds\n",
        "                    val_dict = {\n",
        "                              'model_state_dict': self.model.state_dict(),\n",
        "                              'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                              'training_loss': training_loss,\n",
        "                              'validation_loss': validation_loss,\n",
        "                              'dict_metrics':self.dict_metrics,\n",
        "                              'elapsed_seconds': epoch_time\n",
        "                              }\n",
        "                    self.log_checkpoint(val_dict)\n",
        "\n",
        "            print(f\"Total training time: {time.time()-self.tot_time:.0f} sec\")\n",
        "\n",
        "            # Log model --> end run\n",
        "            mlflow.pytorch.log_model(self.model, artifact_path='model')\n",
        "            mlflow.end_run()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Exception thrown in class Wrapper, method train:\")\n",
        "            print(e)\n",
        "            print('\\n')\n",
        "            mlflow.end_run()\n",
        "\n",
        "        return training_loss, validation_loss\n",
        "\n",
        "\n",
        "\n",
        "    def __train(self, train_loader: DataLoader, epoch:int):\n",
        "\n",
        "        \"\"\" Train for an epoch \"\"\"\n",
        "\n",
        "        self.model.train()\n",
        "        train_batch_loss = []\n",
        "        train_steps = len(train_loader)\n",
        "\n",
        "        if(self.isgnn):\n",
        "\n",
        "            metrics = np.zeros((len(train_loader),3))\n",
        "            i = 0\n",
        "\n",
        "            for batch_mris, batch_graphs, batch_features, batch_labels in train_loader:\n",
        "\n",
        "                batch_graphs = batch_graphs.to(self.device)\n",
        "                batch_features = batch_features.to(self.device)\n",
        "                batch_labels = batch_labels.to(self.device)\n",
        "\n",
        "                logits = self.model(batch_graphs,batch_features)\n",
        "\n",
        "                logits = logits.to(self.device)\n",
        "                _, predicted_classes = torch.max(logits, dim=1)\n",
        "                #predicted_classes = logits.argmax(1)\n",
        "\n",
        "                loss = self.loss_fn(logits, batch_labels)\n",
        "                train_batch_loss.append(loss.detach().item())\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                metrics[i][:] = evaluation.calculate_node_dices(predicted_classes.detach().cpu().numpy(),\n",
        "                                                          batch_labels.detach().cpu().numpy())\n",
        "                print(f\"{metrics[i]}\")\n",
        "\n",
        "                out = f\"Epoch: {epoch}/{self.tot_epochs-1}, Step: {i+1}/{train_steps}, Training Loss: {loss.item():.4f}, Elapsed time: {time.time() - self.tot_time:.0f} sec \"\n",
        "                sys.stdout.write(\"\\r\" + out)\n",
        "                sys.stdout.flush()\n",
        "                i = i+1\n",
        "\n",
        "\n",
        "            self.scheduler.step()\n",
        "            avg_metrics = np.mean(metrics,axis=0)\n",
        "\n",
        "            print(f\"\\ntrain_wt_dice: {np.round(avg_metrics[0],4)}\")\n",
        "            print(f\"train_ct_dice: {np.round(avg_metrics[1],4)}\")\n",
        "            print(f\"train_at_dice: {np.round(avg_metrics[2],4)}\")\n",
        "            #print(f\"train_wt_hd95: {np.round(avg_metrics[3],4)}\")\n",
        "            #print(f\"train_ct_hd95: {np.round(avg_metrics[4],4)}\")\n",
        "            #print(f\"train_at_hd95: {np.round(avg_metrics[5],4)}\\n\")\n",
        "\n",
        "            mlflow.log_metric('train_wt_dice', np.round(avg_metrics[0],4), step = epoch)\n",
        "            mlflow.log_metric('train_ct_dice', np.round(avg_metrics[1],4), step = epoch)\n",
        "            mlflow.log_metric('train_at_dice', np.round(avg_metrics[2],4), step = epoch)\n",
        "            #mlflow.log_metric('train_wt_hd95', np.round(avg_metrics[3],4), step = epoch)\n",
        "            #mlflow.log_metric('train_ct_hd95', np.round(avg_metrics[4],4), step = epoch)\n",
        "            #mlflow.log_metric('train_at_hd95', np.round(avg_metrics[5],4), step = epoch)\n",
        "        else:\n",
        "\n",
        "            for i, (data,labels) in enumerate(train_loader):\n",
        "\n",
        "                #torch.cuda.empty_cache()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                if self.device == 'cuda':\n",
        "                  data = data.type(torch.cuda.FloatTensor)\n",
        "                else:\n",
        "                  data = data.type(torch.FloatTensor)\n",
        "\n",
        "                data = data.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                outputs = self.model(data)\n",
        "\n",
        "                if(self.supervised):\n",
        "                  loss = self.loss_fn(outputs,labels)\n",
        "                else:\n",
        "                  loss = self.loss_fn(outputs, data)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                train_batch_loss.append(loss.item())\n",
        "\n",
        "                out = f\"Epoch: {epoch}/{self.tot_epochs-1}, Step: {i+1}/{train_steps}, Training Loss: {loss.item():.4f}, Elapsed time: {time.time() - self.tot_time:.0f} sec \"\n",
        "                sys.stdout.write(\"\\r\" + out)\n",
        "                sys.stdout.flush()\n",
        "            torch.cuda.empty_cache()\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        return train_batch_loss\n",
        "\n",
        "\n",
        "    def __eval(self, val_loader: DataLoader, epoch:int):\n",
        "\n",
        "        \"\"\" Evaluate for an epoch \"\"\"\n",
        "\n",
        "        val_steps =  len(val_loader)\n",
        "        self.model.eval()\n",
        "        val_batch_loss = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if(self.isgnn):\n",
        "\n",
        "                #metrics stores loss and node dices\n",
        "                metrics = np.zeros((len(val_loader),3))\n",
        "                i=0\n",
        "\n",
        "                for curr_id,batch_graphs,batch_features,batch_labels in val_loader:\n",
        "\n",
        "                    batch_graphs = batch_graphs.to(self.device)\n",
        "                    batch_features = batch_features.to(self.device)\n",
        "                    batch_labels = batch_labels.to(self.device)\n",
        "\n",
        "                    #batch_graphs = batch_graphs.to(self.device)\n",
        "                    #batch_features = torch.FloatTensor(batch_features).to(self.device)\n",
        "                    #batch_labels = torch.LongTensor(batch_labels).to(self.device)\n",
        "\n",
        "                    logits = self.model(batch_graphs,batch_features)\n",
        "                    logits = logits.to(self.device)\n",
        "\n",
        "                    val_loss = self.loss_fn(logits, batch_labels)\n",
        "\n",
        "                    val_batch_loss.append(val_loss.item())\n",
        "                    _, predicted_classes = torch.max(logits, dim=1)\n",
        "                    #predicted_classes = logits.argmax(1)\n",
        "                    res = evaluation.calculate_node_dices(predicted_classes.detach().cpu().numpy(),\n",
        "                                                          batch_labels.detach().cpu().numpy())\n",
        "                    metrics[i][:] = res\n",
        "                    i+=1\n",
        "\n",
        "                    out = f\"Epoch: {epoch}/{self.tot_epochs-1}, Validation Step: {i+1}/{val_steps}, Validation Loss: {val_loss.item():.4f}, Elapsed time: {time.time() - self.tot_time:.0f} sec \"\n",
        "                    sys.stdout.write(\"\\r\" + out)\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "                avg_metrics = np.mean(metrics,axis=0)\n",
        "\n",
        "                print(f\"\\nval_wt_dice:  {np.round(avg_metrics[0],4)}\")\n",
        "                print(f\"val_ct_dice:    {np.round(avg_metrics[1],4)}\")\n",
        "                print(f\"val_at_dice:    {np.round(avg_metrics[2],4)}\")\n",
        "                #print(f\"val_wt_hd95:    {np.round(avg_metrics[3],4)}\")\n",
        "                #print(f\"val_ct_hd95:    {np.round(avg_metrics[4],4)}\")\n",
        "                #print(f\"val_at_hd95:    {np.round(avg_metrics[5],4)}\\n\")\n",
        "\n",
        "                mlflow.log_metric('val_wt_dice', np.round(avg_metrics[0],4), step = epoch)\n",
        "                mlflow.log_metric('val_ct_dice', np.round(avg_metrics[1],4), step = epoch)\n",
        "                mlflow.log_metric('val_at_dice', np.round(avg_metrics[2],4), step = epoch)\n",
        "                #mlflow.log_metric('val_wt_hd95', np.round(avg_metrics[3],4), step = epoch)\n",
        "                #mlflow.log_metric('val_ct_hd95', np.round(avg_metrics[4],4), step = epoch)\n",
        "                #mlflow.log_metric('val_at_hd95', np.round(avg_metrics[5],4), step = epoch)\n",
        "\n",
        "                return val_batch_loss\n",
        "\n",
        "            else:\n",
        "                for i, (data,labels) in enumerate(val_loader):\n",
        "\n",
        "                  #torch.cuda.empty_cache()\n",
        "                  self.optimizer.zero_grad()\n",
        "\n",
        "                  if self.device == 'cuda':\n",
        "                    data = data.type(torch.cuda.FloatTensor)\n",
        "                  else:\n",
        "                    data = data.type(torch.FloatTensor)\n",
        "\n",
        "                  data = data.to(self.device)\n",
        "                  outputs = self.model(data)\n",
        "\n",
        "                  if(self.supervised):\n",
        "                      val_loss = self.loss_fn(outputs, labels)\n",
        "                  else:\n",
        "                      val_loss = self.loss_fn(outputs, data)\n",
        "\n",
        "                  val_batch_loss.append(val_loss.detach().item())\n",
        "\n",
        "                  out = f\"Epoch: {epoch}/{self.tot_epochs-1}, Validation Step: {i+1}/{val_steps}, Validation Loss: {val_loss.item():.4f}, Elapsed time: {time.time() - self.tot_time:.0f} sec \"\n",
        "                  sys.stdout.write(\"\\r\" + out)\n",
        "                  sys.stdout.flush()\n",
        "\n",
        "                torch.cuda.empty_cache()\n",
        "                time.sleep(.5)\n",
        "\n",
        "        return val_batch_loss\n",
        "\n",
        "\n",
        "    def __eval_metrics(self, data_loader:DataLoader):\n",
        "        \"\"\" Evaluates additional metrics aside the loss function \"\"\"\n",
        "        metrics_dict = {}\n",
        "        try:\n",
        "            self.model.eval()\n",
        "            if(self.eval_metrics is not None):\n",
        "\n",
        "                metrics_dict = {k.__class__.__name__:[] for k in self.eval_metrics}\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for i, (data,labels) in enumerate(data_loader):\n",
        "                        torch.cuda.empty_cache()\n",
        "                        self.optimizer.zero_grad()\n",
        "\n",
        "                        for metric in self.eval_metrics:\n",
        "\n",
        "                            if self.device == 'cuda':\n",
        "                                data = data.type(torch.cuda.FloatTensor)\n",
        "                            else:\n",
        "                                data = data.type(torch.FloatTensor)\n",
        "\n",
        "                            data = data.to(self.device)\n",
        "                            outputs = self.model(data)\n",
        "\n",
        "                            if(self.supervised):\n",
        "                                metric_value = metric(outputs, labels)\n",
        "                            else:\n",
        "                                metric_value = metric(outputs, data)\n",
        "\n",
        "                            metrics_dict[metric.__class__.__name__].append(metric_value.detach().item())\n",
        "\n",
        "                    for k in metrics_dict.keys():\n",
        "                        metrics_dict[k] = np.array(metrics_dict[k]).mean()\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Exception thrown in wrapper.__eval_metrics')\n",
        "            print(e)\n",
        "        finally:\n",
        "            return metrics_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict_graph(self, dataset:ImageGraphDataset, save_preds = True):\n",
        "\n",
        "        self.model.eval()\n",
        "        start = time.time()\n",
        "\n",
        "        predictions = {}\n",
        "        metrics = np.zeros([len(dataset), 3])\n",
        "\n",
        "        if(dataset.read_label):\n",
        "\n",
        "            i = 0\n",
        "\n",
        "            for curr_id,graph,feats,labels in dataset:\n",
        "\n",
        "                graph = graph.to(self.device)\n",
        "                feats = torch.FloatTensor(feats).to(self.device)\n",
        "                labels = torch.LongTensor(labels).to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    logits = self.model(graph,feats)\n",
        "\n",
        "                _, predicted_classes = torch.max(logits, dim=1)\n",
        "                #predicted_classes = logits.argmax(1)\n",
        "                res = evaluation.calculate_node_dices(predicted_classes.detach().cpu().numpy(),\n",
        "                                                          labels.detach().cpu().numpy())\n",
        "                metrics[i] = res\n",
        "                predictions[curr_id] = predicted_classes\n",
        "\n",
        "                i += 1\n",
        "\n",
        "                if(save_preds):\n",
        "                    dataset.save_voxel_preds(curr_id, predicted_classes)\n",
        "\n",
        "                out = f\"Step {i}/{len(dataset)+1} Elapsed time: {time.time() - start:.0f} sec \"\n",
        "                sys.stdout.write(\"\\r\" + out)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            avg_metrics = np.mean(metrics,axis=0)\n",
        "            return predictions, avg_metrics\n",
        "\n",
        "        else:\n",
        "            i = 0\n",
        "            for curr_id,graph,feats in dataset:\n",
        "\n",
        "                graph = graph.to(self.device)\n",
        "                feats = torch.FloatTensor(feats).to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    logits = self.model(graph,feats)\n",
        "\n",
        "                predicted_classes = logits.argmax(1).detach().cpu().numpy()\n",
        "                #_, predicted_classes = torch.max(logits, dim=1)\n",
        "                predictions[curr_id] = predicted_classes\n",
        "\n",
        "                if(save_preds):\n",
        "                    dataset.save_voxel_preds(curr_id, predicted_classes )\n",
        "\n",
        "                i += 1\n",
        "                out = f\"Step {i}/{len(dataset)+1} Elapsed time: {time.time() - start:.0f} sec \"\n",
        "                sys.stdout.write(\"\\r\" + out)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            return predictions\n",
        "\n",
        "\n",
        "\n",
        "    def predict_batch(self, data_loader:DataLoader):\n",
        "\n",
        "        output = []\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "              for i, batch in enumerate(data_loader):\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                if(len(batch) == 1):\n",
        "                  data = batch\n",
        "                else:\n",
        "                  data,label = batch\n",
        "\n",
        "                if self.device == 'cuda':\n",
        "                  data = data.type(torch.cuda.FloatTensor)\n",
        "                else:\n",
        "                  data = data.type(torch.FloatTensor)\n",
        "\n",
        "                data.to(self.device)\n",
        "\n",
        "                out = self.model(data)\n",
        "                out = out.cpu().detach().numpy()\n",
        "                output.append(out)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Exception thrown in class {self.model.__class__.__name__ }, method predict_batch:\")\n",
        "                print(e)\n",
        "                print('\\n')\n",
        "\n",
        "        return np.array(output)\n",
        "\n",
        "\n",
        "    def predict(self, data):\n",
        "\n",
        "        if device == 'cuda':\n",
        "          data = data.type(torch.cuda.FloatTensor)\n",
        "        else:\n",
        "          data = data.type(torch.FloatTensor)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            data.to(device)\n",
        "            output = self.model(data)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he-C8aZXQkry"
      },
      "source": [
        "## Loss 🕳"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pUxP6_HiQiP8"
      },
      "outputs": [],
      "source": [
        "class Loss(nn.Module):\n",
        "    def __init__(self, focal):\n",
        "        super(Loss, self).__init__()\n",
        "        if focal:\n",
        "            self.loss_fn = DiceFocalLoss(\n",
        "                include_background=False, softmax=True, to_onehot_y=True, batch=True, gamma=2.0\n",
        "            )\n",
        "        else:\n",
        "            self.loss_fn = DiceCELoss(include_background=False, softmax=True, to_onehot_y=True, batch=True)\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        return self.loss_fn(y_pred, y_true)\n",
        "\n",
        "\n",
        "class LossBraTS(nn.Module):\n",
        "    def __init__(self, focal):\n",
        "        super(LossBraTS, self).__init__()\n",
        "        self.dice = DiceLoss(sigmoid=True, batch=True)\n",
        "        self.ce = FocalLoss(gamma=2.0, to_onehot_y=False) if focal else nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def _loss(self, p, y):\n",
        "\n",
        "        return self.dice(p, y) + self.ce(p, y.float())\n",
        "\n",
        "    def forward(self, p, y):\n",
        "        print('p '+str(p.size()))\n",
        "        print('y '+str(y.size()))\n",
        "        y_wt, y_tc, y_et = y > 0, ((y == 1) + (y == 3)) > 0, y == 3\n",
        "        p_wt, p_tc, p_et = p[:, 0].unsqueeze(1), p[:, 1].unsqueeze(1), p[:, 2].unsqueeze(1)\n",
        "        l_wt, l_tc, l_et = self._loss(p_wt, y_wt), self._loss(p_tc, y_tc), self._loss(p_et, y_et)\n",
        "        return l_wt + l_tc + l_et"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsRN0nOwDif5"
      },
      "source": [
        "## Build dataset 🏗"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9m9XrfCJDqWy"
      },
      "outputs": [],
      "source": [
        "dotenv_path = \"/content/drive/MyDrive/Lorusso/BraTS/.env\"\n",
        "load_dotenv(dotenv_path)\n",
        "\n",
        "\n",
        "INPUT_PATH = os.getenv(\"INPUT_DATA_DIR\")\n",
        "PROCESSED_PATH = os.getenv('PROCESSED_DATA_DIR')\n",
        "INPUT_PATH_PARENT = '/'.join(INPUT_PATH.split('/')[:-1])\n",
        "\n",
        "TRAIN_PATH = os.path.join(INPUT_PATH_PARENT,'train')\n",
        "VAL_PATH = os.path.join(INPUT_PATH_PARENT,'val')\n",
        "TEST_PATH = os.path.join(INPUT_PATH_PARENT,'test')\n",
        "\n",
        "TAR_PATH = os.path.join(INPUT_PATH_PARENT,'BraTS2021_Training_Data.tar')\n",
        "BUILD_DATASET = False\n",
        "\n",
        "if(BUILD_DATASET):\n",
        "  untar_brats(tar_path = '/content/drive/MyDrive/Lorusso/BraTS/data/raw/BraTS2021_Training_Data.tar', extract_path = INPUT_PATH )\n",
        "  #dataset = DataPreprocessor()\n",
        "  #dataset.split_dataset()\n",
        "\n",
        "#dataset = DataPreprocessor()\n",
        "#train_loader = DataLoader(dataset, sampler = SeqSampler(dataset), batch_size = 1, num_workers = 0)\n",
        "#images, labels= next(iter(train_loader))\n",
        "#plot_brain_sections([images[0], labels[0]])\n",
        "#del images, labels, train_loader, dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59tr9TkL0zF3"
      },
      "source": [
        "# Generate Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZkZZCxM101ZL"
      },
      "outputs": [],
      "source": [
        "#dataset = DataPreprocessor(INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/raw/BraTS2023-Test', label_extension = None,  mri_prefix = 'BraTS-GLI-', modality_extensions = ['-t1c.nii.gz', '-t1n.nii.gz', '-t2f.nii.gz', '-t2w.nii.gz'], force_conversion = False)\n",
        "\n",
        "IMG2GRAPH = False\n",
        "if(IMG2GRAPH):\n",
        "    dataset_raw = DataPreprocessor(INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/raw/brats', boxiness_coef=0.1, force_conversion = True)\n",
        "    print(dataset_raw.graph_dir)\n",
        "    print('Finished: ' + str(len(set(dataset_raw.get_status_ids()['Finished']))))\n",
        "    if(not set(dataset_raw.get_status_ids()['Finished']).issuperset(set(dataset_raw.all_ids))):\n",
        "        ids_to_convert = list(set(dataset_raw.all_ids).difference(set(dataset_raw.get_status_ids()['Finished'])))\n",
        "        print('MRIs to convert: '+ str(len(ids_to_convert)))\n",
        "        dataset_raw.all_ids = ids_to_convert\n",
        "        print(f\"Pending conversions: {len(set(dataset_raw.get_status_ids()['Pending']))}\")\n",
        "        dataset_raw.run()\n",
        "    dataset = DataPreprocessor(INPUT_PATH = dataset_raw.graph_dir)\n",
        "    dataset.split_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz-6k-tLLDbh"
      },
      "source": [
        "# Train and predict ⌛"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dle5EmF9LXms"
      },
      "source": [
        "## Autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ5ItOcISTzW",
        "outputId": "fc0e152f-06c5-4e9c-f38c-4050ea5bae9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed epochs: 3\n"
          ]
        }
      ],
      "source": [
        "TRAIN_MODEL = False\n",
        "LOAD_MODEL = True\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "time.sleep(5)\n",
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "  num_workers = 0\n",
        "  batch_size = 1\n",
        "  num_epochs = 1\n",
        "  lr = 0.005\n",
        "  supervised = False\n",
        "\n",
        "  # MONAI AUTOENCODER\n",
        "  autoencoder = AutoEncoder(\n",
        "         spatial_dims=3,\n",
        "         kernel_size = 3,\n",
        "         up_kernel_size = 3,\n",
        "         in_channels=4,\n",
        "         out_channels=4,\n",
        "         channels=(5,),\n",
        "         strides=(2,),\n",
        "         inter_channels=(8, 8, 16),\n",
        "         inter_dilations=(1, 2, 4),\n",
        "         num_inter_units=2\n",
        "     )\n",
        "\n",
        "  optimizer = torch.optim.Adam(autoencoder.parameters(), lr=lr,weight_decay=1e-10)\n",
        "  loss_fn = nn.MSELoss() #SSIMLoss(spatial_dims=3)\n",
        "\n",
        "  wrapper_autoencoder = ModelWrapper(model = autoencoder,\n",
        "                            loss_fn = loss_fn,\n",
        "                            optimizer = optimizer,\n",
        "                            supervised = supervised,\n",
        "                            num_epochs = num_epochs,\n",
        "                            LOAD_MODEL = LOAD_MODEL\n",
        "                        )\n",
        "\n",
        "  dataset = DataPreprocessor(INPUT_PATH = INPUT_PATH)\n",
        "\n",
        "\n",
        "  train_dataset = DataPreprocessor(INPUT_PATH = TRAIN_PATH)\n",
        "  val_dataset = DataPreprocessor(INPUT_PATH = VAL_PATH)\n",
        "  test_dataset = DataPreprocessor(INPUT_PATH = TEST_PATH)\n",
        "\n",
        "  train_loader = DataLoader(dataset = train_dataset,\n",
        "                           sampler = SeqSampler(train_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "  val_loader = DataLoader(dataset = val_dataset,\n",
        "                           sampler = SeqSampler(val_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "  test_loader = DataLoader(dataset = test_dataset,\n",
        "                           sampler = SeqSampler(test_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "\n",
        "  print('Elapsed epochs: ' + str(wrapper_autoencoder.elapsed_epochs))\n",
        "\n",
        "if(TRAIN_MODEL):\n",
        "      training_loss, validation_loss = wrapper_autoencoder.train(train_loader = train_loader, val_loader = val_loader )\n",
        "      torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "Bo0aQf08rHvn",
        "outputId": "870f32b0-8a4e-46cd-c7be-6b7e5dc16a1b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "  xx = range(1,wrapper_autoencoder.elapsed_epochs+1)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(xx, wrapper_autoencoder.training_loss, '-o', label = 'Train')\n",
        "  plt.plot(xx, wrapper_autoencoder.validation_loss,'-o', label = 'Val')\n",
        "  plt.legend(loc='lower left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DvvSdXEvIw5w"
      },
      "outputs": [],
      "source": [
        "#if(TRAIN_MODEL or LOAD_MODEL):\n",
        "if(False):\n",
        "\n",
        "  slice_index = 100\n",
        "  ax = 0\n",
        "\n",
        "  im_test, lab = next(iter(val_loader))\n",
        "  output = wrapper.predict(data = im_test)\n",
        "  out = output[0]\n",
        "  out_numpy = out.cpu().detach().numpy()\n",
        "  im_test_numpy = im_test.cpu().detach().numpy()\n",
        "  print(np.shape(im_test_numpy))\n",
        "\n",
        "  plot_reconstruction(im_orig = np.sum(im_test_numpy[0], axis=0), im_rec = np.sum(out_numpy, axis = 0), ax = ax, slice_index = slice_index)\n",
        "  plot_brain_sections([im_test[0], lab[0]], ax = ax, slice_index = slice_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ETwFBvf1Zjyp"
      },
      "outputs": [],
      "source": [
        "#if(TRAIN_MODEL or LOAD_MODEL):\n",
        "if(False):\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(ker[0], allkernels=True)\n",
        "\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(torch.sum(ker, dim=(0)), allkernels=True)\n",
        "\n",
        "  kk = torch.sum(ker, dim= 0)\n",
        "  kk = torch.sum(kk, dim = 0)\n",
        "  plt.imshow(kk[0,:,:].cpu().detach().numpy())\n",
        "\n",
        "  # Sum over the channels\n",
        "  #ker =ker.sum(axis=(0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lndh0zBN3dGg"
      },
      "source": [
        "## Filters extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7Hx_jatYoMjM"
      },
      "outputs": [],
      "source": [
        "new_features = True\n",
        "# features v1 = quantiles 0.5 e 0.995 e 3 valori singolari più alti\n",
        "# features v1 performano meglio\n",
        "# features v2 = quantiles 0.25 0.5 0.75 0.995 a media valori singolari\n",
        "bottleneck = autoencoder.decode[0][0][0].weight.detach().clone()\n",
        "sum_bottleneck = bottleneck.sum(axis=(0,1)).flatten().cpu()\n",
        "A,S,Vt = np.linalg.svd(sum_bottleneck.reshape(3,3,3))\n",
        "\n",
        "quantile = get_quantiles(np.copy(sum_bottleneck))[-1]\n",
        "filter_mean = sum_bottleneck.mean()\n",
        "global_features = []\n",
        "input_feats = 20\n",
        "if(new_features):\n",
        "    input_feats = 25\n",
        "    for el in S:\n",
        "        global_features.append(el[0])\n",
        "    global_features.append(quantile)\n",
        "    global_features.append(filter_mean.detach().clone().item())\n",
        "    global_features = np.array(global_features, dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNn9uFyniXlr"
      },
      "source": [
        "## GNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! rm -r  '/content/drive/MyDrive/Lorusso/BraTS/data/interim/processed_15000_0.5_0'\n",
        "\n",
        "#! mkdir  '/content/drive/MyDrive/Lorusso/BraTS/data/interim/processed_15000_0.5_0'\n",
        "#! mkdir  '/content/drive/MyDrive/Lorusso/BraTS/data/interim/processed_15000_0.1_0'\n",
        "\n",
        "#t = os.listdir('/content/drive/MyDrive/Lorusso/BraTS/data/processed_15000_0.5_0/brats')[:10]\n",
        "#v = os.listdir('/content/drive/MyDrive/Lorusso/BraTS/data/processed_15000_0.5_0/brats')[10:20]\n",
        "#for el in t:\n",
        "#    shutil.copytree(f\"/content/drive/MyDrive/Lorusso/BraTS/data/processed_15000_0.5_0/brats/{el}\", f\"/content/drive/MyDrive/Lorusso/BraTS/data/interim/processed_15000_0.5_0/train/{el}\")\n",
        "#for el in v:\n",
        "#    shutil.copytree(f\"/content/drive/MyDrive/Lorusso/BraTS/data/processed_15000_0.5_0/brats/{el}\", f\"/content/drive/MyDrive/Lorusso/BraTS/data/interim/processed_15000_0.5_0/val/{el}\")"
      ],
      "metadata": {
        "id": "eDCi5KMRrtJ4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eXPYIaNWDN2o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e064d6bc-abe4-4467-97db-7ea2a8f6877c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1001 MRIs\n",
            "Found 125 MRIs\n",
            "Found 125 MRIs\n",
            "Elapsed epochs: 11\n",
            "[0.83067578 0.4411215  0.36858974]\n",
            "Epoch: 12/19, Step: 1/167, Training Loss: 0.1796, Elapsed time: 14 sec [0.90762686 0.89664708 0.84868067]\n",
            "Epoch: 12/19, Step: 2/167, Training Loss: 0.1529, Elapsed time: 26 sec [0.91152312 0.89494605 0.86277372]\n",
            "Epoch: 12/19, Step: 3/167, Training Loss: 0.1011, Elapsed time: 39 sec [0.89676425 0.91515994 0.88721805]\n",
            "Epoch: 12/19, Step: 4/167, Training Loss: 0.0990, Elapsed time: 51 sec [0.85023738 0.81369248 0.8540305 ]\n",
            "Epoch: 12/19, Step: 5/167, Training Loss: 0.2057, Elapsed time: 63 sec [0.90775255 0.91428571 0.85662867]\n",
            "Epoch: 12/19, Step: 6/167, Training Loss: 0.1454, Elapsed time: 76 sec [0.81646091 0.86464771 0.85292671]\n",
            "Epoch: 12/19, Step: 7/167, Training Loss: 0.1804, Elapsed time: 89 sec [0.89556174 0.78068044 0.82277671]\n",
            "Epoch: 12/19, Step: 8/167, Training Loss: 0.1573, Elapsed time: 102 sec [0.87293275 0.73874806 0.88647746]\n",
            "Epoch: 12/19, Step: 9/167, Training Loss: 0.1722, Elapsed time: 115 sec [0.85848214 0.84710018 0.86642599]\n",
            "Epoch: 12/19, Step: 10/167, Training Loss: 0.1316, Elapsed time: 126 sec [0.88930582 0.85695276 0.84069887]\n",
            "Epoch: 12/19, Step: 11/167, Training Loss: 0.1699, Elapsed time: 139 sec [0.89526293 0.88858696 0.82898551]\n",
            "Epoch: 12/19, Step: 12/167, Training Loss: 0.1515, Elapsed time: 150 sec [0.88547245 0.81049334 0.74037267]\n",
            "Epoch: 12/19, Step: 13/167, Training Loss: 0.2541, Elapsed time: 162 sec [0.91458055 0.88411588 0.83187036]\n",
            "Epoch: 12/19, Step: 14/167, Training Loss: 0.1128, Elapsed time: 176 sec [0.83744681 0.74619883 0.7397541 ]\n",
            "Epoch: 12/19, Step: 15/167, Training Loss: 0.1745, Elapsed time: 188 sec [0.90939263 0.80360236 0.68456376]\n",
            "Epoch: 12/19, Step: 16/167, Training Loss: 0.2846, Elapsed time: 200 sec [0.72273057 0.60875288 0.61111111]\n",
            "Epoch: 12/19, Step: 17/167, Training Loss: 0.3987, Elapsed time: 212 sec [0.87632509 0.86826667 0.85670023]\n",
            "Epoch: 12/19, Step: 18/167, Training Loss: 0.1190, Elapsed time: 226 sec [0.87414188 0.70448772 0.79266896]\n",
            "Epoch: 12/19, Step: 19/167, Training Loss: 0.1409, Elapsed time: 238 sec [0.826732   0.74515393 0.77777778]\n",
            "Epoch: 12/19, Step: 20/167, Training Loss: 0.1843, Elapsed time: 249 sec [0.82362427 0.76882197 0.83937824]\n",
            "Epoch: 12/19, Step: 21/167, Training Loss: 0.2189, Elapsed time: 263 sec [0.9039273  0.75441519 0.78493937]\n",
            "Epoch: 12/19, Step: 22/167, Training Loss: 0.2152, Elapsed time: 275 sec [0.89198709 0.75318962 0.88350983]\n",
            "Epoch: 12/19, Step: 23/167, Training Loss: 0.1814, Elapsed time: 288 sec [0.83348499 0.7929374  0.72439024]\n",
            "Epoch: 12/19, Step: 24/167, Training Loss: 0.4005, Elapsed time: 299 sec [0.81491841 0.86998395 0.84947491]\n",
            "Epoch: 12/19, Step: 25/167, Training Loss: 0.1096, Elapsed time: 312 sec [0.81364477 0.80942184 0.85044865]\n",
            "Epoch: 12/19, Step: 26/167, Training Loss: 0.1604, Elapsed time: 325 sec [0.83316683 0.74545455 0.83420855]\n",
            "Epoch: 12/19, Step: 27/167, Training Loss: 0.1763, Elapsed time: 337 sec [0.84336243 0.80639634 0.68975682]\n",
            "Epoch: 12/19, Step: 28/167, Training Loss: 0.2709, Elapsed time: 350 sec [0.83445787 0.90412148 0.85049834]\n",
            "Epoch: 12/19, Step: 29/167, Training Loss: 0.1689, Elapsed time: 361 sec [0.71199611 0.84566327 0.81338742]\n",
            "Epoch: 12/19, Step: 30/167, Training Loss: 0.1647, Elapsed time: 373 sec [0.79616236 0.6808263  0.68468468]\n",
            "Epoch: 12/19, Step: 31/167, Training Loss: 0.3724, Elapsed time: 386 sec [0.77933557 0.74688057 0.72782411]\n",
            "Epoch: 12/19, Step: 32/167, Training Loss: 0.2527, Elapsed time: 398 sec [0.85845489 0.50409165 0.71505376]\n",
            "Epoch: 12/19, Step: 33/167, Training Loss: 0.2022, Elapsed time: 410 sec [0.88645507 0.84962836 0.84675325]\n",
            "Epoch: 12/19, Step: 34/167, Training Loss: 0.1687, Elapsed time: 423 sec [0.85529768 0.82745651 0.84478372]\n",
            "Epoch: 12/19, Step: 35/167, Training Loss: 0.1268, Elapsed time: 434 sec [0.8428997  0.76628352 0.7226219 ]\n",
            "Epoch: 12/19, Step: 36/167, Training Loss: 0.2060, Elapsed time: 448 sec [0.88174196 0.8628146  0.82501261]\n",
            "Epoch: 12/19, Step: 37/167, Training Loss: 0.2289, Elapsed time: 461 sec [0.85180863 0.75564279 0.65625   ]\n",
            "Epoch: 12/19, Step: 38/167, Training Loss: 0.2602, Elapsed time: 474 sec [0.84112646 0.86771911 0.81940299]\n",
            "Epoch: 12/19, Step: 39/167, Training Loss: 0.0935, Elapsed time: 486 sec [0.84368487 0.71428571 0.68965517]\n",
            "Epoch: 12/19, Step: 40/167, Training Loss: 0.1617, Elapsed time: 497 sec [0.87721022 0.69687964 0.7972028 ]\n",
            "Epoch: 12/19, Step: 41/167, Training Loss: 0.1355, Elapsed time: 509 sec [0.82792087 0.83084404 0.76125245]\n",
            "Epoch: 12/19, Step: 42/167, Training Loss: 0.1657, Elapsed time: 521 sec [0.82943013 0.90294957 0.87338501]\n",
            "Epoch: 12/19, Step: 43/167, Training Loss: 0.1522, Elapsed time: 534 sec [0.88882129 0.85360963 0.82886216]\n",
            "Epoch: 12/19, Step: 44/167, Training Loss: 0.1149, Elapsed time: 546 sec [0.8655914  0.83621838 0.84758942]\n",
            "Epoch: 12/19, Step: 45/167, Training Loss: 0.0804, Elapsed time: 557 sec [0.84918519 0.91876751 0.86965377]\n",
            "Epoch: 12/19, Step: 46/167, Training Loss: 0.1040, Elapsed time: 570 sec [0.84534773 0.86760563 0.77705346]\n",
            "Epoch: 12/19, Step: 47/167, Training Loss: 0.1156, Elapsed time: 582 sec [0.6224239  0.83727599 0.85548173]\n",
            "Epoch: 12/19, Step: 48/167, Training Loss: 0.2378, Elapsed time: 594 sec [0.863103   0.85879518 0.86116984]\n",
            "Epoch: 12/19, Step: 49/167, Training Loss: 0.1560, Elapsed time: 607 sec [0.90948825 0.83847737 0.83300589]\n",
            "Epoch: 12/19, Step: 50/167, Training Loss: 0.2121, Elapsed time: 620 sec [0.86456665 0.76873228 0.79867257]\n",
            "Epoch: 12/19, Step: 51/167, Training Loss: 0.3322, Elapsed time: 632 sec [0.8477842  0.66151468 0.52261307]\n",
            "Epoch: 12/19, Step: 52/167, Training Loss: 0.1620, Elapsed time: 643 sec [0.87544244 0.93250328 0.88132388]\n",
            "Epoch: 12/19, Step: 53/167, Training Loss: 0.1641, Elapsed time: 656 sec [0.841691   0.76418055 0.74089404]\n",
            "Epoch: 12/19, Step: 54/167, Training Loss: 0.6367, Elapsed time: 668 sec [0.88195792 0.79618044 0.70967742]\n",
            "Epoch: 12/19, Step: 55/167, Training Loss: 0.3030, Elapsed time: 681 sec [0.83814433 0.86062881 0.85542971]\n",
            "Epoch: 12/19, Step: 56/167, Training Loss: 0.1362, Elapsed time: 694 sec [0.80244129 0.6048747  0.3954023 ]\n",
            "Epoch: 12/19, Step: 57/167, Training Loss: 0.2401, Elapsed time: 706 sec [0.80017301 0.5645933  0.41057935]\n",
            "Epoch: 12/19, Step: 58/167, Training Loss: 0.3063, Elapsed time: 720 sec [0.73889294 0.67238422 0.78462558]\n",
            "Epoch: 12/19, Step: 59/167, Training Loss: 0.1872, Elapsed time: 731 sec [0.85772607 0.78142077 0.80989181]\n",
            "Epoch: 12/19, Step: 60/167, Training Loss: 0.1784, Elapsed time: 742 sec [0.81709936 0.87836383 0.87865497]\n",
            "Epoch: 12/19, Step: 61/167, Training Loss: 0.1364, Elapsed time: 756 sec [0.81334014 0.83268482 0.79876923]\n",
            "Epoch: 12/19, Step: 62/167, Training Loss: 0.1860, Elapsed time: 768 sec [0.85948589 0.87048193 0.85506287]\n",
            "Epoch: 12/19, Step: 63/167, Training Loss: 0.1714, Elapsed time: 781 sec [0.84795526 0.85681024 0.8358209 ]\n",
            "Epoch: 12/19, Step: 64/167, Training Loss: 0.1467, Elapsed time: 792 sec [0.91498771 0.9431061  0.89261745]\n",
            "Epoch: 12/19, Step: 65/167, Training Loss: 0.1134, Elapsed time: 804 sec [0.85099198 0.79098361 0.87804878]\n",
            "Epoch: 12/19, Step: 66/167, Training Loss: 0.1226, Elapsed time: 817 sec [0.86314954 0.80485232 0.83974933]\n",
            "Epoch: 12/19, Step: 67/167, Training Loss: 0.1574, Elapsed time: 829 sec [0.93992395 0.93561532 0.9197225 ]\n",
            "Epoch: 12/19, Step: 68/167, Training Loss: 0.0963, Elapsed time: 842 sec [0.85923515 0.91294643 0.89121887]\n",
            "Epoch: 12/19, Step: 69/167, Training Loss: 0.1567, Elapsed time: 853 sec [0.91317671 0.88648032 0.89925769]\n",
            "Epoch: 12/19, Step: 70/167, Training Loss: 0.1708, Elapsed time: 865 sec [0.87449892 0.93839219 0.87811104]\n",
            "Epoch: 12/19, Step: 71/167, Training Loss: 0.2883, Elapsed time: 878 sec [0.90277778 0.80981595 0.85429448]\n",
            "Epoch: 12/19, Step: 72/167, Training Loss: 0.1503, Elapsed time: 891 sec [0.87644152 0.8419244  0.78087649]\n",
            "Epoch: 12/19, Step: 73/167, Training Loss: 0.0910, Elapsed time: 905 sec [0.86422473 0.92358079 0.87549564]\n",
            "Epoch: 12/19, Step: 74/167, Training Loss: 0.1123, Elapsed time: 916 sec [0.77944933 0.86099585 0.82323232]\n",
            "Epoch: 12/19, Step: 75/167, Training Loss: 0.1231, Elapsed time: 929 sec [0.83280335 0.89826087 0.66722548]\n",
            "Epoch: 12/19, Step: 76/167, Training Loss: 0.2756, Elapsed time: 942 sec [0.85859657 0.90745192 0.83492063]\n",
            "Epoch: 12/19, Step: 77/167, Training Loss: 0.1245, Elapsed time: 953 sec [0.77634682 0.71962617 0.70672646]\n",
            "Epoch: 12/19, Step: 78/167, Training Loss: 0.2161, Elapsed time: 966 sec [0.76935075 0.69915254 0.85491071]\n",
            "Epoch: 12/19, Step: 79/167, Training Loss: 0.4185, Elapsed time: 978 sec [0.73160251 0.8615917  0.79945982]\n",
            "Epoch: 12/19, Step: 80/167, Training Loss: 0.2065, Elapsed time: 991 sec [0.70848199 0.91135881 0.90090674]\n",
            "Epoch: 12/19, Step: 81/167, Training Loss: 0.2103, Elapsed time: 1005 sec [0.86513017 0.81234445 0.83221477]\n",
            "Epoch: 12/19, Step: 82/167, Training Loss: 0.1507, Elapsed time: 1018 sec [0.66861666 0.74619539 0.74762551]\n",
            "Epoch: 12/19, Step: 83/167, Training Loss: 0.2001, Elapsed time: 1030 sec [0.87425414 0.7150838  0.83793738]\n",
            "Epoch: 12/19, Step: 84/167, Training Loss: 0.1381, Elapsed time: 1044 sec [0.85777532 0.66405944 0.78984127]\n",
            "Epoch: 12/19, Step: 85/167, Training Loss: 0.2661, Elapsed time: 1056 sec [0.88662268 0.73369565 0.79274611]\n",
            "Epoch: 12/19, Step: 86/167, Training Loss: 0.2067, Elapsed time: 1068 sec [0.89068564 0.71931408 0.60017346]\n",
            "Epoch: 12/19, Step: 87/167, Training Loss: 0.3017, Elapsed time: 1080 sec [0.92416525 0.66327827 0.71119134]\n",
            "Epoch: 12/19, Step: 88/167, Training Loss: 0.0951, Elapsed time: 1093 sec [0.85835585 0.81419624 0.85779817]\n",
            "Epoch: 12/19, Step: 89/167, Training Loss: 0.1922, Elapsed time: 1105 sec [0.92882031 0.87193049 0.8784029 ]\n",
            "Epoch: 12/19, Step: 90/167, Training Loss: 0.1589, Elapsed time: 1117 sec [0.91981204 0.80616479 0.86143573]\n",
            "Epoch: 12/19, Step: 91/167, Training Loss: 0.2378, Elapsed time: 1130 sec [0.91127098 0.87909516 0.88597376]\n",
            "Epoch: 12/19, Step: 92/167, Training Loss: 0.1872, Elapsed time: 1144 sec [0.904795   0.79553385 0.86267806]\n",
            "Epoch: 12/19, Step: 93/167, Training Loss: 0.1698, Elapsed time: 1157 sec [0.87810722 0.86441558 0.86468647]\n",
            "Epoch: 12/19, Step: 94/167, Training Loss: 0.1911, Elapsed time: 1169 sec [0.88522279 0.8879892  0.87023977]\n",
            "Epoch: 12/19, Step: 95/167, Training Loss: 0.1526, Elapsed time: 1184 sec [0.84258211 0.86153846 0.82508251]\n",
            "Epoch: 12/19, Step: 96/167, Training Loss: 0.1849, Elapsed time: 1196 sec [0.90759825 0.90227464 0.87217195]\n",
            "Epoch: 12/19, Step: 97/167, Training Loss: 0.1169, Elapsed time: 1209 sec [0.83625837 0.74915483 0.76650246]\n",
            "Epoch: 12/19, Step: 98/167, Training Loss: 0.1252, Elapsed time: 1221 sec [0.8608894  0.82398453 0.81222707]\n",
            "Epoch: 12/19, Step: 99/167, Training Loss: 0.1494, Elapsed time: 1232 sec [0.82859941 0.75260116 0.74295191]\n",
            "Epoch: 12/19, Step: 100/167, Training Loss: 0.2072, Elapsed time: 1245 sec [0.88704193 0.84875558 0.81391304]\n",
            "Epoch: 12/19, Step: 101/167, Training Loss: 0.1363, Elapsed time: 1257 sec [0.9154665  0.91060291 0.87943925]\n",
            "Epoch: 12/19, Step: 102/167, Training Loss: 0.1256, Elapsed time: 1270 sec [0.87066432 0.86731392 0.81915772]\n",
            "Epoch: 12/19, Step: 103/167, Training Loss: 0.0934, Elapsed time: 1282 sec [0.83862069 0.81169273 0.79543459]\n",
            "Epoch: 12/19, Step: 104/167, Training Loss: 0.1150, Elapsed time: 1293 sec [0.8245497  0.84543126 0.83220652]\n",
            "Epoch: 12/19, Step: 105/167, Training Loss: 0.1157, Elapsed time: 1306 sec [0.88497693 0.8567697  0.8458445 ]\n",
            "Epoch: 12/19, Step: 106/167, Training Loss: 0.1758, Elapsed time: 1317 sec [0.91389914 0.82833021 0.74268416]\n",
            "Epoch: 12/19, Step: 107/167, Training Loss: 0.1504, Elapsed time: 1332 sec [0.83038816 0.54618744 0.73038073]\n",
            "Epoch: 12/19, Step: 108/167, Training Loss: 0.5388, Elapsed time: 1344 sec [0.88377499 0.83691236 0.89171023]\n",
            "Epoch: 12/19, Step: 109/167, Training Loss: 0.1303, Elapsed time: 1354 sec [0.90965583 0.87787918 0.79839357]\n",
            "Epoch: 12/19, Step: 110/167, Training Loss: 0.1646, Elapsed time: 1367 sec [0.7393877  0.49108368 0.67292225]\n",
            "Epoch: 12/19, Step: 111/167, Training Loss: 0.1896, Elapsed time: 1377 sec [0.80640466 0.77723699 0.86406744]\n",
            "Epoch: 12/19, Step: 112/167, Training Loss: 0.2522, Elapsed time: 1392 sec [0.83538371 0.75401346 0.87519026]\n",
            "Epoch: 12/19, Step: 113/167, Training Loss: 0.1780, Elapsed time: 1403 sec [0.83722397 0.86898017 0.84805195]\n",
            "Epoch: 12/19, Step: 114/167, Training Loss: 0.1773, Elapsed time: 1415 sec [0.80932515 0.86750917 0.84863222]\n",
            "Epoch: 12/19, Step: 115/167, Training Loss: 0.1397, Elapsed time: 1428 sec [0.84169473 0.82620087 0.8754491 ]\n",
            "Epoch: 12/19, Step: 116/167, Training Loss: 0.1463, Elapsed time: 1440 sec [0.81571429 0.76549865 0.80987654]\n",
            "Epoch: 12/19, Step: 117/167, Training Loss: 0.1345, Elapsed time: 1452 sec [0.83464956 0.8713355  0.86821705]\n",
            "Epoch: 12/19, Step: 118/167, Training Loss: 0.1382, Elapsed time: 1466 sec [0.83504823 0.79174931 0.82880235]\n",
            "Epoch: 12/19, Step: 119/167, Training Loss: 0.2525, Elapsed time: 1478 sec [0.86589951 0.79162248 0.84688581]\n",
            "Epoch: 12/19, Step: 120/167, Training Loss: 0.3267, Elapsed time: 1491 sec [0.82546722 0.8894858  0.85087226]\n",
            "Epoch: 12/19, Step: 121/167, Training Loss: 0.2326, Elapsed time: 1503 sec [0.89179756 0.86078799 0.9097035 ]\n",
            "Epoch: 12/19, Step: 122/167, Training Loss: 0.1313, Elapsed time: 1516 sec [0.77637511 0.85041551 0.82022472]\n",
            "Epoch: 12/19, Step: 123/167, Training Loss: 0.1473, Elapsed time: 1528 sec [0.91096305 0.85806037 0.85235435]\n",
            "Epoch: 12/19, Step: 124/167, Training Loss: 0.0841, Elapsed time: 1540 sec [0.81210828 0.85834032 0.87763713]\n",
            "Epoch: 12/19, Step: 125/167, Training Loss: 0.2283, Elapsed time: 1554 sec [0.86303217 0.87945671 0.8744186 ]\n",
            "Epoch: 12/19, Step: 126/167, Training Loss: 0.1357, Elapsed time: 1566 sec [0.78205834 0.84577114 0.8109589 ]\n",
            "Epoch: 12/19, Step: 127/167, Training Loss: 0.2111, Elapsed time: 1579 sec [0.84809649 0.90513219 0.85692771]\n",
            "Epoch: 12/19, Step: 128/167, Training Loss: 0.2350, Elapsed time: 1590 sec [0.88004704 0.83448643 0.8       ]\n",
            "Epoch: 12/19, Step: 129/167, Training Loss: 0.1877, Elapsed time: 1602 sec [0.90686122 0.87336245 0.818776  ]\n",
            "Epoch: 12/19, Step: 130/167, Training Loss: 0.1310, Elapsed time: 1616 sec [0.87753425 0.92362055 0.89631856]\n",
            "Epoch: 12/19, Step: 131/167, Training Loss: 0.1648, Elapsed time: 1627 sec [0.83120065 0.63122102 0.64606328]\n",
            "Epoch: 12/19, Step: 132/167, Training Loss: 0.3991, Elapsed time: 1640 sec [0.8656535  0.8841906  0.78796296]\n",
            "Epoch: 12/19, Step: 133/167, Training Loss: 0.2775, Elapsed time: 1653 sec [0.79181076 0.69683258 0.62949453]\n",
            "Epoch: 12/19, Step: 134/167, Training Loss: 0.1870, Elapsed time: 1666 sec [0.78870398 0.82632399 0.75606936]\n",
            "Epoch: 12/19, Step: 135/167, Training Loss: 0.1663, Elapsed time: 1679 sec [0.88156863 0.55668995 0.53846154]\n",
            "Epoch: 12/19, Step: 136/167, Training Loss: 0.2081, Elapsed time: 1690 sec [0.8592535  0.7504714  0.71378402]\n",
            "Epoch: 12/19, Step: 137/167, Training Loss: 0.2521, Elapsed time: 1703 sec [0.87566522 0.88016745 0.83368719]\n",
            "Epoch: 12/19, Step: 138/167, Training Loss: 0.0958, Elapsed time: 1716 sec [0.91161179 0.90245123 0.84040093]\n",
            "Epoch: 12/19, Step: 139/167, Training Loss: 0.1164, Elapsed time: 1728 sec [0.88358556 0.87288598 0.87442573]\n",
            "Epoch: 12/19, Step: 140/167, Training Loss: 0.1662, Elapsed time: 1741 sec [0.88399158 0.84722941 0.80673759]\n",
            "Epoch: 12/19, Step: 141/167, Training Loss: 0.1457, Elapsed time: 1755 sec [0.91331353 0.91159136 0.87705957]\n",
            "Epoch: 12/19, Step: 142/167, Training Loss: 0.1044, Elapsed time: 1768 sec [0.89994452 0.79231568 0.8372093 ]\n",
            "Epoch: 12/19, Step: 143/167, Training Loss: 0.2029, Elapsed time: 1781 sec [0.90743603 0.87736967 0.84581498]\n",
            "Epoch: 12/19, Step: 144/167, Training Loss: 0.2311, Elapsed time: 1794 sec [0.86962045 0.76674107 0.80885972]\n",
            "Epoch: 12/19, Step: 145/167, Training Loss: 0.1798, Elapsed time: 1806 sec [0.88720584 0.84923381 0.8709918 ]\n",
            "Epoch: 12/19, Step: 146/167, Training Loss: 0.1774, Elapsed time: 1818 sec [0.79351032 0.82110092 0.86614173]\n",
            "Epoch: 12/19, Step: 147/167, Training Loss: 0.1991, Elapsed time: 1831 sec [0.84235631 0.66260032 0.80439331]\n",
            "Epoch: 12/19, Step: 148/167, Training Loss: 0.1950, Elapsed time: 1843 sec [0.89494729 0.89042208 0.86962277]\n",
            "Epoch: 12/19, Step: 149/167, Training Loss: 0.1186, Elapsed time: 1857 sec [0.85218207 0.86209887 0.76406136]\n",
            "Epoch: 12/19, Step: 150/167, Training Loss: 0.1728, Elapsed time: 1869 sec [0.88844451 0.83371931 0.87400123]\n",
            "Epoch: 12/19, Step: 151/167, Training Loss: 0.1767, Elapsed time: 1881 sec [0.92950093 0.9029361  0.87343672]\n",
            "Epoch: 12/19, Step: 152/167, Training Loss: 0.1170, Elapsed time: 1895 sec [0.85676932 0.79497307 0.71300685]\n",
            "Epoch: 12/19, Step: 153/167, Training Loss: 0.1920, Elapsed time: 1908 sec [0.68849788 0.61946903 0.75561097]\n",
            "Epoch: 12/19, Step: 154/167, Training Loss: 0.2827, Elapsed time: 1921 sec [0.90219636 0.85714286 0.86643836]\n",
            "Epoch: 12/19, Step: 155/167, Training Loss: 0.2071, Elapsed time: 1933 sec [0.90108865 0.89750506 0.90143291]\n",
            "Epoch: 12/19, Step: 156/167, Training Loss: 0.1188, Elapsed time: 1947 sec [0.80811528 0.87853881 0.85640362]\n",
            "Epoch: 12/19, Step: 157/167, Training Loss: 0.0934, Elapsed time: 1958 sec [0.88698752 0.87696577 0.86451613]\n",
            "Epoch: 12/19, Step: 158/167, Training Loss: 0.0899, Elapsed time: 1970 sec [0.90135357 0.94617842 0.90613836]\n",
            "Epoch: 12/19, Step: 159/167, Training Loss: 0.1867, Elapsed time: 1984 sec [0.9186846  0.90929878 0.87988281]\n",
            "Epoch: 12/19, Step: 160/167, Training Loss: 0.1575, Elapsed time: 1996 sec [0.71282501 0.66572558 0.70395137]\n",
            "Epoch: 12/19, Step: 161/167, Training Loss: 0.3136, Elapsed time: 2010 sec [0.90868061 0.8968254  0.88080169]\n",
            "Epoch: 12/19, Step: 162/167, Training Loss: 0.1010, Elapsed time: 2023 sec [0.88182083 0.86425018 0.85795996]\n",
            "Epoch: 12/19, Step: 163/167, Training Loss: 0.1502, Elapsed time: 2035 sec [0.8710839  0.89852941 0.88261351]\n",
            "Epoch: 12/19, Step: 164/167, Training Loss: 0.0907, Elapsed time: 2050 sec [0.83776091 0.74728682 0.69119071]\n",
            "Epoch: 12/19, Step: 165/167, Training Loss: 0.2819, Elapsed time: 2060 sec [0.87269585 0.89607764 0.87466088]\n",
            "Epoch: 12/19, Step: 166/167, Training Loss: 0.1077, Elapsed time: 2072 sec [0.8743215  0.77130529 0.76528302]\n",
            "Epoch: 12/19, Step: 167/167, Training Loss: 0.1654, Elapsed time: 2082 sec \n",
            "train_wt_dice: 0.8531\n",
            "train_ct_dice: 0.8132\n",
            "train_at_dice: 0.8068\n",
            "Epoch: 12/19, Mean Training Loss: 0.1849, Epoch elapsed time: 2082 sec\n",
            "Epoch: 12/19, Validation Step: 22/21, Validation Loss: 0.8758, Elapsed time: 2338 sec \n",
            "val_wt_dice:  0.8646\n",
            "val_ct_dice:    0.8204\n",
            "val_at_dice:    0.8439\n",
            "Epoch: 12/19, Mean Validation Loss: 0.2267, Elapsed time: 2338 sec\n",
            "\n",
            "[0.82732733 0.44115083 0.36363636]\n",
            "Epoch: 13/19, Step: 1/167, Training Loss: 0.1842, Elapsed time: 2348 sec [0.9049568  0.88192771 0.84301075]\n",
            "Epoch: 13/19, Step: 2/167, Training Loss: 0.1550, Elapsed time: 2357 sec [0.90964995 0.90304073 0.87417219]\n",
            "Epoch: 13/19, Step: 3/167, Training Loss: 0.0990, Elapsed time: 2367 sec [0.89201155 0.91202243 0.88119484]\n",
            "Epoch: 13/19, Step: 4/167, Training Loss: 0.1098, Elapsed time: 2375 sec [0.861129   0.82804674 0.86123348]\n",
            "Epoch: 13/19, Step: 5/167, Training Loss: 0.2000, Elapsed time: 2386 sec [0.90540751 0.91806958 0.86425608]\n",
            "Epoch: 13/19, Step: 6/167, Training Loss: 0.1428, Elapsed time: 2395 sec [0.82540738 0.88413879 0.86140089]\n",
            "Epoch: 13/19, Step: 7/167, Training Loss: 0.1687, Elapsed time: 2406 sec [0.89800118 0.79726793 0.84645669]\n",
            "Epoch: 13/19, Step: 8/167, Training Loss: 0.1485, Elapsed time: 2415 sec [0.87117068 0.73469388 0.89037657]\n",
            "Epoch: 13/19, Step: 9/167, Training Loss: 0.1717, Elapsed time: 2425 sec [0.85790646 0.85585586 0.87960688]\n",
            "Epoch: 13/19, Step: 10/167, Training Loss: 0.1289, Elapsed time: 2434 sec [0.89311722 0.84804247 0.825     ]\n",
            "Epoch: 13/19, Step: 11/167, Training Loss: 0.1815, Elapsed time: 2444 sec [0.90126142 0.88737201 0.83165468]\n",
            "Epoch: 13/19, Step: 12/167, Training Loss: 0.1481, Elapsed time: 2452 sec [0.88888889 0.80300633 0.73979281]\n",
            "Epoch: 13/19, Step: 13/167, Training Loss: 0.2570, Elapsed time: 2461 sec [0.91864018 0.8769685  0.81776011]\n",
            "Epoch: 13/19, Step: 14/167, Training Loss: 0.1090, Elapsed time: 2471 sec [0.8312566  0.73298429 0.73305955]\n",
            "Epoch: 13/19, Step: 15/167, Training Loss: 0.1774, Elapsed time: 2482 sec [0.91795044 0.80909717 0.71595331]\n",
            "Epoch: 13/19, Step: 16/167, Training Loss: 0.2511, Elapsed time: 2490 sec [0.72713994 0.59875691 0.64404524]\n",
            "Epoch: 13/19, Step: 17/167, Training Loss: 0.3981, Elapsed time: 2500 sec [0.87630589 0.87198715 0.85139319]\n",
            "Epoch: 13/19, Step: 18/167, Training Loss: 0.1191, Elapsed time: 2510 sec [0.8658156  0.6736     0.78970917]\n",
            "Epoch: 13/19, Step: 19/167, Training Loss: 0.1372, Elapsed time: 2519 sec [0.83383274 0.7440273  0.77008032]\n",
            "Epoch: 13/19, Step: 20/167, Training Loss: 0.1764, Elapsed time: 2527 sec [0.82893411 0.77479066 0.85010862]\n",
            "Epoch: 13/19, Step: 21/167, Training Loss: 0.2025, Elapsed time: 2537 sec [0.90244688 0.74231524 0.78072884]\n",
            "Epoch: 13/19, Step: 22/167, Training Loss: 0.2249, Elapsed time: 2547 sec [0.89090643 0.76224085 0.88204358]\n",
            "Epoch: 13/19, Step: 23/167, Training Loss: 0.1754, Elapsed time: 2556 sec [0.83424209 0.78501629 0.71163366]\n",
            "Epoch: 13/19, Step: 24/167, Training Loss: 0.4105, Elapsed time: 2564 sec [0.8195122  0.87346939 0.86057692]\n",
            "Epoch: 13/19, Step: 25/167, Training Loss: 0.1119, Elapsed time: 2575 sec [0.82370976 0.81877729 0.86122449]\n",
            "Epoch: 13/19, Step: 26/167, Training Loss: 0.1552, Elapsed time: 2584 sec [0.83160708 0.76394052 0.85427911]\n",
            "Epoch: 13/19, Step: 27/167, Training Loss: 0.1738, Elapsed time: 2592 sec [0.84973774 0.79577073 0.68135095]\n",
            "Epoch: 13/19, Step: 28/167, Training Loss: 0.2545, Elapsed time: 2602 sec [0.85068164 0.90901288 0.85128883]\n",
            "Epoch: 13/19, Step: 29/167, Training Loss: 0.1582, Elapsed time: 2612 sec [0.7060241  0.81991395 0.79841897]\n",
            "Epoch: 13/19, Step: 30/167, Training Loss: 0.1618, Elapsed time: 2621 sec [0.80166518 0.69609918 0.75930521]\n",
            "Epoch: 13/19, Step: 31/167, Training Loss: 0.3369, Elapsed time: 2631 sec [0.77553517 0.7238422  0.6955893 ]\n",
            "Epoch: 13/19, Step: 32/167, Training Loss: 0.2596, Elapsed time: 2641 sec [0.86541555 0.52464229 0.68304668]\n",
            "Epoch: 13/19, Step: 33/167, Training Loss: 0.1847, Elapsed time: 2650 sec [0.88893796 0.8428169  0.84067797]\n",
            "Epoch: 13/19, Step: 34/167, Training Loss: 0.1604, Elapsed time: 2661 sec [0.8472713  0.79421076 0.80446927]\n",
            "Epoch: 13/19, Step: 35/167, Training Loss: 0.1224, Elapsed time: 2669 sec [0.84422311 0.77253219 0.73792557]\n",
            "Epoch: 13/19, Step: 36/167, Training Loss: 0.1972, Elapsed time: 2678 sec [0.87957599 0.87672176 0.83470674]\n",
            "Epoch: 13/19, Step: 37/167, Training Loss: 0.2047, Elapsed time: 2689 sec [0.85437645 0.76467734 0.67681159]\n",
            "Epoch: 13/19, Step: 38/167, Training Loss: 0.2509, Elapsed time: 2698 sec [0.84830554 0.89752252 0.84764114]\n",
            "Epoch: 13/19, Step: 39/167, Training Loss: 0.0889, Elapsed time: 2706 sec [0.85805491 0.75437136 0.72202797]\n",
            "Epoch: 13/19, Step: 40/167, Training Loss: 0.1427, Elapsed time: 2716 sec [0.8797048  0.73113208 0.79435958]\n",
            "Epoch: 13/19, Step: 41/167, Training Loss: 0.1256, Elapsed time: 2725 sec [0.84045369 0.85406956 0.77693475]\n",
            "Epoch: 13/19, Step: 42/167, Training Loss: 0.1673, Elapsed time: 2734 sec [0.8370341  0.91248207 0.88235294]\n",
            "Epoch: 13/19, Step: 43/167, Training Loss: 0.1479, Elapsed time: 2744 sec [0.89574697 0.87063655 0.8490566 ]\n",
            "Epoch: 13/19, Step: 44/167, Training Loss: 0.1090, Elapsed time: 2754 sec [0.86876356 0.84996606 0.85894405]\n",
            "Epoch: 13/19, Step: 45/167, Training Loss: 0.0747, Elapsed time: 2764 sec [0.84465171 0.91794159 0.86234818]\n",
            "Epoch: 13/19, Step: 46/167, Training Loss: 0.1024, Elapsed time: 2772 sec [0.82541641 0.88846154 0.80544218]\n",
            "Epoch: 13/19, Step: 47/167, Training Loss: 0.1103, Elapsed time: 2783 sec [0.6333462  0.83838384 0.85953177]\n",
            "Epoch: 13/19, Step: 48/167, Training Loss: 0.2300, Elapsed time: 2790 sec [0.87356322 0.84369115 0.8650904 ]\n",
            "Epoch: 13/19, Step: 49/167, Training Loss: 0.1477, Elapsed time: 2800 sec [0.90606936 0.86196624 0.83742911]\n",
            "Epoch: 13/19, Step: 50/167, Training Loss: 0.1889, Elapsed time: 2811 sec [0.8733932  0.77274549 0.80021954]\n",
            "Epoch: 13/19, Step: 51/167, Training Loss: 0.3232, Elapsed time: 2819 sec [0.84858145 0.66409861 0.55887522]\n",
            "Epoch: 13/19, Step: 52/167, Training Loss: 0.1540, Elapsed time: 2827 sec [0.88121314 0.93215917 0.88702148]\n",
            "Epoch: 13/19, Step: 53/167, Training Loss: 0.1516, Elapsed time: 2837 sec [0.8587599  0.78202355 0.75463744]\n",
            "Epoch: 13/19, Step: 54/167, Training Loss: 0.5885, Elapsed time: 2847 sec [0.88292094 0.80134454 0.7234763 ]\n",
            "Epoch: 13/19, Step: 55/167, Training Loss: 0.2992, Elapsed time: 2856 sec [0.84845325 0.89483934 0.89382373]\n",
            "Epoch: 13/19, Step: 56/167, Training Loss: 0.1236, Elapsed time: 2865 sec [0.81628252 0.64647211 0.43243243]\n",
            "Epoch: 13/19, Step: 57/167, Training Loss: 0.2167, Elapsed time: 2877 sec [0.82667064 0.61817047 0.46766169]\n",
            "Epoch: 13/19, Step: 58/167, Training Loss: 0.2904, Elapsed time: 2885 sec [0.74666179 0.71241535 0.80645161]\n",
            "Epoch: 13/19, Step: 59/167, Training Loss: 0.1711, Elapsed time: 2894 sec [0.85598047 0.79944483 0.81055901]\n",
            "Epoch: 13/19, Step: 60/167, Training Loss: 0.1740, Elapsed time: 2906 sec [0.81467031 0.8719317  0.87019579]\n",
            "Epoch: 13/19, Step: 61/167, Training Loss: 0.1354, Elapsed time: 2915 sec [0.81772719 0.8515873  0.79805943]\n",
            "Epoch: 13/19, Step: 62/167, Training Loss: 0.1821, Elapsed time: 2923 sec [0.87196679 0.87938931 0.84203569]\n",
            "Epoch: 13/19, Step: 63/167, Training Loss: 0.1690, Elapsed time: 2934 sec [0.85122541 0.84345794 0.84859813]\n",
            "Epoch: 13/19, Step: 64/167, Training Loss: 0.1462, Elapsed time: 2943 sec [0.90711998 0.9291498  0.8875817 ]\n",
            "Epoch: 13/19, Step: 65/167, Training Loss: 0.1124, Elapsed time: 2951 sec [0.86632391 0.78571429 0.89163237]\n",
            "Epoch: 13/19, Step: 66/167, Training Loss: 0.1156, Elapsed time: 2961 sec [0.861282   0.80468334 0.83096085]\n",
            "Epoch: 13/19, Step: 67/167, Training Loss: 0.1704, Elapsed time: 2972 sec [0.94356659 0.91835085 0.9194499 ]\n",
            "Epoch: 13/19, Step: 68/167, Training Loss: 0.0876, Elapsed time: 2980 sec [0.86679174 0.90165746 0.88481675]\n",
            "Epoch: 13/19, Step: 69/167, Training Loss: 0.1502, Elapsed time: 2989 sec [0.91048079 0.87069864 0.8528827 ]\n",
            "Epoch: 13/19, Step: 70/167, Training Loss: 0.1584, Elapsed time: 3000 sec [0.87548457 0.93904692 0.86742424]\n",
            "Epoch: 13/19, Step: 71/167, Training Loss: 0.2884, Elapsed time: 3008 sec [0.89727974 0.80487805 0.83885542]\n",
            "Epoch: 13/19, Step: 72/167, Training Loss: 0.1404, Elapsed time: 3018 sec [0.88433868 0.84514436 0.79268293]\n",
            "Epoch: 13/19, Step: 73/167, Training Loss: 0.0913, Elapsed time: 3028 sec [0.85855263 0.92442497 0.87669053]\n",
            "Epoch: 13/19, Step: 74/167, Training Loss: 0.1067, Elapsed time: 3037 sec [0.78433631 0.86554622 0.83076923]\n",
            "Epoch: 13/19, Step: 75/167, Training Loss: 0.1247, Elapsed time: 3047 sec [0.83971252 0.89446367 0.67502089]\n",
            "Epoch: 13/19, Step: 76/167, Training Loss: 0.2607, Elapsed time: 3060 sec [0.86465846 0.92540934 0.86564763]\n",
            "Epoch: 13/19, Step: 77/167, Training Loss: 0.1146, Elapsed time: 3070 sec [0.78583226 0.7735369  0.76801579]\n",
            "Epoch: 13/19, Step: 78/167, Training Loss: 0.1940, Elapsed time: 3077 sec [0.78315377 0.69829013 0.88662132]\n",
            "Epoch: 13/19, Step: 79/167, Training Loss: 0.4311, Elapsed time: 3089 sec [0.74343066 0.87929515 0.80470914]\n",
            "Epoch: 13/19, Step: 80/167, Training Loss: 0.1968, Elapsed time: 3099 sec [0.72622546 0.90186793 0.88476312]\n",
            "Epoch: 13/19, Step: 81/167, Training Loss: 0.2078, Elapsed time: 3109 sec [0.86196707 0.81073025 0.81592689]\n",
            "Epoch: 13/19, Step: 82/167, Training Loss: 0.1456, Elapsed time: 3119 sec [0.67252369 0.77176713 0.74154263]\n",
            "Epoch: 13/19, Step: 83/167, Training Loss: 0.1938, Elapsed time: 3128 sec [0.8715697  0.71419715 0.8267148 ]\n",
            "Epoch: 13/19, Step: 84/167, Training Loss: 0.1289, Elapsed time: 3137 sec [0.84552624 0.66257198 0.7635054 ]\n",
            "Epoch: 13/19, Step: 85/167, Training Loss: 0.2542, Elapsed time: 3146 sec [0.88950385 0.71623405 0.75348274]\n",
            "Epoch: 13/19, Step: 86/167, Training Loss: 0.2101, Elapsed time: 3155 sec [0.89036437 0.72470374 0.59845228]\n",
            "Epoch: 13/19, Step: 87/167, Training Loss: 0.3142, Elapsed time: 3165 sec [0.91882655 0.64938272 0.66778523]\n",
            "Epoch: 13/19, Step: 88/167, Training Loss: 0.1038, Elapsed time: 3173 sec [0.85648592 0.82451545 0.86196319]\n",
            "Epoch: 13/19, Step: 89/167, Training Loss: 0.1851, Elapsed time: 3184 sec [0.93193543 0.86230248 0.86803874]\n",
            "Epoch: 13/19, Step: 90/167, Training Loss: 0.1629, Elapsed time: 3195 sec [0.92389327 0.81314776 0.86636467]\n",
            "Epoch: 13/19, Step: 91/167, Training Loss: 0.2255, Elapsed time: 3205 sec [0.90746523 0.89759503 0.88073394]\n",
            "Epoch: 13/19, Step: 92/167, Training Loss: 0.1771, Elapsed time: 3215 sec [0.90750436 0.80014174 0.86200822]\n",
            "Epoch: 13/19, Step: 93/167, Training Loss: 0.1673, Elapsed time: 3225 sec [0.88363636 0.89768638 0.88135593]\n",
            "Epoch: 13/19, Step: 94/167, Training Loss: 0.1606, Elapsed time: 3235 sec [0.8846534  0.89838129 0.86599424]\n",
            "Epoch: 13/19, Step: 95/167, Training Loss: 0.1552, Elapsed time: 3245 sec [0.85141672 0.86834734 0.82876712]\n",
            "Epoch: 13/19, Step: 96/167, Training Loss: 0.1845, Elapsed time: 3254 sec [0.90294476 0.90498509 0.87299771]\n",
            "Epoch: 13/19, Step: 97/167, Training Loss: 0.1213, Elapsed time: 3264 sec [0.8320432  0.74043716 0.75967413]\n",
            "Epoch: 13/19, Step: 98/167, Training Loss: 0.1338, Elapsed time: 3274 sec [0.85406019 0.81688312 0.81277533]\n",
            "Epoch: 13/19, Step: 99/167, Training Loss: 0.1534, Elapsed time: 3282 sec [0.82621502 0.747785   0.75020747]\n",
            "Epoch: 13/19, Step: 100/167, Training Loss: 0.2143, Elapsed time: 3291 sec [0.88763834 0.85057471 0.81779299]\n",
            "Epoch: 13/19, Step: 101/167, Training Loss: 0.1307, Elapsed time: 3302 sec [0.91363423 0.91600415 0.88609949]\n",
            "Epoch: 13/19, Step: 102/167, Training Loss: 0.1178, Elapsed time: 3311 sec [0.87639133 0.88084266 0.83516484]\n",
            "Epoch: 13/19, Step: 103/167, Training Loss: 0.0955, Elapsed time: 3320 sec [0.84508349 0.83889283 0.80896861]\n",
            "Epoch: 13/19, Step: 104/167, Training Loss: 0.1145, Elapsed time: 3330 sec [0.82651685 0.84908933 0.82614057]\n",
            "Epoch: 13/19, Step: 105/167, Training Loss: 0.1188, Elapsed time: 3341 sec [0.89009314 0.847487   0.82790091]\n",
            "Epoch: 13/19, Step: 106/167, Training Loss: 0.1731, Elapsed time: 3349 sec [0.91444945 0.82151136 0.73046875]\n",
            "Epoch: 13/19, Step: 107/167, Training Loss: 0.1486, Elapsed time: 3360 sec [0.82853717 0.54253927 0.71621622]\n",
            "Epoch: 13/19, Step: 108/167, Training Loss: 0.5337, Elapsed time: 3369 sec [0.88728794 0.86512181 0.89624724]\n",
            "Epoch: 13/19, Step: 109/167, Training Loss: 0.1139, Elapsed time: 3377 sec [0.91576674 0.88156182 0.79873217]\n",
            "Epoch: 13/19, Step: 110/167, Training Loss: 0.1644, Elapsed time: 3385 sec [0.75329468 0.52553664 0.71757925]\n",
            "Epoch: 13/19, Step: 111/167, Training Loss: 0.1742, Elapsed time: 3396 sec [0.8108642  0.77803558 0.84796574]\n",
            "Epoch: 13/19, Step: 112/167, Training Loss: 0.2637, Elapsed time: 3404 sec [0.84852089 0.7657563  0.88293803]\n",
            "Epoch: 13/19, Step: 113/167, Training Loss: 0.1681, Elapsed time: 3414 sec [0.84371029 0.86828919 0.85482782]\n",
            "Epoch: 13/19, Step: 114/167, Training Loss: 0.1798, Elapsed time: 3424 sec [0.81829419 0.86995516 0.85868912]\n",
            "Epoch: 13/19, Step: 115/167, Training Loss: 0.1334, Elapsed time: 3433 sec [0.83806167 0.80440305 0.85880302]\n",
            "Epoch: 13/19, Step: 116/167, Training Loss: 0.1477, Elapsed time: 3442 sec [0.82029194 0.760812   0.81242533]\n",
            "Epoch: 13/19, Step: 117/167, Training Loss: 0.1334, Elapsed time: 3452 sec [0.82748609 0.90174326 0.88721805]\n",
            "Epoch: 13/19, Step: 118/167, Training Loss: 0.1208, Elapsed time: 3462 sec [0.8327044  0.78406872 0.82863341]\n",
            "Epoch: 13/19, Step: 119/167, Training Loss: 0.2544, Elapsed time: 3470 sec [0.86759542 0.79601468 0.85068259]\n",
            "Epoch: 13/19, Step: 120/167, Training Loss: 0.3173, Elapsed time: 3482 sec [0.82634731 0.8754717  0.85763098]\n",
            "Epoch: 13/19, Step: 121/167, Training Loss: 0.2241, Elapsed time: 3492 sec [0.89001889 0.86616541 0.90995261]\n",
            "Epoch: 13/19, Step: 122/167, Training Loss: 0.1329, Elapsed time: 3500 sec [0.79315132 0.86318759 0.82115385]\n",
            "Epoch: 13/19, Step: 123/167, Training Loss: 0.1487, Elapsed time: 3510 sec [0.91557912 0.88580858 0.87530562]\n",
            "Epoch: 13/19, Step: 124/167, Training Loss: 0.0783, Elapsed time: 3520 sec [0.8165088  0.86495875 0.88400488]\n",
            "Epoch: 13/19, Step: 125/167, Training Loss: 0.2451, Elapsed time: 3528 sec [0.87438375 0.90865955 0.89320388]\n",
            "Epoch: 13/19, Step: 126/167, Training Loss: 0.1379, Elapsed time: 3537 sec [0.79909194 0.86697483 0.828125  ]\n",
            "Epoch: 13/19, Step: 127/167, Training Loss: 0.2100, Elapsed time: 3547 sec [0.85231587 0.92154812 0.87272727]\n",
            "Epoch: 13/19, Step: 128/167, Training Loss: 0.2219, Elapsed time: 3557 sec [0.88091936 0.85062466 0.81984334]\n",
            "Epoch: 13/19, Step: 129/167, Training Loss: 0.1951, Elapsed time: 3565 sec [0.90314582 0.87165134 0.81145585]\n",
            "Epoch: 13/19, Step: 130/167, Training Loss: 0.1341, Elapsed time: 3575 sec [0.87597005 0.92126845 0.89419795]\n",
            "Epoch: 13/19, Step: 131/167, Training Loss: 0.1730, Elapsed time: 3584 sec [0.83084376 0.6403698  0.65714286]\n",
            "Epoch: 13/19, Step: 132/167, Training Loss: 0.3650, Elapsed time: 3592 sec [0.85543786 0.89234678 0.79273405]\n",
            "Epoch: 13/19, Step: 133/167, Training Loss: 0.2743, Elapsed time: 3604 sec [0.74198119 0.60903614 0.51890034]\n",
            "Epoch: 13/19, Step: 134/167, Training Loss: 0.2205, Elapsed time: 3614 sec [0.75375279 0.81180529 0.71947195]\n",
            "Epoch: 13/19, Step: 135/167, Training Loss: 0.1781, Elapsed time: 3625 sec [0.87956276 0.58347386 0.56867892]\n",
            "Epoch: 13/19, Step: 136/167, Training Loss: 0.2005, Elapsed time: 3634 sec [0.86806734 0.77904328 0.7337225 ]\n",
            "Epoch: 13/19, Step: 137/167, Training Loss: 0.2494, Elapsed time: 3644 sec [0.88581818 0.89375334 0.84286749]\n",
            "Epoch: 13/19, Step: 138/167, Training Loss: 0.0912, Elapsed time: 3653 sec [0.91755577 0.89275074 0.83460949]\n",
            "Epoch: 13/19, Step: 139/167, Training Loss: 0.1101, Elapsed time: 3664 sec [0.88254535 0.86534216 0.86499215]\n",
            "Epoch: 13/19, Step: 140/167, Training Loss: 0.1671, Elapsed time: 3673 sec [0.8921877  0.85573174 0.80508103]\n",
            "Epoch: 13/19, Step: 141/167, Training Loss: 0.1453, Elapsed time: 3684 sec [0.91979195 0.88957055 0.84913218]\n",
            "Epoch: 13/19, Step: 142/167, Training Loss: 0.1164, Elapsed time: 3694 sec [0.89865735 0.79895013 0.83134583]\n",
            "Epoch: 13/19, Step: 143/167, Training Loss: 0.1986, Elapsed time: 3703 sec [0.91499227 0.87309353 0.79256966]\n",
            "Epoch: 13/19, Step: 144/167, Training Loss: 0.2159, Elapsed time: 3712 sec [0.87393095 0.76353887 0.80811232]\n",
            "Epoch: 13/19, Step: 145/167, Training Loss: 0.1704, Elapsed time: 3723 sec [0.89391681 0.84911243 0.84857352]\n",
            "Epoch: 13/19, Step: 146/167, Training Loss: 0.1707, Elapsed time: 3730 sec [0.79874629 0.83807746 0.86826736]\n",
            "Epoch: 13/19, Step: 147/167, Training Loss: 0.1999, Elapsed time: 3740 sec [0.85372051 0.68393094 0.8100471 ]\n",
            "Epoch: 13/19, Step: 148/167, Training Loss: 0.1885, Elapsed time: 3751 sec [0.9029304  0.89769511 0.87665782]\n",
            "Epoch: 13/19, Step: 149/167, Training Loss: 0.1082, Elapsed time: 3760 sec [0.8564728  0.86516854 0.78041543]\n",
            "Epoch: 13/19, Step: 150/167, Training Loss: 0.1570, Elapsed time: 3770 sec [0.88656845 0.84514925 0.86539644]\n",
            "Epoch: 13/19, Step: 151/167, Training Loss: 0.1665, Elapsed time: 3782 sec [0.93687231 0.91278459 0.88514548]\n",
            "Epoch: 13/19, Step: 152/167, Training Loss: 0.1133, Elapsed time: 3792 sec [0.87457122 0.86252946 0.80118694]\n",
            "Epoch: 13/19, Step: 153/167, Training Loss: 0.1798, Elapsed time: 3800 sec [0.70340249 0.64150943 0.76395939]\n",
            "Epoch: 13/19, Step: 154/167, Training Loss: 0.2747, Elapsed time: 3810 sec [0.90712823 0.84924827 0.86228218]\n",
            "Epoch: 13/19, Step: 155/167, Training Loss: 0.1937, Elapsed time: 3819 sec [0.90185759 0.90342052 0.90562771]\n",
            "Epoch: 13/19, Step: 156/167, Training Loss: 0.1139, Elapsed time: 3829 sec [0.79659133 0.86631016 0.84489281]\n",
            "Epoch: 13/19, Step: 157/167, Training Loss: 0.0910, Elapsed time: 3837 sec [0.89102564 0.87932647 0.85466377]\n",
            "Epoch: 13/19, Step: 158/167, Training Loss: 0.0934, Elapsed time: 3849 sec [0.89824777 0.94403535 0.90809061]\n",
            "Epoch: 13/19, Step: 159/167, Training Loss: 0.1840, Elapsed time: 3859 sec [0.92527538 0.92098299 0.89007782]\n",
            "Epoch: 13/19, Step: 160/167, Training Loss: 0.1384, Elapsed time: 3866 sec [0.72789725 0.68446026 0.69392813]\n",
            "Epoch: 13/19, Step: 161/167, Training Loss: 0.3107, Elapsed time: 3878 sec [0.90783755 0.89881735 0.87578288]\n",
            "Epoch: 13/19, Step: 162/167, Training Loss: 0.1038, Elapsed time: 3888 sec [0.88364642 0.85932086 0.85687558]\n",
            "Epoch: 13/19, Step: 163/167, Training Loss: 0.1466, Elapsed time: 3897 sec [0.87409812 0.89819919 0.87582418]\n",
            "Epoch: 13/19, Step: 164/167, Training Loss: 0.0888, Elapsed time: 3909 sec [0.85404716 0.77058353 0.73207931]\n",
            "Epoch: 13/19, Step: 165/167, Training Loss: 0.2666, Elapsed time: 3918 sec [0.87119908 0.8871939  0.86839266]\n",
            "Epoch: 13/19, Step: 166/167, Training Loss: 0.1084, Elapsed time: 3927 sec [0.87755532 0.78582931 0.76666667]\n",
            "Epoch: 13/19, Step: 167/167, Training Loss: 0.1603, Elapsed time: 3935 sec \n",
            "train_wt_dice: 0.8559\n",
            "train_ct_dice: 0.8183\n",
            "train_at_dice: 0.8096\n",
            "Epoch: 13/19, Mean Training Loss: 0.1801, Epoch elapsed time: 1597 sec\n",
            "Epoch: 13/19, Validation Step: 22/21, Validation Loss: 0.8310, Elapsed time: 4134 sec \n",
            "val_wt_dice:  0.8693\n",
            "val_ct_dice:    0.8263\n",
            "val_at_dice:    0.851\n",
            "Epoch: 13/19, Mean Validation Loss: 0.2252, Elapsed time: 1795 sec\n",
            "\n",
            "[0.85319693 0.51076321 0.43856333]\n",
            "Epoch: 14/19, Step: 1/167, Training Loss: 0.1622, Elapsed time: 4143 sec [0.91133896 0.89824561 0.86132178]\n",
            "Epoch: 14/19, Step: 2/167, Training Loss: 0.1387, Elapsed time: 4153 sec [0.91223909 0.90552995 0.87899035]\n",
            "Epoch: 14/19, Step: 3/167, Training Loss: 0.1028, Elapsed time: 4162 sec [0.89264452 0.92469352 0.89583333]\n",
            "Epoch: 14/19, Step: 4/167, Training Loss: 0.0992, Elapsed time: 4171 sec [0.85603782 0.82973893 0.86256983]\n",
            "Epoch: 14/19, Step: 5/167, Training Loss: 0.2021, Elapsed time: 4182 sec [0.90488751 0.92164179 0.86588235]\n",
            "Epoch: 14/19, Step: 6/167, Training Loss: 0.1365, Elapsed time: 4193 sec [0.82041963 0.87336775 0.86303387]\n",
            "Epoch: 14/19, Step: 7/167, Training Loss: 0.1620, Elapsed time: 4203 sec [0.89803115 0.8025     0.84508899]\n",
            "Epoch: 14/19, Step: 8/167, Training Loss: 0.1471, Elapsed time: 4213 sec [0.86595138 0.76586667 0.89765101]\n",
            "Epoch: 14/19, Step: 9/167, Training Loss: 0.1630, Elapsed time: 4222 sec [0.87344913 0.85094851 0.88997555]\n",
            "Epoch: 14/19, Step: 10/167, Training Loss: 0.1188, Elapsed time: 4231 sec [0.89402428 0.85409727 0.8285124 ]\n",
            "Epoch: 14/19, Step: 11/167, Training Loss: 0.1568, Elapsed time: 4242 sec [0.89707476 0.89581951 0.83734088]\n",
            "Epoch: 14/19, Step: 12/167, Training Loss: 0.1345, Elapsed time: 4249 sec [0.90334962 0.84160381 0.76719243]\n",
            "Epoch: 14/19, Step: 13/167, Training Loss: 0.2234, Elapsed time: 4259 sec [0.91768293 0.88866499 0.84368601]\n",
            "Epoch: 14/19, Step: 14/167, Training Loss: 0.1042, Elapsed time: 4270 sec [0.83713008 0.76311031 0.7752443 ]\n",
            "Epoch: 14/19, Step: 15/167, Training Loss: 0.1633, Elapsed time: 4278 sec [0.91308691 0.81286752 0.71196911]\n",
            "Epoch: 14/19, Step: 16/167, Training Loss: 0.2533, Elapsed time: 4287 sec [0.73064469 0.62066621 0.65306122]\n",
            "Epoch: 14/19, Step: 17/167, Training Loss: 0.3780, Elapsed time: 4298 sec [0.88488907 0.87553648 0.84839204]\n",
            "Epoch: 14/19, Step: 18/167, Training Loss: 0.1111, Elapsed time: 4307 sec [0.8779661  0.67749797 0.77704194]\n",
            "Epoch: 14/19, Step: 19/167, Training Loss: 0.1358, Elapsed time: 4316 sec [0.81986237 0.73867792 0.7745491 ]\n",
            "Epoch: 14/19, Step: 20/167, Training Loss: 0.1717, Elapsed time: 4326 sec [0.83486052 0.77145359 0.85465116]\n",
            "Epoch: 14/19, Step: 21/167, Training Loss: 0.2058, Elapsed time: 4337 sec [0.90434783 0.73158066 0.7622549 ]\n",
            "Epoch: 14/19, Step: 22/167, Training Loss: 0.2264, Elapsed time: 4345 sec [0.88371416 0.76321635 0.8776435 ]\n",
            "Epoch: 14/19, Step: 23/167, Training Loss: 0.1783, Elapsed time: 4355 sec [0.83528775 0.78805237 0.71064095]\n",
            "Epoch: 14/19, Step: 24/167, Training Loss: 0.3918, Elapsed time: 4366 sec [0.82684186 0.8872428  0.89493201]\n",
            "Epoch: 14/19, Step: 25/167, Training Loss: 0.1061, Elapsed time: 4374 sec [0.84216028 0.81297297 0.85424073]\n",
            "Epoch: 14/19, Step: 26/167, Training Loss: 0.1543, Elapsed time: 4384 sec [0.84260037 0.76930233 0.86615385]\n",
            "Epoch: 14/19, Step: 27/167, Training Loss: 0.1695, Elapsed time: 4394 sec [0.85736506 0.81753079 0.72885375]\n",
            "Epoch: 14/19, Step: 28/167, Training Loss: 0.2516, Elapsed time: 4404 sec [0.84610473 0.92033087 0.87432432]\n",
            "Epoch: 14/19, Step: 29/167, Training Loss: 0.1583, Elapsed time: 4412 sec [0.73095179 0.84270373 0.83162218]\n",
            "Epoch: 14/19, Step: 30/167, Training Loss: 0.1615, Elapsed time: 4423 sec [0.79935701 0.69375736 0.73659118]\n",
            "Epoch: 14/19, Step: 31/167, Training Loss: 0.3281, Elapsed time: 4432 sec [0.77166988 0.72060683 0.68718683]\n",
            "Epoch: 14/19, Step: 32/167, Training Loss: 0.2460, Elapsed time: 4441 sec [0.86338216 0.5        0.695     ]\n",
            "Epoch: 14/19, Step: 33/167, Training Loss: 0.2029, Elapsed time: 4451 sec [0.88570168 0.86557005 0.85490877]\n",
            "Epoch: 14/19, Step: 34/167, Training Loss: 0.1618, Elapsed time: 4461 sec [0.85241379 0.82013048 0.8358209 ]\n",
            "Epoch: 14/19, Step: 35/167, Training Loss: 0.1216, Elapsed time: 4471 sec [0.84575321 0.76877761 0.7329591 ]\n",
            "Epoch: 14/19, Step: 36/167, Training Loss: 0.2053, Elapsed time: 4482 sec [0.88232386 0.86139667 0.82306163]\n",
            "Epoch: 14/19, Step: 37/167, Training Loss: 0.2235, Elapsed time: 4491 sec [0.85191604 0.7590536  0.65223665]\n",
            "Epoch: 14/19, Step: 38/167, Training Loss: 0.2522, Elapsed time: 4501 sec [0.85025449 0.87880465 0.82967449]\n",
            "Epoch: 14/19, Step: 39/167, Training Loss: 0.0836, Elapsed time: 4510 sec [0.85139384 0.72206759 0.70851247]\n",
            "Epoch: 14/19, Step: 40/167, Training Loss: 0.1481, Elapsed time: 4520 sec [0.87952698 0.74739374 0.79532164]\n",
            "Epoch: 14/19, Step: 41/167, Training Loss: 0.1208, Elapsed time: 4529 sec [0.83374129 0.8372093  0.75516224]\n",
            "Epoch: 14/19, Step: 42/167, Training Loss: 0.1515, Elapsed time: 4537 sec [0.83688784 0.90359168 0.8715655 ]\n",
            "Epoch: 14/19, Step: 43/167, Training Loss: 0.1482, Elapsed time: 4548 sec [0.8949464  0.85456971 0.82944673]\n",
            "Epoch: 14/19, Step: 44/167, Training Loss: 0.1085, Elapsed time: 4558 sec [0.86515642 0.84636119 0.84782609]\n",
            "Epoch: 14/19, Step: 45/167, Training Loss: 0.0764, Elapsed time: 4567 sec [0.84814597 0.92125436 0.87184662]\n",
            "Epoch: 14/19, Step: 46/167, Training Loss: 0.0983, Elapsed time: 4577 sec [0.83291615 0.86440678 0.79194631]\n",
            "Epoch: 14/19, Step: 47/167, Training Loss: 0.1088, Elapsed time: 4586 sec [0.62143122 0.81742739 0.82747604]\n",
            "Epoch: 14/19, Step: 48/167, Training Loss: 0.2305, Elapsed time: 4595 sec [0.86422553 0.85727924 0.87219101]\n",
            "Epoch: 14/19, Step: 49/167, Training Loss: 0.1459, Elapsed time: 4604 sec [0.90820383 0.867      0.85245902]\n",
            "Epoch: 14/19, Step: 50/167, Training Loss: 0.1860, Elapsed time: 4616 sec [0.86518047 0.76440207 0.79082469]\n",
            "Epoch: 14/19, Step: 51/167, Training Loss: 0.3212, Elapsed time: 4625 sec [0.84073604 0.67449139 0.53198653]\n",
            "Epoch: 14/19, Step: 52/167, Training Loss: 0.1496, Elapsed time: 4635 sec [0.88295896 0.93017128 0.89300999]\n",
            "Epoch: 14/19, Step: 53/167, Training Loss: 0.1521, Elapsed time: 4643 sec [0.85424845 0.77818538 0.76387097]\n",
            "Epoch: 14/19, Step: 54/167, Training Loss: 0.6034, Elapsed time: 4653 sec [0.89403573 0.80749574 0.73333333]\n",
            "Epoch: 14/19, Step: 55/167, Training Loss: 0.2954, Elapsed time: 4664 sec [0.85008696 0.88953488 0.88033012]\n",
            "Epoch: 14/19, Step: 56/167, Training Loss: 0.1216, Elapsed time: 4673 sec [0.8195438  0.63805436 0.41959184]\n",
            "Epoch: 14/19, Step: 57/167, Training Loss: 0.2132, Elapsed time: 4682 sec [0.84546148 0.65235772 0.47816432]\n",
            "Epoch: 14/19, Step: 58/167, Training Loss: 0.2865, Elapsed time: 4694 sec [0.73680392 0.659699   0.78143133]\n",
            "Epoch: 14/19, Step: 59/167, Training Loss: 0.1782, Elapsed time: 4703 sec [0.85581207 0.79189189 0.80902256]\n",
            "Epoch: 14/19, Step: 60/167, Training Loss: 0.1749, Elapsed time: 4711 sec [0.79599168 0.86614173 0.86657303]\n",
            "Epoch: 14/19, Step: 61/167, Training Loss: 0.1307, Elapsed time: 4722 sec [0.80836707 0.83722741 0.7966607 ]\n",
            "Epoch: 14/19, Step: 62/167, Training Loss: 0.1762, Elapsed time: 4731 sec [0.85219832 0.86423467 0.84086444]\n",
            "Epoch: 14/19, Step: 63/167, Training Loss: 0.1739, Elapsed time: 4739 sec [0.84676037 0.84869976 0.83116883]\n",
            "Epoch: 14/19, Step: 64/167, Training Loss: 0.1386, Elapsed time: 4750 sec [0.91312836 0.93535354 0.9009247 ]\n",
            "Epoch: 14/19, Step: 65/167, Training Loss: 0.1047, Elapsed time: 4761 sec [0.86237006 0.78895464 0.88157895]\n",
            "Epoch: 14/19, Step: 66/167, Training Loss: 0.1107, Elapsed time: 4770 sec [0.88322632 0.83489785 0.86248831]\n",
            "Epoch: 14/19, Step: 67/167, Training Loss: 0.1616, Elapsed time: 4779 sec [0.94691608 0.91599354 0.91566265]\n",
            "Epoch: 14/19, Step: 68/167, Training Loss: 0.0905, Elapsed time: 4789 sec [0.85963478 0.93318233 0.90691489]\n",
            "Epoch: 14/19, Step: 69/167, Training Loss: 0.1504, Elapsed time: 4798 sec [0.9054665  0.88668867 0.8971368 ]\n",
            "Epoch: 14/19, Step: 70/167, Training Loss: 0.1639, Elapsed time: 4806 sec [0.86955163 0.93593106 0.86319846]\n",
            "Epoch: 14/19, Step: 71/167, Training Loss: 0.2967, Elapsed time: 4817 sec [0.91311475 0.82971429 0.88262911]\n",
            "Epoch: 14/19, Step: 72/167, Training Loss: 0.1431, Elapsed time: 4826 sec [0.8896284  0.84716157 0.79505664]\n",
            "Epoch: 14/19, Step: 73/167, Training Loss: 0.0880, Elapsed time: 4835 sec [0.86852368 0.93056315 0.8752    ]\n",
            "Epoch: 14/19, Step: 74/167, Training Loss: 0.1075, Elapsed time: 4845 sec [0.80058997 0.87835052 0.86259542]\n",
            "Epoch: 14/19, Step: 75/167, Training Loss: 0.1109, Elapsed time: 4855 sec [0.84997457 0.90995671 0.69782972]\n",
            "Epoch: 14/19, Step: 76/167, Training Loss: 0.2573, Elapsed time: 4865 sec [0.86973099 0.92391304 0.85782748]\n",
            "Epoch: 14/19, Step: 77/167, Training Loss: 0.1156, Elapsed time: 4875 sec [0.78055858 0.7519902  0.73504274]\n",
            "Epoch: 14/19, Step: 78/167, Training Loss: 0.2033, Elapsed time: 4884 sec [0.78250683 0.71559633 0.88636364]\n",
            "Epoch: 14/19, Step: 79/167, Training Loss: 0.3884, Elapsed time: 4893 sec [0.73262032 0.87394247 0.8172888 ]\n",
            "Epoch: 14/19, Step: 80/167, Training Loss: 0.1824, Elapsed time: 4905 sec [0.71944283 0.90899443 0.8951951 ]\n",
            "Epoch: 14/19, Step: 81/167, Training Loss: 0.2049, Elapsed time: 4915 sec [0.86040127 0.81822684 0.82516556]\n",
            "Epoch: 14/19, Step: 82/167, Training Loss: 0.1435, Elapsed time: 4925 sec [0.67817084 0.8042328  0.77793296]\n",
            "Epoch: 14/19, Step: 83/167, Training Loss: 0.1835, Elapsed time: 4936 sec [0.87444934 0.7207995  0.840553  ]\n",
            "Epoch: 14/19, Step: 84/167, Training Loss: 0.1333, Elapsed time: 4944 sec [0.85639615 0.70544859 0.78937381]\n",
            "Epoch: 14/19, Step: 85/167, Training Loss: 0.2395, Elapsed time: 4953 sec [0.88835726 0.74896361 0.79666453]\n",
            "Epoch: 14/19, Step: 86/167, Training Loss: 0.1933, Elapsed time: 4964 sec [0.893049   0.74427302 0.61702128]\n",
            "Epoch: 14/19, Step: 87/167, Training Loss: 0.3042, Elapsed time: 4972 sec [0.91128338 0.70526316 0.67549669]\n",
            "Epoch: 14/19, Step: 88/167, Training Loss: 0.1026, Elapsed time: 4981 sec [0.86915177 0.83661522 0.87987758]\n",
            "Epoch: 14/19, Step: 89/167, Training Loss: 0.1745, Elapsed time: 4991 sec [0.93213273 0.87495249 0.88984088]\n",
            "Epoch: 14/19, Step: 90/167, Training Loss: 0.1554, Elapsed time: 5002 sec [0.92900485 0.8202847  0.87258248]\n",
            "Epoch: 14/19, Step: 91/167, Training Loss: 0.2281, Elapsed time: 5010 sec [0.91554294 0.88687426 0.88205128]\n",
            "Epoch: 14/19, Step: 92/167, Training Loss: 0.1895, Elapsed time: 5020 sec [0.91504642 0.79971691 0.87419544]\n",
            "Epoch: 14/19, Step: 93/167, Training Loss: 0.1658, Elapsed time: 5031 sec [0.89090909 0.8937247  0.86696231]\n",
            "Epoch: 14/19, Step: 94/167, Training Loss: 0.1562, Elapsed time: 5042 sec [0.88847952 0.89189189 0.87902649]\n",
            "Epoch: 14/19, Step: 95/167, Training Loss: 0.1512, Elapsed time: 5051 sec [0.85119048 0.87978142 0.83654652]\n",
            "Epoch: 14/19, Step: 96/167, Training Loss: 0.1684, Elapsed time: 5062 sec [0.89813686 0.90101523 0.87054833]\n",
            "Epoch: 14/19, Step: 97/167, Training Loss: 0.1195, Elapsed time: 5071 sec [0.83604189 0.74412357 0.77386935]\n",
            "Epoch: 14/19, Step: 98/167, Training Loss: 0.1235, Elapsed time: 5080 sec [0.85885031 0.81088083 0.80752212]\n",
            "Epoch: 14/19, Step: 99/167, Training Loss: 0.1595, Elapsed time: 5090 sec [0.83223443 0.7587822  0.75207987]\n",
            "Epoch: 14/19, Step: 100/167, Training Loss: 0.1924, Elapsed time: 5099 sec [0.88176427 0.85384615 0.81332165]\n",
            "Epoch: 14/19, Step: 101/167, Training Loss: 0.1317, Elapsed time: 5108 sec [0.91386587 0.91122625 0.8844732 ]\n",
            "Epoch: 14/19, Step: 102/167, Training Loss: 0.1163, Elapsed time: 5118 sec [0.87218485 0.86045016 0.82107023]\n",
            "Epoch: 14/19, Step: 103/167, Training Loss: 0.0914, Elapsed time: 5128 sec [0.83585511 0.8372093  0.82533937]\n",
            "Epoch: 14/19, Step: 104/167, Training Loss: 0.1133, Elapsed time: 5137 sec [0.82229654 0.84725227 0.82839506]\n",
            "Epoch: 14/19, Step: 105/167, Training Loss: 0.1184, Elapsed time: 5146 sec [0.88896481 0.86810763 0.85083612]\n",
            "Epoch: 14/19, Step: 106/167, Training Loss: 0.1697, Elapsed time: 5155 sec [0.91838417 0.84240688 0.76476684]\n",
            "Epoch: 14/19, Step: 107/167, Training Loss: 0.1380, Elapsed time: 5166 sec [0.844      0.56956372 0.74704492]\n",
            "Epoch: 14/19, Step: 108/167, Training Loss: 0.5038, Elapsed time: 5178 sec [0.89007669 0.87294833 0.88573562]\n",
            "Epoch: 14/19, Step: 109/167, Training Loss: 0.1176, Elapsed time: 5185 sec [0.91609223 0.87899741 0.79591837]\n",
            "Epoch: 14/19, Step: 110/167, Training Loss: 0.1564, Elapsed time: 5194 sec [0.77943166 0.56490952 0.73198847]\n",
            "Epoch: 14/19, Step: 111/167, Training Loss: 0.1630, Elapsed time: 5202 sec [0.80511492 0.77760968 0.85789752]\n",
            "Epoch: 14/19, Step: 112/167, Training Loss: 0.2516, Elapsed time: 5214 sec [0.84982227 0.75518672 0.8698941 ]\n",
            "Epoch: 14/19, Step: 113/167, Training Loss: 0.1654, Elapsed time: 5224 sec [0.85201081 0.86256591 0.86067708]\n",
            "Epoch: 14/19, Step: 114/167, Training Loss: 0.1682, Elapsed time: 5233 sec [0.80067568 0.86508256 0.8519174 ]\n",
            "Epoch: 14/19, Step: 115/167, Training Loss: 0.1366, Elapsed time: 5243 sec [0.84255319 0.82475119 0.87119438]\n",
            "Epoch: 14/19, Step: 116/167, Training Loss: 0.1391, Elapsed time: 5252 sec [0.82462865 0.76678445 0.82624544]\n",
            "Epoch: 14/19, Step: 117/167, Training Loss: 0.1274, Elapsed time: 5262 sec [0.83560311 0.88997555 0.88423374]\n",
            "Epoch: 14/19, Step: 118/167, Training Loss: 0.1281, Elapsed time: 5273 sec [0.84079923 0.80382014 0.83054545]\n",
            "Epoch: 14/19, Step: 119/167, Training Loss: 0.2415, Elapsed time: 5281 sec [0.87055053 0.78830593 0.86063783]\n",
            "Epoch: 14/19, Step: 120/167, Training Loss: 0.3165, Elapsed time: 5291 sec [0.82091668 0.89136276 0.86666667]\n",
            "Epoch: 14/19, Step: 121/167, Training Loss: 0.2156, Elapsed time: 5301 sec [0.89018182 0.86382497 0.90485969]\n",
            "Epoch: 14/19, Step: 122/167, Training Loss: 0.1340, Elapsed time: 5310 sec [0.78661471 0.85174825 0.82352941]\n",
            "Epoch: 14/19, Step: 123/167, Training Loss: 0.1462, Elapsed time: 5319 sec [0.91323648 0.87069526 0.86525974]\n",
            "Epoch: 14/19, Step: 124/167, Training Loss: 0.0809, Elapsed time: 5332 sec [0.81622273 0.87615838 0.87439614]\n",
            "Epoch: 14/19, Step: 125/167, Training Loss: 0.2280, Elapsed time: 5342 sec [0.86158631 0.9020979  0.88643533]\n",
            "Epoch: 14/19, Step: 126/167, Training Loss: 0.1382, Elapsed time: 5350 sec [0.79392265 0.85318833 0.8109215 ]\n",
            "Epoch: 14/19, Step: 127/167, Training Loss: 0.1997, Elapsed time: 5362 sec [0.85763228 0.92944297 0.87604563]\n",
            "Epoch: 14/19, Step: 128/167, Training Loss: 0.2184, Elapsed time: 5371 sec [0.87414098 0.85900217 0.82108003]\n",
            "Epoch: 14/19, Step: 129/167, Training Loss: 0.1873, Elapsed time: 5379 sec [0.91107304 0.88469265 0.83464567]\n",
            "Epoch: 14/19, Step: 130/167, Training Loss: 0.1226, Elapsed time: 5391 sec [0.8758508  0.92580469 0.89479681]\n",
            "Epoch: 14/19, Step: 131/167, Training Loss: 0.1711, Elapsed time: 5400 sec [0.83895853 0.63353639 0.70152856]\n",
            "Epoch: 14/19, Step: 132/167, Training Loss: 0.3922, Elapsed time: 5410 sec [0.86227184 0.88732394 0.78486998]\n",
            "Epoch: 14/19, Step: 133/167, Training Loss: 0.2772, Elapsed time: 5421 sec [0.76429235 0.65359897 0.56674473]\n",
            "Epoch: 14/19, Step: 134/167, Training Loss: 0.2007, Elapsed time: 5431 sec [0.76880405 0.82384824 0.74360432]\n",
            "Epoch: 14/19, Step: 135/167, Training Loss: 0.1724, Elapsed time: 5440 sec [0.88317298 0.56022099 0.55294118]\n",
            "Epoch: 14/19, Step: 136/167, Training Loss: 0.2147, Elapsed time: 5452 sec [0.86314433 0.76331547 0.71982571]\n",
            "Epoch: 14/19, Step: 137/167, Training Loss: 0.2382, Elapsed time: 5461 sec [0.88148685 0.89230769 0.84599006]\n",
            "Epoch: 14/19, Step: 138/167, Training Loss: 0.0879, Elapsed time: 5471 sec [0.91333719 0.90110999 0.83925811]\n",
            "Epoch: 14/19, Step: 139/167, Training Loss: 0.1097, Elapsed time: 5483 sec [0.88536155 0.86590038 0.87017002]\n",
            "Epoch: 14/19, Step: 140/167, Training Loss: 0.1584, Elapsed time: 5492 sec [0.89360627 0.86238532 0.82315978]\n",
            "Epoch: 14/19, Step: 141/167, Training Loss: 0.1448, Elapsed time: 5502 sec [0.92316047 0.91525424 0.89058524]\n",
            "Epoch: 14/19, Step: 142/167, Training Loss: 0.1035, Elapsed time: 5513 sec [0.89725143 0.80187696 0.82893617]\n",
            "Epoch: 14/19, Step: 143/167, Training Loss: 0.1984, Elapsed time: 5523 sec [0.91734535 0.88288817 0.8453159 ]\n",
            "Epoch: 14/19, Step: 144/167, Training Loss: 0.2241, Elapsed time: 5532 sec [0.87747662 0.77945282 0.82042834]\n",
            "Epoch: 14/19, Step: 145/167, Training Loss: 0.1743, Elapsed time: 5543 sec [0.89383655 0.86511156 0.86384266]\n",
            "Epoch: 14/19, Step: 146/167, Training Loss: 0.1700, Elapsed time: 5552 sec [0.79815456 0.83155452 0.87023886]\n",
            "Epoch: 14/19, Step: 147/167, Training Loss: 0.1977, Elapsed time: 5561 sec [0.853076   0.6863984  0.82851018]\n",
            "Epoch: 14/19, Step: 148/167, Training Loss: 0.1844, Elapsed time: 5572 sec [0.89932642 0.9025974  0.87723955]\n",
            "Epoch: 14/19, Step: 149/167, Training Loss: 0.1078, Elapsed time: 5582 sec [0.86240264 0.87757789 0.78747204]\n",
            "Epoch: 14/19, Step: 150/167, Training Loss: 0.1549, Elapsed time: 5592 sec [0.88617769 0.83838384 0.87128713]\n",
            "Epoch: 14/19, Step: 151/167, Training Loss: 0.1755, Elapsed time: 5604 sec [0.93260994 0.9051397  0.87357462]\n",
            "Epoch: 14/19, Step: 152/167, Training Loss: 0.1120, Elapsed time: 5613 sec [0.87016615 0.85207547 0.78843931]\n",
            "Epoch: 14/19, Step: 153/167, Training Loss: 0.1692, Elapsed time: 5622 sec [0.70262343 0.63870968 0.7688378 ]\n",
            "Epoch: 14/19, Step: 154/167, Training Loss: 0.2601, Elapsed time: 5634 sec [0.91319637 0.8602762  0.86202965]\n",
            "Epoch: 14/19, Step: 155/167, Training Loss: 0.1907, Elapsed time: 5642 sec [0.90599625 0.89662094 0.9038961 ]\n",
            "Epoch: 14/19, Step: 156/167, Training Loss: 0.1133, Elapsed time: 5652 sec [0.80473595 0.88294931 0.85237484]\n",
            "Epoch: 14/19, Step: 157/167, Training Loss: 0.0942, Elapsed time: 5663 sec [0.88653982 0.86267281 0.84516129]\n",
            "Epoch: 14/19, Step: 158/167, Training Loss: 0.0891, Elapsed time: 5671 sec [0.90039759 0.94731707 0.91084028]\n",
            "Epoch: 14/19, Step: 159/167, Training Loss: 0.1753, Elapsed time: 5681 sec [0.92266348 0.91641337 0.88180481]\n",
            "Epoch: 14/19, Step: 160/167, Training Loss: 0.1430, Elapsed time: 5692 sec [0.68961657 0.63660907 0.67979155]\n",
            "Epoch: 14/19, Step: 161/167, Training Loss: 0.3263, Elapsed time: 5702 sec [0.90936465 0.91734052 0.88960343]\n",
            "Epoch: 14/19, Step: 162/167, Training Loss: 0.0980, Elapsed time: 5711 sec [0.88732187 0.88268956 0.86956522]\n",
            "Epoch: 14/19, Step: 163/167, Training Loss: 0.1427, Elapsed time: 5723 sec [0.87246377 0.9064588  0.8757526 ]\n",
            "Epoch: 14/19, Step: 164/167, Training Loss: 0.0854, Elapsed time: 5732 sec [0.84859155 0.76015625 0.70785961]\n",
            "Epoch: 14/19, Step: 165/167, Training Loss: 0.2743, Elapsed time: 5743 sec [0.8755787  0.90671031 0.8762215 ]\n",
            "Epoch: 14/19, Step: 166/167, Training Loss: 0.1111, Elapsed time: 5754 sec [0.88470489 0.79424779 0.79215686]\n",
            "Epoch: 14/19, Step: 167/167, Training Loss: 0.1585, Elapsed time: 5761 sec \n",
            "train_wt_dice: 0.8571\n",
            "train_ct_dice: 0.8225\n",
            "train_at_dice: 0.8152\n",
            "Epoch: 14/19, Mean Training Loss: 0.1764, Epoch elapsed time: 1627 sec\n",
            "Epoch: 14/19, Validation Step: 22/21, Validation Loss: 0.8618, Elapsed time: 5957 sec \n",
            "val_wt_dice:  0.87\n",
            "val_ct_dice:    0.8311\n",
            "val_at_dice:    0.8541\n",
            "Epoch: 14/19, Mean Validation Loss: 0.2217, Elapsed time: 1824 sec\n",
            "\n",
            "[0.84438776 0.51456311 0.41620626]\n",
            "Epoch: 15/19, Step: 1/167, Training Loss: 0.1570, Elapsed time: 5967 sec [0.90939193 0.895637   0.8528464 ]\n",
            "Epoch: 15/19, Step: 2/167, Training Loss: 0.1382, Elapsed time: 5976 sec [0.90896226 0.90182648 0.86969253]\n",
            "Epoch: 15/19, Step: 3/167, Training Loss: 0.0993, Elapsed time: 5984 sec [0.89320762 0.91807812 0.88557888]\n",
            "Epoch: 15/19, Step: 4/167, Training Loss: 0.0998, Elapsed time: 5996 sec [0.85361702 0.81707317 0.85124864]\n",
            "Epoch: 15/19, Step: 5/167, Training Loss: 0.1953, Elapsed time: 6005 sec [0.89917254 0.92753623 0.87345497]\n",
            "Epoch: 15/19, Step: 6/167, Training Loss: 0.1274, Elapsed time: 6013 sec [0.81660105 0.87736132 0.85165106]\n",
            "Epoch: 15/19, Step: 7/167, Training Loss: 0.1627, Elapsed time: 6027 sec [0.88905004 0.76567657 0.82639334]\n",
            "Epoch: 15/19, Step: 8/167, Training Loss: 0.1622, Elapsed time: 6036 sec [0.8701104  0.76023392 0.90166667]\n",
            "Epoch: 15/19, Step: 9/167, Training Loss: 0.1542, Elapsed time: 6045 sec [0.86506714 0.85354897 0.87364621]\n",
            "Epoch: 15/19, Step: 10/167, Training Loss: 0.1208, Elapsed time: 6055 sec [0.89778615 0.85866667 0.84504132]\n",
            "Epoch: 15/19, Step: 11/167, Training Loss: 0.1599, Elapsed time: 6064 sec [0.89390618 0.90201342 0.84225352]\n",
            "Epoch: 15/19, Step: 12/167, Training Loss: 0.1440, Elapsed time: 6073 sec [0.90325973 0.8540203  0.78244514]\n",
            "Epoch: 15/19, Step: 13/167, Training Loss: 0.2146, Elapsed time: 6083 sec [0.92405063 0.90325918 0.85532816]\n",
            "Epoch: 15/19, Step: 14/167, Training Loss: 0.1019, Elapsed time: 6092 sec [0.82765912 0.75419664 0.79819617]\n",
            "Epoch: 15/19, Step: 15/167, Training Loss: 0.1758, Elapsed time: 6102 sec [0.92336322 0.81397059 0.76245847]\n",
            "Epoch: 15/19, Step: 16/167, Training Loss: 0.2735, Elapsed time: 6112 sec [0.73187875 0.6109238  0.67353952]\n",
            "Epoch: 15/19, Step: 17/167, Training Loss: 0.3791, Elapsed time: 6120 sec [0.88652038 0.87553191 0.85258359]\n",
            "Epoch: 15/19, Step: 18/167, Training Loss: 0.1129, Elapsed time: 6130 sec [0.87520985 0.67580645 0.79545455]\n",
            "Epoch: 15/19, Step: 19/167, Training Loss: 0.1353, Elapsed time: 6141 sec [0.8242915  0.73722834 0.78680203]\n",
            "Epoch: 15/19, Step: 20/167, Training Loss: 0.1751, Elapsed time: 6148 sec [0.82850955 0.74074074 0.8627451 ]\n",
            "Epoch: 15/19, Step: 21/167, Training Loss: 0.2010, Elapsed time: 6158 sec [0.90414508 0.75775978 0.7893401 ]\n",
            "Epoch: 15/19, Step: 22/167, Training Loss: 0.2108, Elapsed time: 6170 sec [0.88673709 0.75283843 0.86982249]\n",
            "Epoch: 15/19, Step: 23/167, Training Loss: 0.1758, Elapsed time: 6178 sec [0.83893102 0.78753994 0.73764706]\n",
            "Epoch: 15/19, Step: 24/167, Training Loss: 0.3646, Elapsed time: 6188 sec [0.83073286 0.87510073 0.87589499]\n",
            "Epoch: 15/19, Step: 25/167, Training Loss: 0.1078, Elapsed time: 6200 sec [0.83525564 0.813995   0.86087393]\n",
            "Epoch: 15/19, Step: 26/167, Training Loss: 0.1529, Elapsed time: 6215 sec [0.83920521 0.75224417 0.8493353 ]\n",
            "Epoch: 15/19, Step: 27/167, Training Loss: 0.1676, Elapsed time: 6224 sec [0.86477987 0.80402878 0.74221868]\n",
            "Epoch: 15/19, Step: 28/167, Training Loss: 0.2538, Elapsed time: 6235 sec [0.85263835 0.90687554 0.86784741]\n",
            "Epoch: 15/19, Step: 29/167, Training Loss: 0.1679, Elapsed time: 6243 sec [0.735455   0.85751128 0.82546201]\n",
            "Epoch: 15/19, Step: 30/167, Training Loss: 0.1560, Elapsed time: 6252 sec [0.80300353 0.69988067 0.75342466]\n",
            "Epoch: 15/19, Step: 31/167, Training Loss: 0.3365, Elapsed time: 6264 sec [0.78451692 0.79762449 0.78747941]\n",
            "Epoch: 15/19, Step: 32/167, Training Loss: 0.2256, Elapsed time: 6272 sec [0.86834437 0.53530378 0.71727749]\n",
            "Epoch: 15/19, Step: 33/167, Training Loss: 0.1796, Elapsed time: 6281 sec [0.87723897 0.87000563 0.85615848]\n",
            "Epoch: 15/19, Step: 34/167, Training Loss: 0.1519, Elapsed time: 6293 sec [0.85810543 0.82667919 0.83876358]\n",
            "Epoch: 15/19, Step: 35/167, Training Loss: 0.1122, Elapsed time: 6301 sec [0.82044359 0.72263736 0.65007112]\n",
            "Epoch: 15/19, Step: 36/167, Training Loss: 0.1952, Elapsed time: 6312 sec [0.88545046 0.88479596 0.84730539]\n",
            "Epoch: 15/19, Step: 37/167, Training Loss: 0.2115, Elapsed time: 6323 sec [0.85521236 0.76713661 0.66085694]\n",
            "Epoch: 15/19, Step: 38/167, Training Loss: 0.2432, Elapsed time: 6331 sec [0.84994641 0.88864143 0.84153846]\n",
            "Epoch: 15/19, Step: 39/167, Training Loss: 0.0829, Elapsed time: 6340 sec [0.8579258  0.7526971  0.72217353]\n",
            "Epoch: 15/19, Step: 40/167, Training Loss: 0.1370, Elapsed time: 6351 sec [0.8832637  0.75566343 0.79342723]\n",
            "Epoch: 15/19, Step: 41/167, Training Loss: 0.1177, Elapsed time: 6359 sec [0.83509434 0.82850157 0.73947111]\n",
            "Epoch: 15/19, Step: 42/167, Training Loss: 0.1670, Elapsed time: 6368 sec [0.84394904 0.91371045 0.88277669]\n",
            "Epoch: 15/19, Step: 43/167, Training Loss: 0.1373, Elapsed time: 6378 sec [0.89929165 0.88336784 0.84264913]\n",
            "Epoch: 15/19, Step: 44/167, Training Loss: 0.0965, Elapsed time: 6388 sec [0.86623517 0.85362517 0.85289515]\n",
            "Epoch: 15/19, Step: 45/167, Training Loss: 0.0704, Elapsed time: 6396 sec [0.85570668 0.92200557 0.87411883]\n",
            "Epoch: 15/19, Step: 46/167, Training Loss: 0.0957, Elapsed time: 6406 sec [0.83276238 0.85582255 0.76863753]\n",
            "Epoch: 15/19, Step: 47/167, Training Loss: 0.1126, Elapsed time: 6416 sec [0.65009709 0.83994334 0.86562242]\n",
            "Epoch: 15/19, Step: 48/167, Training Loss: 0.2149, Elapsed time: 6423 sec [0.8708836  0.88397247 0.84975542]\n",
            "Epoch: 15/19, Step: 49/167, Training Loss: 0.1448, Elapsed time: 6433 sec [0.91330317 0.8707552  0.8523622 ]\n",
            "Epoch: 15/19, Step: 50/167, Training Loss: 0.1881, Elapsed time: 6446 sec [0.875044   0.77746704 0.80129241]\n",
            "Epoch: 15/19, Step: 51/167, Training Loss: 0.3053, Elapsed time: 6453 sec [0.84236916 0.64100666 0.52059308]\n",
            "Epoch: 15/19, Step: 52/167, Training Loss: 0.1518, Elapsed time: 6462 sec [0.88326586 0.9234794  0.87793427]\n",
            "Epoch: 15/19, Step: 53/167, Training Loss: 0.1527, Elapsed time: 6472 sec [0.85365854 0.77609756 0.76339097]\n",
            "Epoch: 15/19, Step: 54/167, Training Loss: 0.5948, Elapsed time: 6481 sec [0.89062971 0.8127702  0.73276353]\n",
            "Epoch: 15/19, Step: 55/167, Training Loss: 0.2837, Elapsed time: 6491 sec [0.86163078 0.89168279 0.89564616]\n",
            "Epoch: 15/19, Step: 56/167, Training Loss: 0.1188, Elapsed time: 6501 sec [0.83566914 0.65894658 0.45326279]\n",
            "Epoch: 15/19, Step: 57/167, Training Loss: 0.2150, Elapsed time: 6511 sec [0.83651677 0.6301627  0.47598253]\n",
            "Epoch: 15/19, Step: 58/167, Training Loss: 0.2795, Elapsed time: 6520 sec [0.75700761 0.68884026 0.80053369]\n",
            "Epoch: 15/19, Step: 59/167, Training Loss: 0.1649, Elapsed time: 6529 sec [0.84981833 0.78962536 0.82866044]\n",
            "Epoch: 15/19, Step: 60/167, Training Loss: 0.1809, Elapsed time: 6539 sec [0.81571233 0.88972566 0.88567294]\n",
            "Epoch: 15/19, Step: 61/167, Training Loss: 0.1295, Elapsed time: 6549 sec [0.81484003 0.84355346 0.80072246]\n",
            "Epoch: 15/19, Step: 62/167, Training Loss: 0.1757, Elapsed time: 6556 sec [0.86135508 0.86423467 0.84864508]\n",
            "Epoch: 15/19, Step: 63/167, Training Loss: 0.1692, Elapsed time: 6567 sec [0.84129119 0.84813084 0.82962963]\n",
            "Epoch: 15/19, Step: 64/167, Training Loss: 0.1418, Elapsed time: 6576 sec [0.9154619  0.94623656 0.90616622]\n",
            "Epoch: 15/19, Step: 65/167, Training Loss: 0.1059, Elapsed time: 6586 sec [0.86586876 0.79717457 0.89344262]\n",
            "Epoch: 15/19, Step: 66/167, Training Loss: 0.1098, Elapsed time: 6597 sec [0.87138584 0.81587302 0.85321101]\n",
            "Epoch: 15/19, Step: 67/167, Training Loss: 0.1479, Elapsed time: 6606 sec [0.93896006 0.92532468 0.9320197 ]\n",
            "Epoch: 15/19, Step: 68/167, Training Loss: 0.0873, Elapsed time: 6614 sec [0.86823781 0.93333333 0.90740741]\n",
            "Epoch: 15/19, Step: 69/167, Training Loss: 0.1389, Elapsed time: 6625 sec [0.91610806 0.8741573  0.9092827 ]\n",
            "Epoch: 15/19, Step: 70/167, Training Loss: 0.1591, Elapsed time: 6634 sec [0.8743237  0.93754647 0.86095238]\n",
            "Epoch: 15/19, Step: 71/167, Training Loss: 0.2936, Elapsed time: 6642 sec [0.90807914 0.84100877 0.88130334]\n",
            "Epoch: 15/19, Step: 72/167, Training Loss: 0.1319, Elapsed time: 6653 sec [0.89308525 0.83219178 0.77089479]\n",
            "Epoch: 15/19, Step: 73/167, Training Loss: 0.0882, Elapsed time: 6662 sec [0.87745925 0.92382496 0.86324111]\n",
            "Epoch: 15/19, Step: 74/167, Training Loss: 0.1009, Elapsed time: 6670 sec [0.82938826 0.87898089 0.84141547]\n",
            "Epoch: 15/19, Step: 75/167, Training Loss: 0.1150, Elapsed time: 6681 sec [0.85371786 0.88800705 0.67659947]\n",
            "Epoch: 15/19, Step: 76/167, Training Loss: 0.2640, Elapsed time: 6690 sec [0.8778593  0.91905052 0.84347121]\n",
            "Epoch: 15/19, Step: 77/167, Training Loss: 0.1167, Elapsed time: 6701 sec [0.78210258 0.72803347 0.72934473]\n",
            "Epoch: 15/19, Step: 78/167, Training Loss: 0.2091, Elapsed time: 6711 sec [0.77579519 0.70626003 0.86419753]\n",
            "Epoch: 15/19, Step: 79/167, Training Loss: 0.3941, Elapsed time: 6723 sec [0.72720898 0.88851635 0.82740789]\n",
            "Epoch: 15/19, Step: 80/167, Training Loss: 0.1754, Elapsed time: 6732 sec [0.71903323 0.90942601 0.89220986]\n",
            "Epoch: 15/19, Step: 81/167, Training Loss: 0.1977, Elapsed time: 6744 sec [0.86614835 0.82658671 0.82910547]\n",
            "Epoch: 15/19, Step: 82/167, Training Loss: 0.1326, Elapsed time: 6753 sec [0.67155775 0.79830598 0.77568134]\n",
            "Epoch: 15/19, Step: 83/167, Training Loss: 0.1844, Elapsed time: 6762 sec [0.88378259 0.74644243 0.84047402]\n",
            "Epoch: 15/19, Step: 84/167, Training Loss: 0.1212, Elapsed time: 6773 sec [0.85191257 0.7        0.78984127]\n",
            "Epoch: 15/19, Step: 85/167, Training Loss: 0.2402, Elapsed time: 6780 sec [0.9046875  0.74155251 0.79434447]\n",
            "Epoch: 15/19, Step: 86/167, Training Loss: 0.1880, Elapsed time: 6790 sec [0.89442004 0.76501682 0.62432916]\n",
            "Epoch: 15/19, Step: 87/167, Training Loss: 0.2898, Elapsed time: 6800 sec [0.93095512 0.76479076 0.75797373]\n",
            "Epoch: 15/19, Step: 88/167, Training Loss: 0.0856, Elapsed time: 6809 sec [0.87274818 0.86423841 0.87881157]\n",
            "Epoch: 15/19, Step: 89/167, Training Loss: 0.1706, Elapsed time: 6818 sec [0.93413776 0.88356164 0.89023651]\n",
            "Epoch: 15/19, Step: 90/167, Training Loss: 0.1529, Elapsed time: 6828 sec [0.92431856 0.83766432 0.88245315]\n",
            "Epoch: 15/19, Step: 91/167, Training Loss: 0.2077, Elapsed time: 6839 sec [0.91366089 0.9037037  0.8793456 ]\n",
            "Epoch: 15/19, Step: 92/167, Training Loss: 0.1813, Elapsed time: 6848 sec [0.9127611  0.80340667 0.86740012]\n",
            "Epoch: 15/19, Step: 93/167, Training Loss: 0.1585, Elapsed time: 6860 sec [0.89309713 0.89307536 0.87568757]\n",
            "Epoch: 15/19, Step: 94/167, Training Loss: 0.1571, Elapsed time: 6869 sec [0.89559497 0.89682894 0.8696265 ]\n",
            "Epoch: 15/19, Step: 95/167, Training Loss: 0.1379, Elapsed time: 6879 sec [0.84995425 0.87101997 0.83770492]\n",
            "Epoch: 15/19, Step: 96/167, Training Loss: 0.1610, Elapsed time: 6890 sec [0.90325954 0.90809444 0.87859966]\n",
            "Epoch: 15/19, Step: 97/167, Training Loss: 0.1151, Elapsed time: 6898 sec [0.8327351  0.75235532 0.78615071]\n",
            "Epoch: 15/19, Step: 98/167, Training Loss: 0.1244, Elapsed time: 6907 sec [0.85222786 0.82851638 0.81596452]\n",
            "Epoch: 15/19, Step: 99/167, Training Loss: 0.1496, Elapsed time: 6916 sec [0.83325278 0.76207097 0.74542429]\n",
            "Epoch: 15/19, Step: 100/167, Training Loss: 0.1909, Elapsed time: 6926 sec [0.87749496 0.84964811 0.8200692 ]\n",
            "Epoch: 15/19, Step: 101/167, Training Loss: 0.1287, Elapsed time: 6936 sec [0.91596376 0.92109837 0.88575458]\n",
            "Epoch: 15/19, Step: 102/167, Training Loss: 0.1186, Elapsed time: 6945 sec [0.88044759 0.88992662 0.85169124]\n",
            "Epoch: 15/19, Step: 103/167, Training Loss: 0.0882, Elapsed time: 6955 sec [0.85561993 0.86086957 0.8387698 ]\n",
            "Epoch: 15/19, Step: 104/167, Training Loss: 0.1052, Elapsed time: 6964 sec [0.83751412 0.86879433 0.8513599 ]\n",
            "Epoch: 15/19, Step: 105/167, Training Loss: 0.1113, Elapsed time: 6973 sec [0.89141886 0.87037037 0.85675492]\n",
            "Epoch: 15/19, Step: 106/167, Training Loss: 0.1669, Elapsed time: 6983 sec [0.92099323 0.83951807 0.78510638]\n",
            "Epoch: 15/19, Step: 107/167, Training Loss: 0.1395, Elapsed time: 6993 sec [0.84326643 0.56977141 0.76809211]\n",
            "Epoch: 15/19, Step: 108/167, Training Loss: 0.4879, Elapsed time: 7003 sec [0.88647399 0.88131467 0.88970588]\n",
            "Epoch: 15/19, Step: 109/167, Training Loss: 0.1119, Elapsed time: 7011 sec [0.91620648 0.86964054 0.79872713]\n",
            "Epoch: 15/19, Step: 110/167, Training Loss: 0.1642, Elapsed time: 7020 sec [0.76321353 0.50983146 0.66666667]\n",
            "Epoch: 15/19, Step: 111/167, Training Loss: 0.1742, Elapsed time: 7028 sec [0.81275552 0.79268293 0.86758475]\n",
            "Epoch: 15/19, Step: 112/167, Training Loss: 0.2477, Elapsed time: 7039 sec [0.85752769 0.77546667 0.88275862]\n",
            "Epoch: 15/19, Step: 113/167, Training Loss: 0.1627, Elapsed time: 7048 sec [0.85645549 0.87288136 0.86804657]\n",
            "Epoch: 15/19, Step: 114/167, Training Loss: 0.1617, Elapsed time: 7057 sec [0.80608549 0.86718431 0.84406984]\n",
            "Epoch: 15/19, Step: 115/167, Training Loss: 0.1335, Elapsed time: 7067 sec [0.84909026 0.82179931 0.86486486]\n",
            "Epoch: 15/19, Step: 116/167, Training Loss: 0.1375, Elapsed time: 7076 sec [0.83232811 0.76190476 0.83173077]\n",
            "Epoch: 15/19, Step: 117/167, Training Loss: 0.1259, Elapsed time: 7086 sec [0.83697234 0.88960516 0.88913774]\n",
            "Epoch: 15/19, Step: 118/167, Training Loss: 0.1204, Elapsed time: 7096 sec [0.85480244 0.81466877 0.84951816]\n",
            "Epoch: 15/19, Step: 119/167, Training Loss: 0.2323, Elapsed time: 7105 sec [0.8686431  0.79788079 0.8653169 ]\n",
            "Epoch: 15/19, Step: 120/167, Training Loss: 0.3107, Elapsed time: 7114 sec [0.84296462 0.88514548 0.86845169]\n",
            "Epoch: 15/19, Step: 121/167, Training Loss: 0.2074, Elapsed time: 7124 sec [0.8930598  0.87045455 0.90625   ]\n",
            "Epoch: 15/19, Step: 122/167, Training Loss: 0.1260, Elapsed time: 7135 sec [0.8063372  0.87729196 0.84907498]\n",
            "Epoch: 15/19, Step: 123/167, Training Loss: 0.1407, Elapsed time: 7143 sec [0.91497976 0.88933068 0.88429752]\n",
            "Epoch: 15/19, Step: 124/167, Training Loss: 0.0732, Elapsed time: 7153 sec [0.82311828 0.87644292 0.88067838]\n",
            "Epoch: 15/19, Step: 125/167, Training Loss: 0.2231, Elapsed time: 7163 sec [0.87233627 0.91360947 0.89511609]\n",
            "Epoch: 15/19, Step: 126/167, Training Loss: 0.1323, Elapsed time: 7172 sec [0.79654884 0.86268657 0.82159945]\n",
            "Epoch: 15/19, Step: 127/167, Training Loss: 0.1940, Elapsed time: 7181 sec [0.85595306 0.9300106  0.87556904]\n",
            "Epoch: 15/19, Step: 128/167, Training Loss: 0.2229, Elapsed time: 7192 sec [0.87705246 0.86297689 0.83493899]\n",
            "Epoch: 15/19, Step: 129/167, Training Loss: 0.1908, Elapsed time: 7200 sec [0.90895014 0.87524752 0.81988304]\n",
            "Epoch: 15/19, Step: 130/167, Training Loss: 0.1210, Elapsed time: 7209 sec [0.87411637 0.93122271 0.90977444]\n",
            "Epoch: 15/19, Step: 131/167, Training Loss: 0.1561, Elapsed time: 7220 sec [0.84213414 0.64447317 0.68690096]\n",
            "Epoch: 15/19, Step: 132/167, Training Loss: 0.3654, Elapsed time: 7228 sec [0.86822175 0.8873057  0.79043062]\n",
            "Epoch: 15/19, Step: 133/167, Training Loss: 0.2787, Elapsed time: 7238 sec [0.7782006  0.69655172 0.61894954]\n",
            "Epoch: 15/19, Step: 134/167, Training Loss: 0.1880, Elapsed time: 7250 sec [0.75972982 0.82807018 0.7554535 ]\n",
            "Epoch: 15/19, Step: 135/167, Training Loss: 0.1725, Elapsed time: 7258 sec [0.87765026 0.58428246 0.57876106]\n",
            "Epoch: 15/19, Step: 136/167, Training Loss: 0.1969, Elapsed time: 7268 sec [0.86600336 0.78215902 0.73300315]\n",
            "Epoch: 15/19, Step: 137/167, Training Loss: 0.2312, Elapsed time: 7279 sec [0.87702344 0.88912467 0.83866382]\n",
            "Epoch: 15/19, Step: 138/167, Training Loss: 0.0918, Elapsed time: 7289 sec [0.91400966 0.88877855 0.82611997]\n",
            "Epoch: 15/19, Step: 139/167, Training Loss: 0.1128, Elapsed time: 7297 sec [0.88708743 0.85090522 0.84517577]\n",
            "Epoch: 15/19, Step: 140/167, Training Loss: 0.1606, Elapsed time: 7308 sec [0.88972431 0.85581155 0.79896462]\n",
            "Epoch: 15/19, Step: 141/167, Training Loss: 0.1416, Elapsed time: 7318 sec [0.92587564 0.921875   0.89      ]\n",
            "Epoch: 15/19, Step: 142/167, Training Loss: 0.0947, Elapsed time: 7326 sec [0.90467495 0.79282051 0.84067797]\n",
            "Epoch: 15/19, Step: 143/167, Training Loss: 0.1812, Elapsed time: 7338 sec [0.92123525 0.88       0.84061135]\n",
            "Epoch: 15/19, Step: 144/167, Training Loss: 0.2217, Elapsed time: 7347 sec [0.87137059 0.77648305 0.82629108]\n",
            "Epoch: 15/19, Step: 145/167, Training Loss: 0.1602, Elapsed time: 7356 sec [0.8996313  0.86886076 0.86775632]\n",
            "Epoch: 15/19, Step: 146/167, Training Loss: 0.1663, Elapsed time: 7365 sec [0.80742227 0.84264913 0.88073394]\n",
            "Epoch: 15/19, Step: 147/167, Training Loss: 0.1925, Elapsed time: 7374 sec [0.85592705 0.70503597 0.82982172]\n",
            "Epoch: 15/19, Step: 148/167, Training Loss: 0.1812, Elapsed time: 7383 sec [0.90641662 0.90841276 0.88634835]\n",
            "Epoch: 15/19, Step: 149/167, Training Loss: 0.1129, Elapsed time: 7394 sec [0.86201991 0.87702407 0.7909434 ]\n",
            "Epoch: 15/19, Step: 150/167, Training Loss: 0.1589, Elapsed time: 7404 sec [0.89519464 0.835711   0.87336245]\n",
            "Epoch: 15/19, Step: 151/167, Training Loss: 0.1634, Elapsed time: 7414 sec [0.93059712 0.89976052 0.87903226]\n",
            "Epoch: 15/19, Step: 152/167, Training Loss: 0.1163, Elapsed time: 7425 sec [0.89289534 0.89217252 0.83834586]\n",
            "Epoch: 15/19, Step: 153/167, Training Loss: 0.1671, Elapsed time: 7433 sec [0.72109754 0.67528271 0.78205128]\n",
            "Epoch: 15/19, Step: 154/167, Training Loss: 0.2435, Elapsed time: 7442 sec [0.90915819 0.86017989 0.86292655]\n",
            "Epoch: 15/19, Step: 155/167, Training Loss: 0.1930, Elapsed time: 7452 sec [0.89654112 0.88163534 0.89943893]\n",
            "Epoch: 15/19, Step: 156/167, Training Loss: 0.1241, Elapsed time: 7461 sec [0.79268293 0.88868445 0.86743887]\n",
            "Epoch: 15/19, Step: 157/167, Training Loss: 0.0930, Elapsed time: 7471 sec [0.89145658 0.85504587 0.83686441]\n",
            "Epoch: 15/19, Step: 158/167, Training Loss: 0.0873, Elapsed time: 7481 sec [0.89838595 0.94485115 0.90293742]\n",
            "Epoch: 15/19, Step: 159/167, Training Loss: 0.1727, Elapsed time: 7490 sec [0.92039065 0.91009989 0.88846881]\n",
            "Epoch: 15/19, Step: 160/167, Training Loss: 0.1286, Elapsed time: 7499 sec [0.71584144 0.66814404 0.69600939]\n",
            "Epoch: 15/19, Step: 161/167, Training Loss: 0.2947, Elapsed time: 7510 sec [0.91209121 0.9153605  0.89243876]\n",
            "Epoch: 15/19, Step: 162/167, Training Loss: 0.0926, Elapsed time: 7520 sec [0.88693357 0.88285714 0.86964795]\n",
            "Epoch: 15/19, Step: 163/167, Training Loss: 0.1395, Elapsed time: 7529 sec [0.86914378 0.89845475 0.87575925]\n",
            "Epoch: 15/19, Step: 164/167, Training Loss: 0.0859, Elapsed time: 7539 sec [0.84631377 0.75741987 0.71572581]\n",
            "Epoch: 15/19, Step: 165/167, Training Loss: 0.2722, Elapsed time: 7550 sec [0.88621701 0.91345756 0.8950583 ]\n",
            "Epoch: 15/19, Step: 166/167, Training Loss: 0.0982, Elapsed time: 7560 sec [0.89139693 0.81967213 0.8157683 ]\n",
            "Epoch: 15/19, Step: 167/167, Training Loss: 0.1436, Elapsed time: 7566 sec \n",
            "train_wt_dice: 0.8599\n",
            "train_ct_dice: 0.8265\n",
            "train_at_dice: 0.8198\n",
            "Epoch: 15/19, Mean Training Loss: 0.1722, Epoch elapsed time: 1608 sec\n",
            "Epoch: 15/19, Validation Step: 22/21, Validation Loss: 0.8383, Elapsed time: 7763 sec \n",
            "val_wt_dice:  0.8763\n",
            "val_ct_dice:    0.834\n",
            "val_at_dice:    0.8598\n",
            "Epoch: 15/19, Mean Validation Loss: 0.2195, Elapsed time: 1805 sec\n",
            "\n",
            "[0.85100946 0.48664122 0.41791045]\n",
            "Epoch: 16/19, Step: 1/167, Training Loss: 0.1601, Elapsed time: 7771 sec [0.90975167 0.90014164 0.85745614]\n",
            "Epoch: 16/19, Step: 2/167, Training Loss: 0.1446, Elapsed time: 7782 sec [0.91291008 0.90982194 0.88070692]\n",
            "Epoch: 16/19, Step: 3/167, Training Loss: 0.0940, Elapsed time: 7791 sec [0.899171   0.91850829 0.88292011]\n",
            "Epoch: 16/19, Step: 4/167, Training Loss: 0.0960, Elapsed time: 7800 sec [0.87175153 0.8627451  0.84955752]\n",
            "Epoch: 16/19, Step: 5/167, Training Loss: 0.1648, Elapsed time: 7812 sec [0.90700202 0.93095768 0.87619048]\n",
            "Epoch: 16/19, Step: 6/167, Training Loss: 0.1224, Elapsed time: 7822 sec [0.82208995 0.87466022 0.84608031]\n",
            "Epoch: 16/19, Step: 7/167, Training Loss: 0.1554, Elapsed time: 7832 sec [0.89257642 0.77267231 0.82641509]\n",
            "Epoch: 16/19, Step: 8/167, Training Loss: 0.1535, Elapsed time: 7841 sec [0.87021653 0.80793201 0.89864865]\n",
            "Epoch: 16/19, Step: 9/167, Training Loss: 0.1440, Elapsed time: 7850 sec [0.8691358  0.86762075 0.88810365]\n",
            "Epoch: 16/19, Step: 10/167, Training Loss: 0.1166, Elapsed time: 7860 sec [0.89622642 0.87238352 0.85062241]\n",
            "Epoch: 16/19, Step: 11/167, Training Loss: 0.1656, Elapsed time: 7871 sec [0.89873418 0.90118577 0.81869688]\n",
            "Epoch: 16/19, Step: 12/167, Training Loss: 0.1407, Elapsed time: 7878 sec [0.90662915 0.85450346 0.77435265]\n",
            "Epoch: 16/19, Step: 13/167, Training Loss: 0.2142, Elapsed time: 7887 sec [0.92581262 0.91076923 0.86376404]\n",
            "Epoch: 16/19, Step: 14/167, Training Loss: 0.0965, Elapsed time: 7899 sec [0.85546109 0.80793854 0.81902552]\n",
            "Epoch: 16/19, Step: 15/167, Training Loss: 0.1626, Elapsed time: 7907 sec [0.92829293 0.83052091 0.7691016 ]\n",
            "Epoch: 16/19, Step: 16/167, Training Loss: 0.2530, Elapsed time: 7916 sec [0.73354972 0.59382588 0.6763285 ]\n",
            "Epoch: 16/19, Step: 17/167, Training Loss: 0.3759, Elapsed time: 7927 sec [0.88616025 0.88840501 0.85388128]\n",
            "Epoch: 16/19, Step: 18/167, Training Loss: 0.1100, Elapsed time: 7936 sec [0.89051919 0.69135802 0.78289474]\n",
            "Epoch: 16/19, Step: 19/167, Training Loss: 0.1244, Elapsed time: 7945 sec [0.82700535 0.73782033 0.76772616]\n",
            "Epoch: 16/19, Step: 20/167, Training Loss: 0.1700, Elapsed time: 7956 sec [0.83718153 0.76890034 0.86661962]\n",
            "Epoch: 16/19, Step: 21/167, Training Loss: 0.1851, Elapsed time: 7965 sec [0.91005974 0.75551102 0.76477833]\n",
            "Epoch: 16/19, Step: 22/167, Training Loss: 0.2187, Elapsed time: 7974 sec [0.89974974 0.77582318 0.8944571 ]\n",
            "Epoch: 16/19, Step: 23/167, Training Loss: 0.1726, Elapsed time: 7985 sec [0.8507135  0.80946651 0.74653823]\n",
            "Epoch: 16/19, Step: 24/167, Training Loss: 0.3642, Elapsed time: 7993 sec [0.84064222 0.88779689 0.89272503]\n",
            "Epoch: 16/19, Step: 25/167, Training Loss: 0.1021, Elapsed time: 8003 sec [0.85310734 0.84124629 0.86442406]\n",
            "Epoch: 16/19, Step: 26/167, Training Loss: 0.1455, Elapsed time: 8013 sec [0.85660954 0.75504587 0.86756542]\n",
            "Epoch: 16/19, Step: 27/167, Training Loss: 0.1684, Elapsed time: 8021 sec [0.87236404 0.81692573 0.75657895]\n",
            "Epoch: 16/19, Step: 28/167, Training Loss: 0.2342, Elapsed time: 8031 sec [0.86129971 0.92010536 0.88719723]\n",
            "Epoch: 16/19, Step: 29/167, Training Loss: 0.1533, Elapsed time: 8042 sec [0.73562059 0.86048021 0.83367983]\n",
            "Epoch: 16/19, Step: 30/167, Training Loss: 0.1585, Elapsed time: 8050 sec [0.80605431 0.7141558  0.76635514]\n",
            "Epoch: 16/19, Step: 31/167, Training Loss: 0.3280, Elapsed time: 8059 sec [0.78178539 0.77602524 0.7425897 ]\n",
            "Epoch: 16/19, Step: 32/167, Training Loss: 0.2345, Elapsed time: 8071 sec [0.87899787 0.57142857 0.73712737]\n",
            "Epoch: 16/19, Step: 33/167, Training Loss: 0.1770, Elapsed time: 8082 sec [0.87697297 0.86721763 0.84009942]\n",
            "Epoch: 16/19, Step: 34/167, Training Loss: 0.1427, Elapsed time: 8091 sec [0.86949623 0.84868735 0.8097869 ]\n",
            "Epoch: 16/19, Step: 35/167, Training Loss: 0.1064, Elapsed time: 8102 sec [0.83436293 0.75613142 0.69276219]\n",
            "Epoch: 16/19, Step: 36/167, Training Loss: 0.1862, Elapsed time: 8111 sec [0.89150135 0.86850591 0.82155215]\n",
            "Epoch: 16/19, Step: 37/167, Training Loss: 0.2093, Elapsed time: 8121 sec [0.85780081 0.78893741 0.69651741]\n",
            "Epoch: 16/19, Step: 38/167, Training Loss: 0.2166, Elapsed time: 8133 sec [0.84775641 0.89558906 0.85145482]\n",
            "Epoch: 16/19, Step: 39/167, Training Loss: 0.0816, Elapsed time: 8140 sec [0.85156084 0.74119574 0.7255075 ]\n",
            "Epoch: 16/19, Step: 40/167, Training Loss: 0.1450, Elapsed time: 8150 sec [0.88198758 0.7761194  0.82885086]\n",
            "Epoch: 16/19, Step: 41/167, Training Loss: 0.1154, Elapsed time: 8160 sec [0.86759788 0.87016776 0.79773546]\n",
            "Epoch: 16/19, Step: 42/167, Training Loss: 0.1395, Elapsed time: 8168 sec [0.84576915 0.90639924 0.87377371]\n",
            "Epoch: 16/19, Step: 43/167, Training Loss: 0.1452, Elapsed time: 8178 sec [0.90134321 0.8817278  0.84056604]\n",
            "Epoch: 16/19, Step: 44/167, Training Loss: 0.0949, Elapsed time: 8189 sec [0.88322368 0.86606523 0.86283892]\n",
            "Epoch: 16/19, Step: 45/167, Training Loss: 0.0679, Elapsed time: 8198 sec [0.85849335 0.91776088 0.868     ]\n",
            "Epoch: 16/19, Step: 46/167, Training Loss: 0.0959, Elapsed time: 8207 sec [0.84170854 0.8700565  0.80107527]\n",
            "Epoch: 16/19, Step: 47/167, Training Loss: 0.1045, Elapsed time: 8219 sec [0.65331519 0.84866039 0.87058824]\n",
            "Epoch: 16/19, Step: 48/167, Training Loss: 0.2172, Elapsed time: 8228 sec [0.86244154 0.8509638  0.86344828]\n",
            "Epoch: 16/19, Step: 49/167, Training Loss: 0.1426, Elapsed time: 8236 sec [0.91096133 0.86423358 0.8490566 ]\n",
            "Epoch: 16/19, Step: 50/167, Training Loss: 0.1700, Elapsed time: 8247 sec [0.87026087 0.77833656 0.81493333]\n",
            "Epoch: 16/19, Step: 51/167, Training Loss: 0.2843, Elapsed time: 8256 sec [0.83538841 0.65141114 0.5210084 ]\n",
            "Epoch: 16/19, Step: 52/167, Training Loss: 0.1464, Elapsed time: 8263 sec [0.88267026 0.92647544 0.88405111]\n",
            "Epoch: 16/19, Step: 53/167, Training Loss: 0.1537, Elapsed time: 8272 sec [0.85214546 0.77761557 0.76069462]\n",
            "Epoch: 16/19, Step: 54/167, Training Loss: 0.5907, Elapsed time: 8284 sec [0.8959757  0.81654798 0.75086906]\n",
            "Epoch: 16/19, Step: 55/167, Training Loss: 0.2843, Elapsed time: 8293 sec [0.86071491 0.89560976 0.88445667]\n",
            "Epoch: 16/19, Step: 56/167, Training Loss: 0.1154, Elapsed time: 8302 sec [0.83888889 0.67337461 0.47985005]\n",
            "Epoch: 16/19, Step: 57/167, Training Loss: 0.2051, Elapsed time: 8313 sec [0.84711316 0.63736959 0.48600884]\n",
            "Epoch: 16/19, Step: 58/167, Training Loss: 0.2838, Elapsed time: 8323 sec [0.76204707 0.69136879 0.8042328 ]\n",
            "Epoch: 16/19, Step: 59/167, Training Loss: 0.1681, Elapsed time: 8331 sec [0.85603589 0.78185745 0.82866044]\n",
            "Epoch: 16/19, Step: 60/167, Training Loss: 0.1832, Elapsed time: 8341 sec [0.79779342 0.89092873 0.88825215]\n",
            "Epoch: 16/19, Step: 61/167, Training Loss: 0.1252, Elapsed time: 8351 sec [0.80628973 0.84582189 0.80023923]\n",
            "Epoch: 16/19, Step: 62/167, Training Loss: 0.1739, Elapsed time: 8361 sec [0.85946792 0.85640074 0.84438608]\n",
            "Epoch: 16/19, Step: 63/167, Training Loss: 0.1659, Elapsed time: 8371 sec [0.83512786 0.8410372  0.84990958]\n",
            "Epoch: 16/19, Step: 64/167, Training Loss: 0.1349, Elapsed time: 8380 sec [0.91134321 0.93252769 0.90563565]\n",
            "Epoch: 16/19, Step: 65/167, Training Loss: 0.1027, Elapsed time: 8388 sec [0.85621709 0.79463364 0.88163265]\n",
            "Epoch: 16/19, Step: 66/167, Training Loss: 0.1088, Elapsed time: 8399 sec [0.87397969 0.82422294 0.8744186 ]\n",
            "Epoch: 16/19, Step: 67/167, Training Loss: 0.1496, Elapsed time: 8409 sec [0.94135338 0.92628205 0.9324192 ]\n",
            "Epoch: 16/19, Step: 68/167, Training Loss: 0.0857, Elapsed time: 8417 sec [0.86316351 0.91574279 0.89583333]\n",
            "Epoch: 16/19, Step: 69/167, Training Loss: 0.1352, Elapsed time: 8427 sec [0.91215941 0.88498403 0.89233954]\n",
            "Epoch: 16/19, Step: 70/167, Training Loss: 0.1528, Elapsed time: 8437 sec [0.87739345 0.94222222 0.86239696]\n",
            "Epoch: 16/19, Step: 71/167, Training Loss: 0.2871, Elapsed time: 8444 sec [0.90159249 0.80929741 0.88087774]\n",
            "Epoch: 16/19, Step: 72/167, Training Loss: 0.1379, Elapsed time: 8456 sec [0.89631919 0.86762075 0.80288958]\n",
            "Epoch: 16/19, Step: 73/167, Training Loss: 0.0888, Elapsed time: 8465 sec [0.87711387 0.93340671 0.8812199 ]\n",
            "Epoch: 16/19, Step: 74/167, Training Loss: 0.0982, Elapsed time: 8473 sec [0.83980283 0.88259958 0.85529716]\n",
            "Epoch: 16/19, Step: 75/167, Training Loss: 0.1090, Elapsed time: 8484 sec [0.85684539 0.90436316 0.71126761]\n",
            "Epoch: 16/19, Step: 76/167, Training Loss: 0.2417, Elapsed time: 8495 sec [0.88344887 0.92222903 0.85504886]\n",
            "Epoch: 16/19, Step: 77/167, Training Loss: 0.1129, Elapsed time: 8504 sec [0.80104031 0.76972418 0.76845298]\n",
            "Epoch: 16/19, Step: 78/167, Training Loss: 0.1930, Elapsed time: 8514 sec [0.7980139  0.69436851 0.87276786]\n",
            "Epoch: 16/19, Step: 79/167, Training Loss: 0.3966, Elapsed time: 8523 sec [0.73252496 0.88689714 0.82345191]\n",
            "Epoch: 16/19, Step: 80/167, Training Loss: 0.1830, Elapsed time: 8533 sec [0.71914795 0.91010048 0.89652956]\n",
            "Epoch: 16/19, Step: 81/167, Training Loss: 0.1992, Elapsed time: 8544 sec [0.86115123 0.82666667 0.82926829]\n",
            "Epoch: 16/19, Step: 82/167, Training Loss: 0.1268, Elapsed time: 8553 sec [0.65841004 0.79557429 0.7636113 ]\n",
            "Epoch: 16/19, Step: 83/167, Training Loss: 0.1887, Elapsed time: 8562 sec [0.86964129 0.7381258  0.83119266]\n",
            "Epoch: 16/19, Step: 84/167, Training Loss: 0.1248, Elapsed time: 8573 sec [0.85044947 0.68594059 0.76810712]\n",
            "Epoch: 16/19, Step: 85/167, Training Loss: 0.2463, Elapsed time: 8580 sec [0.8920243  0.73751135 0.78404053]\n",
            "Epoch: 16/19, Step: 86/167, Training Loss: 0.1991, Elapsed time: 8590 sec [0.89849927 0.74425153 0.59607173]\n",
            "Epoch: 16/19, Step: 87/167, Training Loss: 0.3039, Elapsed time: 8600 sec [0.93279492 0.7007874  0.69520548]\n",
            "Epoch: 16/19, Step: 88/167, Training Loss: 0.0920, Elapsed time: 8609 sec [0.87121502 0.84047109 0.87711214]\n",
            "Epoch: 16/19, Step: 89/167, Training Loss: 0.1749, Elapsed time: 8618 sec [0.93429433 0.88820355 0.8996328 ]\n",
            "Epoch: 16/19, Step: 90/167, Training Loss: 0.1505, Elapsed time: 8630 sec [0.92835366 0.8286399  0.87755102]\n",
            "Epoch: 16/19, Step: 91/167, Training Loss: 0.2152, Elapsed time: 8640 sec [0.91584512 0.90403447 0.9025641 ]\n",
            "Epoch: 16/19, Step: 92/167, Training Loss: 0.1775, Elapsed time: 8649 sec [0.92389381 0.81606311 0.88545994]\n",
            "Epoch: 16/19, Step: 93/167, Training Loss: 0.1496, Elapsed time: 8659 sec [0.89648799 0.89591837 0.86450168]\n",
            "Epoch: 16/19, Step: 94/167, Training Loss: 0.1573, Elapsed time: 8669 sec [0.90020132 0.90851449 0.87671233]\n",
            "Epoch: 16/19, Step: 95/167, Training Loss: 0.1385, Elapsed time: 8678 sec [0.85345027 0.87568757 0.82571912]\n",
            "Epoch: 16/19, Step: 96/167, Training Loss: 0.1712, Elapsed time: 8688 sec [0.90571328 0.91335894 0.88275862]\n",
            "Epoch: 16/19, Step: 97/167, Training Loss: 0.1168, Elapsed time: 8698 sec [0.84788578 0.77473684 0.81236674]\n",
            "Epoch: 16/19, Step: 98/167, Training Loss: 0.1191, Elapsed time: 8707 sec [0.85763001 0.8240621  0.81021088]\n",
            "Epoch: 16/19, Step: 99/167, Training Loss: 0.1533, Elapsed time: 8716 sec [0.82705685 0.75443618 0.7400163 ]\n",
            "Epoch: 16/19, Step: 100/167, Training Loss: 0.1972, Elapsed time: 8725 sec [0.87597039 0.86843811 0.83275261]\n",
            "Epoch: 16/19, Step: 101/167, Training Loss: 0.1182, Elapsed time: 8735 sec [0.91339564 0.92065711 0.89456265]\n",
            "Epoch: 16/19, Step: 102/167, Training Loss: 0.1181, Elapsed time: 8746 sec [0.88235294 0.89618218 0.85416667]\n",
            "Epoch: 16/19, Step: 103/167, Training Loss: 0.0828, Elapsed time: 8754 sec [0.85154449 0.86784141 0.84456929]\n",
            "Epoch: 16/19, Step: 104/167, Training Loss: 0.1047, Elapsed time: 8763 sec [0.82803115 0.84305317 0.82453988]\n",
            "Epoch: 16/19, Step: 105/167, Training Loss: 0.1076, Elapsed time: 8775 sec [0.89436979 0.86844444 0.86199864]\n",
            "Epoch: 16/19, Step: 106/167, Training Loss: 0.1660, Elapsed time: 8783 sec [0.91752154 0.84501669 0.78187566]\n",
            "Epoch: 16/19, Step: 107/167, Training Loss: 0.1363, Elapsed time: 8792 sec [0.84361064 0.58558251 0.7715655 ]\n",
            "Epoch: 16/19, Step: 108/167, Training Loss: 0.4902, Elapsed time: 8803 sec [0.89760451 0.89773423 0.90175953]\n",
            "Epoch: 16/19, Step: 109/167, Training Loss: 0.1062, Elapsed time: 8810 sec [0.91332225 0.87606838 0.79352851]\n",
            "Epoch: 16/19, Step: 110/167, Training Loss: 0.1595, Elapsed time: 8819 sec [0.79285714 0.57276995 0.70748299]\n",
            "Epoch: 16/19, Step: 111/167, Training Loss: 0.1577, Elapsed time: 8828 sec [0.81752066 0.80458015 0.87136929]\n",
            "Epoch: 16/19, Step: 112/167, Training Loss: 0.2399, Elapsed time: 8838 sec [0.86377427 0.79389313 0.88097043]\n",
            "Epoch: 16/19, Step: 113/167, Training Loss: 0.1490, Elapsed time: 8847 sec [0.86240626 0.88107324 0.86662243]\n",
            "Epoch: 16/19, Step: 114/167, Training Loss: 0.1640, Elapsed time: 8856 sec [0.82320988 0.8936528  0.85627284]\n",
            "Epoch: 16/19, Step: 115/167, Training Loss: 0.1276, Elapsed time: 8866 sec [0.85575365 0.82414242 0.86148451]\n",
            "Epoch: 16/19, Step: 116/167, Training Loss: 0.1314, Elapsed time: 8876 sec [0.82486696 0.75021608 0.82107843]\n",
            "Epoch: 16/19, Step: 117/167, Training Loss: 0.1293, Elapsed time: 8885 sec [0.84266409 0.90780142 0.89059081]\n",
            "Epoch: 16/19, Step: 118/167, Training Loss: 0.1065, Elapsed time: 8895 sec [0.85081288 0.79969004 0.84552846]\n",
            "Epoch: 16/19, Step: 119/167, Training Loss: 0.2403, Elapsed time: 8906 sec [0.86562112 0.79201245 0.85764345]\n",
            "Epoch: 16/19, Step: 120/167, Training Loss: 0.3023, Elapsed time: 8916 sec [0.83027163 0.88966049 0.88363851]\n",
            "Epoch: 16/19, Step: 121/167, Training Loss: 0.2081, Elapsed time: 8925 sec [0.88628811 0.86606471 0.91447811]\n",
            "Epoch: 16/19, Step: 122/167, Training Loss: 0.1290, Elapsed time: 8934 sec [0.78626648 0.87983134 0.87118977]\n",
            "Epoch: 16/19, Step: 123/167, Training Loss: 0.1337, Elapsed time: 8945 sec [0.9110349  0.88580858 0.88211047]\n",
            "Epoch: 16/19, Step: 124/167, Training Loss: 0.0718, Elapsed time: 8954 sec [0.80662548 0.87563452 0.87477314]\n",
            "Epoch: 16/19, Step: 125/167, Training Loss: 0.2279, Elapsed time: 8963 sec [0.87576055 0.93405929 0.90983607]\n",
            "Epoch: 16/19, Step: 126/167, Training Loss: 0.1321, Elapsed time: 8973 sec [0.79832168 0.87302397 0.84328883]\n",
            "Epoch: 16/19, Step: 127/167, Training Loss: 0.1914, Elapsed time: 8982 sec [0.85960214 0.93632959 0.88190184]\n",
            "Epoch: 16/19, Step: 128/167, Training Loss: 0.2178, Elapsed time: 8991 sec [0.884286   0.87109161 0.83552632]\n",
            "Epoch: 16/19, Step: 129/167, Training Loss: 0.1792, Elapsed time: 9000 sec [0.91127397 0.89168357 0.84582572]\n",
            "Epoch: 16/19, Step: 130/167, Training Loss: 0.1145, Elapsed time: 9009 sec [0.8829597  0.93307839 0.90317757]\n",
            "Epoch: 16/19, Step: 131/167, Training Loss: 0.1537, Elapsed time: 9019 sec [0.84178443 0.65486134 0.72058824]\n",
            "Epoch: 16/19, Step: 132/167, Training Loss: 0.3664, Elapsed time: 9028 sec [0.87232116 0.89098335 0.78703704]\n",
            "Epoch: 16/19, Step: 133/167, Training Loss: 0.2601, Elapsed time: 9040 sec [0.79322221 0.70467033 0.62506394]\n",
            "Epoch: 16/19, Step: 134/167, Training Loss: 0.1920, Elapsed time: 9050 sec [0.76980763 0.83840379 0.75291375]\n",
            "Epoch: 16/19, Step: 135/167, Training Loss: 0.1740, Elapsed time: 9061 sec [0.86552849 0.54632588 0.53777417]\n",
            "Epoch: 16/19, Step: 136/167, Training Loss: 0.2129, Elapsed time: 9070 sec [0.8601023  0.76879699 0.720035  ]\n",
            "Epoch: 16/19, Step: 137/167, Training Loss: 0.2241, Elapsed time: 9078 sec [0.8808891  0.88736118 0.83227176]\n",
            "Epoch: 16/19, Step: 138/167, Training Loss: 0.0891, Elapsed time: 9088 sec [0.91475851 0.89944972 0.83359014]\n",
            "Epoch: 16/19, Step: 139/167, Training Loss: 0.1090, Elapsed time: 9099 sec [0.88650218 0.86178862 0.84973303]\n",
            "Epoch: 16/19, Step: 140/167, Training Loss: 0.1504, Elapsed time: 9108 sec [0.8927327  0.849626   0.81225033]\n",
            "Epoch: 16/19, Step: 141/167, Training Loss: 0.1417, Elapsed time: 9119 sec [0.92706911 0.92822026 0.8973384 ]\n",
            "Epoch: 16/19, Step: 142/167, Training Loss: 0.0958, Elapsed time: 9128 sec [0.90341428 0.80470348 0.85223368]\n",
            "Epoch: 16/19, Step: 143/167, Training Loss: 0.1723, Elapsed time: 9138 sec [0.91936044 0.88690476 0.85148515]\n",
            "Epoch: 16/19, Step: 144/167, Training Loss: 0.2191, Elapsed time: 9148 sec [0.88017846 0.78862242 0.81320451]\n",
            "Epoch: 16/19, Step: 145/167, Training Loss: 0.1638, Elapsed time: 9157 sec [0.90711136 0.8784474  0.86646434]\n",
            "Epoch: 16/19, Step: 146/167, Training Loss: 0.1604, Elapsed time: 9167 sec [0.8227095  0.84770115 0.86500655]\n",
            "Epoch: 16/19, Step: 147/167, Training Loss: 0.1936, Elapsed time: 9178 sec [0.85399516 0.69574037 0.82725322]\n",
            "Epoch: 16/19, Step: 148/167, Training Loss: 0.1865, Elapsed time: 9187 sec [0.9007326  0.9100616  0.87359365]\n",
            "Epoch: 16/19, Step: 149/167, Training Loss: 0.1041, Elapsed time: 9196 sec [0.86245004 0.88300221 0.7976012 ]\n",
            "Epoch: 16/19, Step: 150/167, Training Loss: 0.1481, Elapsed time: 9208 sec [0.88806396 0.84568765 0.86466626]\n",
            "Epoch: 16/19, Step: 151/167, Training Loss: 0.1666, Elapsed time: 9216 sec [0.92787303 0.9007214  0.87687688]\n",
            "Epoch: 16/19, Step: 152/167, Training Loss: 0.1126, Elapsed time: 9226 sec [0.88683533 0.89082278 0.83007335]\n",
            "Epoch: 16/19, Step: 153/167, Training Loss: 0.1665, Elapsed time: 9237 sec [0.73521414 0.63702506 0.77313054]\n",
            "Epoch: 16/19, Step: 154/167, Training Loss: 0.2517, Elapsed time: 9245 sec [0.91322009 0.86495308 0.87399771]\n",
            "Epoch: 16/19, Step: 155/167, Training Loss: 0.1902, Elapsed time: 9254 sec [0.90512305 0.89714665 0.91208315]\n",
            "Epoch: 16/19, Step: 156/167, Training Loss: 0.1100, Elapsed time: 9265 sec [0.81318681 0.89055192 0.88741722]\n",
            "Epoch: 16/19, Step: 157/167, Training Loss: 0.0873, Elapsed time: 9273 sec [0.89005602 0.85923218 0.85378869]\n",
            "Epoch: 16/19, Step: 158/167, Training Loss: 0.0833, Elapsed time: 9283 sec [0.90047585 0.94734264 0.90762794]\n",
            "Epoch: 16/19, Step: 159/167, Training Loss: 0.1776, Elapsed time: 9295 sec [0.91981132 0.92089552 0.89671585]\n",
            "Epoch: 16/19, Step: 160/167, Training Loss: 0.1238, Elapsed time: 9304 sec [0.72607357 0.66741196 0.69952494]\n",
            "Epoch: 16/19, Step: 161/167, Training Loss: 0.2899, Elapsed time: 9313 sec [0.91139623 0.91969493 0.89806867]\n",
            "Epoch: 16/19, Step: 162/167, Training Loss: 0.0958, Elapsed time: 9325 sec [0.89174041 0.88125894 0.87380497]\n",
            "Epoch: 16/19, Step: 163/167, Training Loss: 0.1437, Elapsed time: 9334 sec [0.88010158 0.90485006 0.88379888]\n",
            "Epoch: 16/19, Step: 164/167, Training Loss: 0.0848, Elapsed time: 9343 sec [0.86297845 0.79296236 0.75373134]\n",
            "Epoch: 16/19, Step: 165/167, Training Loss: 0.2586, Elapsed time: 9353 sec [0.88644689 0.9001223  0.88484848]\n",
            "Epoch: 16/19, Step: 166/167, Training Loss: 0.0990, Elapsed time: 9363 sec [0.89440994 0.8225256  0.83009709]\n",
            "Epoch: 16/19, Step: 167/167, Training Loss: 0.1458, Elapsed time: 9369 sec \n",
            "train_wt_dice: 0.8627\n",
            "train_ct_dice: 0.8312\n",
            "train_at_dice: 0.8234\n",
            "Epoch: 16/19, Mean Training Loss: 0.1691, Epoch elapsed time: 1607 sec\n",
            "Epoch: 16/19, Validation Step: 22/21, Validation Loss: 0.8139, Elapsed time: 9568 sec \n",
            "val_wt_dice:  0.8779\n",
            "val_ct_dice:    0.839\n",
            "val_at_dice:    0.8646\n",
            "Epoch: 16/19, Mean Validation Loss: 0.2157, Elapsed time: 1805 sec\n",
            "\n",
            "[0.86652763 0.53305785 0.50554324]\n",
            "Epoch: 17/19, Step: 1/167, Training Loss: 0.1505, Elapsed time: 9576 sec [0.90950158 0.90115264 0.84819017]\n",
            "Epoch: 17/19, Step: 2/167, Training Loss: 0.1331, Elapsed time: 9587 sec [0.9080649  0.90585534 0.87776141]\n",
            "Epoch: 17/19, Step: 3/167, Training Loss: 0.0980, Elapsed time: 9597 sec [0.89497892 0.91727241 0.88949523]\n",
            "Epoch: 17/19, Step: 4/167, Training Loss: 0.0928, Elapsed time: 9604 sec [0.88126316 0.87363834 0.86089814]\n",
            "Epoch: 17/19, Step: 5/167, Training Loss: 0.1592, Elapsed time: 9616 sec [0.90588235 0.93367914 0.87149533]\n",
            "Epoch: 17/19, Step: 6/167, Training Loss: 0.1201, Elapsed time: 9625 sec [0.81812213 0.88321168 0.85617761]\n",
            "Epoch: 17/19, Step: 7/167, Training Loss: 0.1555, Elapsed time: 9634 sec [0.89241058 0.76700783 0.82345443]\n",
            "Epoch: 17/19, Step: 8/167, Training Loss: 0.1528, Elapsed time: 9645 sec [0.8789879  0.80697805 0.90431837]\n",
            "Epoch: 17/19, Step: 9/167, Training Loss: 0.1334, Elapsed time: 9655 sec [0.87305122 0.85714286 0.88405797]\n",
            "Epoch: 17/19, Step: 10/167, Training Loss: 0.1128, Elapsed time: 9664 sec [0.89789227 0.87085869 0.84810127]\n",
            "Epoch: 17/19, Step: 11/167, Training Loss: 0.1580, Elapsed time: 9674 sec [0.90050295 0.89795918 0.83815029]\n",
            "Epoch: 17/19, Step: 12/167, Training Loss: 0.1365, Elapsed time: 9682 sec [0.91442716 0.86554292 0.80075424]\n",
            "Epoch: 17/19, Step: 13/167, Training Loss: 0.2047, Elapsed time: 9692 sec [0.92740854 0.90631365 0.85052632]\n",
            "Epoch: 17/19, Step: 14/167, Training Loss: 0.1015, Elapsed time: 9702 sec [0.84745763 0.8        0.81703107]\n",
            "Epoch: 17/19, Step: 15/167, Training Loss: 0.1682, Elapsed time: 9712 sec [0.9277759  0.81147839 0.77272727]\n",
            "Epoch: 17/19, Step: 16/167, Training Loss: 0.2549, Elapsed time: 9721 sec [0.74902665 0.64423077 0.70782484]\n",
            "Epoch: 17/19, Step: 17/167, Training Loss: 0.3591, Elapsed time: 9731 sec [0.88680816 0.88840737 0.86615385]\n",
            "Epoch: 17/19, Step: 18/167, Training Loss: 0.1064, Elapsed time: 9741 sec [0.88579545 0.72089041 0.78941566]\n",
            "Epoch: 17/19, Step: 19/167, Training Loss: 0.1222, Elapsed time: 9751 sec [0.8280409  0.75229358 0.78446989]\n",
            "Epoch: 17/19, Step: 20/167, Training Loss: 0.1606, Elapsed time: 9759 sec [0.8385031  0.78530323 0.85653105]\n",
            "Epoch: 17/19, Step: 21/167, Training Loss: 0.1873, Elapsed time: 9769 sec [0.90973451 0.75232404 0.74396135]\n",
            "Epoch: 17/19, Step: 22/167, Training Loss: 0.2196, Elapsed time: 9778 sec [0.89660223 0.78       0.86661754]\n",
            "Epoch: 17/19, Step: 23/167, Training Loss: 0.1686, Elapsed time: 9788 sec [0.84753603 0.81592432 0.75377468]\n",
            "Epoch: 17/19, Step: 24/167, Training Loss: 0.3321, Elapsed time: 9797 sec [0.83774834 0.89108911 0.89079755]\n",
            "Epoch: 17/19, Step: 25/167, Training Loss: 0.0974, Elapsed time: 9806 sec [0.84206861 0.84365782 0.86960934]\n",
            "Epoch: 17/19, Step: 26/167, Training Loss: 0.1442, Elapsed time: 9816 sec [0.85462555 0.75899772 0.86153846]\n",
            "Epoch: 17/19, Step: 27/167, Training Loss: 0.1670, Elapsed time: 9827 sec [0.87804878 0.81775972 0.78767123]\n",
            "Epoch: 17/19, Step: 28/167, Training Loss: 0.2386, Elapsed time: 9838 sec [0.86580792 0.92186128 0.88595271]\n",
            "Epoch: 17/19, Step: 29/167, Training Loss: 0.1509, Elapsed time: 9846 sec [0.74764451 0.88375165 0.86666667]\n",
            "Epoch: 17/19, Step: 30/167, Training Loss: 0.1443, Elapsed time: 9855 sec [0.81733653 0.71167544 0.78108808]\n",
            "Epoch: 17/19, Step: 31/167, Training Loss: 0.3246, Elapsed time: 9864 sec [0.78493467 0.82276119 0.80334728]\n",
            "Epoch: 17/19, Step: 32/167, Training Loss: 0.2124, Elapsed time: 9874 sec [0.86088969 0.54       0.70588235]\n",
            "Epoch: 17/19, Step: 33/167, Training Loss: 0.1863, Elapsed time: 9884 sec [0.87717789 0.87723214 0.85933504]\n",
            "Epoch: 17/19, Step: 34/167, Training Loss: 0.1469, Elapsed time: 9893 sec [0.86631435 0.82434944 0.82686084]\n",
            "Epoch: 17/19, Step: 35/167, Training Loss: 0.1037, Elapsed time: 9903 sec [0.82917752 0.73778195 0.6929368 ]\n",
            "Epoch: 17/19, Step: 36/167, Training Loss: 0.1898, Elapsed time: 9912 sec [0.88241082 0.88174274 0.82876064]\n",
            "Epoch: 17/19, Step: 37/167, Training Loss: 0.1935, Elapsed time: 9923 sec [0.85454545 0.77855478 0.67571235]\n",
            "Epoch: 17/19, Step: 38/167, Training Loss: 0.2083, Elapsed time: 9931 sec [0.85538297 0.8858246  0.83458647]\n",
            "Epoch: 17/19, Step: 39/167, Training Loss: 0.0796, Elapsed time: 9941 sec [0.85141903 0.71914557 0.68723937]\n",
            "Epoch: 17/19, Step: 40/167, Training Loss: 0.1466, Elapsed time: 9951 sec [0.89187843 0.76533115 0.80701754]\n",
            "Epoch: 17/19, Step: 41/167, Training Loss: 0.1173, Elapsed time: 9961 sec [0.86492845 0.86817189 0.78840285]\n",
            "Epoch: 17/19, Step: 42/167, Training Loss: 0.1392, Elapsed time: 9970 sec [0.85385082 0.91074856 0.87206266]\n",
            "Epoch: 17/19, Step: 43/167, Training Loss: 0.1394, Elapsed time: 9980 sec [0.90728374 0.89004871 0.85755121]\n",
            "Epoch: 17/19, Step: 44/167, Training Loss: 0.0956, Elapsed time: 9990 sec [0.88492172 0.88700565 0.87693562]\n",
            "Epoch: 17/19, Step: 45/167, Training Loss: 0.0656, Elapsed time: 9999 sec [0.85310904 0.92847125 0.8863869 ]\n",
            "Epoch: 17/19, Step: 46/167, Training Loss: 0.1005, Elapsed time: 10009 sec [0.85916855 0.88590604 0.8027027 ]\n",
            "Epoch: 17/19, Step: 47/167, Training Loss: 0.0911, Elapsed time: 10017 sec [0.65857478 0.87361419 0.88494078]\n",
            "Epoch: 17/19, Step: 48/167, Training Loss: 0.2102, Elapsed time: 10026 sec [0.8698619  0.88385827 0.88294798]\n",
            "Epoch: 17/19, Step: 49/167, Training Loss: 0.1390, Elapsed time: 10036 sec [0.91221848 0.86345382 0.86395233]\n",
            "Epoch: 17/19, Step: 50/167, Training Loss: 0.1690, Elapsed time: 10045 sec [0.87255587 0.78355502 0.82034273]\n",
            "Epoch: 17/19, Step: 51/167, Training Loss: 0.2979, Elapsed time: 10054 sec [0.83966636 0.66614786 0.5480427 ]\n",
            "Epoch: 17/19, Step: 52/167, Training Loss: 0.1422, Elapsed time: 10064 sec [0.87244304 0.92490119 0.88171021]\n",
            "Epoch: 17/19, Step: 53/167, Training Loss: 0.1544, Elapsed time: 10074 sec [0.859115   0.79485347 0.76134944]\n",
            "Epoch: 17/19, Step: 54/167, Training Loss: 0.5433, Elapsed time: 10083 sec [0.89407865 0.82516556 0.74899483]\n",
            "Epoch: 17/19, Step: 55/167, Training Loss: 0.2719, Elapsed time: 10095 sec [0.861116   0.89946052 0.87731323]\n",
            "Epoch: 17/19, Step: 56/167, Training Loss: 0.1112, Elapsed time: 10105 sec [0.83234768 0.67464299 0.44851658]\n",
            "Epoch: 17/19, Step: 57/167, Training Loss: 0.2039, Elapsed time: 10115 sec [0.84827164 0.62794349 0.48961424]\n",
            "Epoch: 17/19, Step: 58/167, Training Loss: 0.2846, Elapsed time: 10125 sec [0.77994642 0.69457911 0.80505655]\n",
            "Epoch: 17/19, Step: 59/167, Training Loss: 0.1523, Elapsed time: 10134 sec [0.86413155 0.78248385 0.81776417]\n",
            "Epoch: 17/19, Step: 60/167, Training Loss: 0.1868, Elapsed time: 10143 sec [0.81578433 0.89128095 0.8849178 ]\n",
            "Epoch: 17/19, Step: 61/167, Training Loss: 0.1266, Elapsed time: 10154 sec [0.81683421 0.84467165 0.81275842]\n",
            "Epoch: 17/19, Step: 62/167, Training Loss: 0.1586, Elapsed time: 10162 sec [0.87149422 0.87338893 0.86390533]\n",
            "Epoch: 17/19, Step: 63/167, Training Loss: 0.1571, Elapsed time: 10171 sec [0.85293132 0.85125858 0.84341637]\n",
            "Epoch: 17/19, Step: 64/167, Training Loss: 0.1255, Elapsed time: 10182 sec [0.91553398 0.93152064 0.90485695]\n",
            "Epoch: 17/19, Step: 65/167, Training Loss: 0.1040, Elapsed time: 10190 sec [0.88266776 0.80364372 0.91428571]\n",
            "Epoch: 17/19, Step: 66/167, Training Loss: 0.0975, Elapsed time: 10200 sec [0.88414266 0.84805266 0.88363292]\n",
            "Epoch: 17/19, Step: 67/167, Training Loss: 0.1456, Elapsed time: 10213 sec [0.94369706 0.9237013  0.91474423]\n",
            "Epoch: 17/19, Step: 68/167, Training Loss: 0.0901, Elapsed time: 10220 sec [0.87410926 0.92694064 0.89839572]\n",
            "Epoch: 17/19, Step: 69/167, Training Loss: 0.1255, Elapsed time: 10229 sec [0.91637843 0.89983398 0.92422625]\n",
            "Epoch: 17/19, Step: 70/167, Training Loss: 0.1461, Elapsed time: 10239 sec [0.87886279 0.94390426 0.86601942]\n",
            "Epoch: 17/19, Step: 71/167, Training Loss: 0.2761, Elapsed time: 10249 sec [0.90845792 0.82608696 0.88076312]\n",
            "Epoch: 17/19, Step: 72/167, Training Loss: 0.1424, Elapsed time: 10257 sec [0.8907563  0.86170213 0.80701754]\n",
            "Epoch: 17/19, Step: 73/167, Training Loss: 0.0860, Elapsed time: 10267 sec [0.88006757 0.9336235  0.87619048]\n",
            "Epoch: 17/19, Step: 74/167, Training Loss: 0.0964, Elapsed time: 10277 sec [0.83307904 0.86956522 0.84903226]\n",
            "Epoch: 17/19, Step: 75/167, Training Loss: 0.1053, Elapsed time: 10285 sec [0.86141678 0.91426072 0.69587629]\n",
            "Epoch: 17/19, Step: 76/167, Training Loss: 0.2523, Elapsed time: 10297 sec [0.88617363 0.91781644 0.84694687]\n",
            "Epoch: 17/19, Step: 77/167, Training Loss: 0.1107, Elapsed time: 10306 sec [0.79328785 0.75866415 0.75712881]\n",
            "Epoch: 17/19, Step: 78/167, Training Loss: 0.1996, Elapsed time: 10315 sec [0.78746327 0.71238703 0.87064117]\n",
            "Epoch: 17/19, Step: 79/167, Training Loss: 0.3797, Elapsed time: 10325 sec [0.76834264 0.89728353 0.82283465]\n",
            "Epoch: 17/19, Step: 80/167, Training Loss: 0.1740, Elapsed time: 10334 sec [0.74674385 0.91191983 0.90301863]\n",
            "Epoch: 17/19, Step: 81/167, Training Loss: 0.1910, Elapsed time: 10346 sec [0.86817127 0.83037475 0.832     ]\n",
            "Epoch: 17/19, Step: 82/167, Training Loss: 0.1265, Elapsed time: 10358 sec [0.68105263 0.8306674  0.7887931 ]\n",
            "Epoch: 17/19, Step: 83/167, Training Loss: 0.1828, Elapsed time: 10366 sec [0.88091569 0.75130208 0.85714286]\n",
            "Epoch: 17/19, Step: 84/167, Training Loss: 0.1139, Elapsed time: 10375 sec [0.85640044 0.7214876  0.79037365]\n",
            "Epoch: 17/19, Step: 85/167, Training Loss: 0.2284, Elapsed time: 10386 sec [0.89528193 0.73270014 0.78791774]\n",
            "Epoch: 17/19, Step: 86/167, Training Loss: 0.1760, Elapsed time: 10394 sec [0.89583671 0.75980861 0.61497797]\n",
            "Epoch: 17/19, Step: 87/167, Training Loss: 0.2819, Elapsed time: 10404 sec [0.92812677 0.7010582  0.67343486]\n",
            "Epoch: 17/19, Step: 88/167, Training Loss: 0.0888, Elapsed time: 10414 sec [0.88087597 0.85511208 0.88067744]\n",
            "Epoch: 17/19, Step: 89/167, Training Loss: 0.1608, Elapsed time: 10424 sec [0.93707455 0.87443946 0.89498807]\n",
            "Epoch: 17/19, Step: 90/167, Training Loss: 0.1451, Elapsed time: 10432 sec [0.92558491 0.83039186 0.87514061]\n",
            "Epoch: 17/19, Step: 91/167, Training Loss: 0.2123, Elapsed time: 10443 sec [0.9134889  0.90502355 0.89908257]\n",
            "Epoch: 17/19, Step: 92/167, Training Loss: 0.1789, Elapsed time: 10453 sec [0.9204666  0.81733524 0.88273424]\n",
            "Epoch: 17/19, Step: 93/167, Training Loss: 0.1540, Elapsed time: 10464 sec [0.8984351  0.89679715 0.8745838 ]\n",
            "Epoch: 17/19, Step: 94/167, Training Loss: 0.1505, Elapsed time: 10474 sec [0.90365449 0.91023339 0.88428571]\n",
            "Epoch: 17/19, Step: 95/167, Training Loss: 0.1354, Elapsed time: 10483 sec [0.86730634 0.87807545 0.84043442]\n",
            "Epoch: 17/19, Step: 96/167, Training Loss: 0.1617, Elapsed time: 10495 sec [0.89987769 0.90708528 0.87542469]\n",
            "Epoch: 17/19, Step: 97/167, Training Loss: 0.1129, Elapsed time: 10502 sec [0.85761834 0.76976422 0.80462185]\n",
            "Epoch: 17/19, Step: 98/167, Training Loss: 0.1206, Elapsed time: 10512 sec [0.86674325 0.84054229 0.83636364]\n",
            "Epoch: 17/19, Step: 99/167, Training Loss: 0.1422, Elapsed time: 10523 sec [0.83260974 0.77938622 0.76592225]\n",
            "Epoch: 17/19, Step: 100/167, Training Loss: 0.1862, Elapsed time: 10530 sec [0.86770777 0.85806452 0.82114537]\n",
            "Epoch: 17/19, Step: 101/167, Training Loss: 0.1250, Elapsed time: 10540 sec [0.91066998 0.92265002 0.89461358]\n",
            "Epoch: 17/19, Step: 102/167, Training Loss: 0.1139, Elapsed time: 10551 sec [0.87943678 0.90699253 0.86267606]\n",
            "Epoch: 17/19, Step: 103/167, Training Loss: 0.0794, Elapsed time: 10560 sec [0.85345027 0.84813754 0.83166515]\n",
            "Epoch: 17/19, Step: 104/167, Training Loss: 0.1057, Elapsed time: 10568 sec [0.84397002 0.8643617  0.85060394]\n",
            "Epoch: 17/19, Step: 105/167, Training Loss: 0.1056, Elapsed time: 10579 sec [0.89058784 0.86983289 0.86158001]\n",
            "Epoch: 17/19, Step: 106/167, Training Loss: 0.1667, Elapsed time: 10587 sec [0.92003264 0.83552632 0.76923077]\n",
            "Epoch: 17/19, Step: 107/167, Training Loss: 0.1341, Elapsed time: 10598 sec [0.8445122  0.60212202 0.76507937]\n",
            "Epoch: 17/19, Step: 108/167, Training Loss: 0.4416, Elapsed time: 10609 sec [0.89568261 0.89242054 0.89442815]\n",
            "Epoch: 17/19, Step: 109/167, Training Loss: 0.1081, Elapsed time: 10617 sec [0.91840153 0.88189655 0.80405933]\n",
            "Epoch: 17/19, Step: 110/167, Training Loss: 0.1557, Elapsed time: 10625 sec [0.79404467 0.58938776 0.70180305]\n",
            "Epoch: 17/19, Step: 111/167, Training Loss: 0.1575, Elapsed time: 10635 sec [0.81663641 0.79366306 0.86334746]\n",
            "Epoch: 17/19, Step: 112/167, Training Loss: 0.2478, Elapsed time: 10645 sec [0.86822416 0.81026786 0.88906009]\n",
            "Epoch: 17/19, Step: 113/167, Training Loss: 0.1432, Elapsed time: 10653 sec [0.86231416 0.85623003 0.86613119]\n",
            "Epoch: 17/19, Step: 114/167, Training Loss: 0.1630, Elapsed time: 10663 sec [0.80279316 0.89158644 0.85149701]\n",
            "Epoch: 17/19, Step: 115/167, Training Loss: 0.1239, Elapsed time: 10673 sec [0.84765137 0.83195111 0.86809994]\n",
            "Epoch: 17/19, Step: 116/167, Training Loss: 0.1322, Elapsed time: 10683 sec [0.83772672 0.75605536 0.81764005]\n",
            "Epoch: 17/19, Step: 117/167, Training Loss: 0.1221, Elapsed time: 10692 sec [0.85073171 0.89044289 0.86680761]\n",
            "Epoch: 17/19, Step: 118/167, Training Loss: 0.1029, Elapsed time: 10701 sec [0.84490446 0.81285878 0.81664316]\n",
            "Epoch: 17/19, Step: 119/167, Training Loss: 0.2175, Elapsed time: 10712 sec [0.87106599 0.80802746 0.86072664]\n",
            "Epoch: 17/19, Step: 120/167, Training Loss: 0.2942, Elapsed time: 10720 sec [0.84475032 0.89914663 0.88369375]\n",
            "Epoch: 17/19, Step: 121/167, Training Loss: 0.2007, Elapsed time: 10731 sec [0.89276373 0.87648184 0.90970808]\n",
            "Epoch: 17/19, Step: 122/167, Training Loss: 0.1215, Elapsed time: 10743 sec [0.81179245 0.89781022 0.86372745]\n",
            "Epoch: 17/19, Step: 123/167, Training Loss: 0.1309, Elapsed time: 10750 sec [0.91297275 0.89724647 0.88113051]\n",
            "Epoch: 17/19, Step: 124/167, Training Loss: 0.0732, Elapsed time: 10760 sec [0.82151458 0.88888889 0.87869644]\n",
            "Epoch: 17/19, Step: 125/167, Training Loss: 0.2156, Elapsed time: 10771 sec [0.86913871 0.91785714 0.89716599]\n",
            "Epoch: 17/19, Step: 126/167, Training Loss: 0.1340, Elapsed time: 10781 sec [0.80167131 0.87532068 0.83261803]\n",
            "Epoch: 17/19, Step: 127/167, Training Loss: 0.1966, Elapsed time: 10790 sec [0.85905016 0.93612453 0.87993803]\n",
            "Epoch: 17/19, Step: 128/167, Training Loss: 0.2132, Elapsed time: 10800 sec [0.88123515 0.8776824  0.8407479 ]\n",
            "Epoch: 17/19, Step: 129/167, Training Loss: 0.1719, Elapsed time: 10809 sec [0.90976514 0.88834951 0.84031573]\n",
            "Epoch: 17/19, Step: 130/167, Training Loss: 0.1162, Elapsed time: 10819 sec [0.8739473  0.93387978 0.90407855]\n",
            "Epoch: 17/19, Step: 131/167, Training Loss: 0.1563, Elapsed time: 10827 sec [0.85385564 0.69751117 0.73138075]\n",
            "Epoch: 17/19, Step: 132/167, Training Loss: 0.3308, Elapsed time: 10837 sec [0.86818454 0.89987326 0.80205415]\n",
            "Epoch: 17/19, Step: 133/167, Training Loss: 0.2502, Elapsed time: 10848 sec [0.8071509  0.74835406 0.68044693]\n",
            "Epoch: 17/19, Step: 134/167, Training Loss: 0.1700, Elapsed time: 10859 sec [0.79318489 0.84640259 0.76777251]\n",
            "Epoch: 17/19, Step: 135/167, Training Loss: 0.1659, Elapsed time: 10868 sec [0.87546    0.5631381  0.53247795]\n",
            "Epoch: 17/19, Step: 136/167, Training Loss: 0.2042, Elapsed time: 10878 sec [0.85875272 0.78774194 0.73353955]\n",
            "Epoch: 17/19, Step: 137/167, Training Loss: 0.2213, Elapsed time: 10887 sec [0.88084392 0.8881823  0.8416608 ]\n",
            "Epoch: 17/19, Step: 138/167, Training Loss: 0.0859, Elapsed time: 10897 sec [0.91884615 0.90763052 0.84633205]\n",
            "Epoch: 17/19, Step: 139/167, Training Loss: 0.1023, Elapsed time: 10908 sec [0.89108336 0.87022901 0.87143957]\n",
            "Epoch: 17/19, Step: 140/167, Training Loss: 0.1511, Elapsed time: 10916 sec [0.89422001 0.85795602 0.82511993]\n",
            "Epoch: 17/19, Step: 141/167, Training Loss: 0.1278, Elapsed time: 10927 sec [0.92718844 0.91675025 0.88318357]\n",
            "Epoch: 17/19, Step: 142/167, Training Loss: 0.0968, Elapsed time: 10937 sec [0.90609555 0.83703303 0.85059423]\n",
            "Epoch: 17/19, Step: 143/167, Training Loss: 0.1663, Elapsed time: 10947 sec [0.92654975 0.89665297 0.85993485]\n",
            "Epoch: 17/19, Step: 144/167, Training Loss: 0.1978, Elapsed time: 10955 sec [0.8918528  0.84173088 0.82714055]\n",
            "Epoch: 17/19, Step: 145/167, Training Loss: 0.1576, Elapsed time: 10966 sec [0.90162813 0.87886467 0.88072018]\n",
            "Epoch: 17/19, Step: 146/167, Training Loss: 0.1601, Elapsed time: 10975 sec [0.81838223 0.85603865 0.86533333]\n",
            "Epoch: 17/19, Step: 147/167, Training Loss: 0.1954, Elapsed time: 10986 sec [0.86100951 0.70483373 0.82695418]\n",
            "Epoch: 17/19, Step: 148/167, Training Loss: 0.1766, Elapsed time: 10995 sec [0.90838852 0.91117479 0.88858879]\n",
            "Epoch: 17/19, Step: 149/167, Training Loss: 0.0994, Elapsed time: 11005 sec [0.85501513 0.85861624 0.79034691]\n",
            "Epoch: 17/19, Step: 150/167, Training Loss: 0.1462, Elapsed time: 11016 sec [0.9001867  0.87052078 0.87562189]\n",
            "Epoch: 17/19, Step: 151/167, Training Loss: 0.1526, Elapsed time: 11024 sec [0.93200285 0.89629378 0.87143572]\n",
            "Epoch: 17/19, Step: 152/167, Training Loss: 0.1118, Elapsed time: 11034 sec [0.89115543 0.89748704 0.84433375]\n",
            "Epoch: 17/19, Step: 153/167, Training Loss: 0.1532, Elapsed time: 11045 sec [0.73893654 0.65617233 0.78249337]\n",
            "Epoch: 17/19, Step: 154/167, Training Loss: 0.2486, Elapsed time: 11054 sec [0.90639481 0.85516116 0.86443562]\n",
            "Epoch: 17/19, Step: 155/167, Training Loss: 0.1933, Elapsed time: 11062 sec [0.90633225 0.89896632 0.89690722]\n",
            "Epoch: 17/19, Step: 156/167, Training Loss: 0.1132, Elapsed time: 11073 sec [0.80668796 0.88267148 0.8633461 ]\n",
            "Epoch: 17/19, Step: 157/167, Training Loss: 0.0834, Elapsed time: 11082 sec [0.89020153 0.86545455 0.85774499]\n",
            "Epoch: 17/19, Step: 158/167, Training Loss: 0.0797, Elapsed time: 11091 sec [0.90325825 0.94973158 0.91262755]\n",
            "Epoch: 17/19, Step: 159/167, Training Loss: 0.1610, Elapsed time: 11102 sec [0.91942849 0.92641016 0.90119332]\n",
            "Epoch: 17/19, Step: 160/167, Training Loss: 0.1218, Elapsed time: 11113 sec [0.73031727 0.68933718 0.70506108]\n",
            "Epoch: 17/19, Step: 161/167, Training Loss: 0.2870, Elapsed time: 11124 sec [0.91110446 0.91143747 0.89186296]\n",
            "Epoch: 17/19, Step: 162/167, Training Loss: 0.0939, Elapsed time: 11132 sec [0.89016321 0.87746479 0.87606433]\n",
            "Epoch: 17/19, Step: 163/167, Training Loss: 0.1362, Elapsed time: 11142 sec [0.8827912  0.9063893  0.88284287]\n",
            "Epoch: 17/19, Step: 164/167, Training Loss: 0.0803, Elapsed time: 11153 sec [0.85718809 0.79421222 0.74507772]\n",
            "Epoch: 17/19, Step: 165/167, Training Loss: 0.2551, Elapsed time: 11162 sec [0.89072652 0.91390728 0.89543938]\n",
            "Epoch: 17/19, Step: 166/167, Training Loss: 0.1002, Elapsed time: 11171 sec [0.89653689 0.83090705 0.82372055]\n",
            "Epoch: 17/19, Step: 167/167, Training Loss: 0.1364, Elapsed time: 11180 sec \n",
            "train_wt_dice: 0.8657\n",
            "train_ct_dice: 0.8367\n",
            "train_at_dice: 0.8266\n",
            "Epoch: 17/19, Mean Training Loss: 0.1639, Epoch elapsed time: 1612 sec\n",
            "Epoch: 17/19, Validation Step: 22/21, Validation Loss: 0.8412, Elapsed time: 11376 sec \n",
            "val_wt_dice:  0.8804\n",
            "val_ct_dice:    0.8417\n",
            "val_at_dice:    0.8693\n",
            "Epoch: 17/19, Mean Validation Loss: 0.2138, Elapsed time: 1808 sec\n",
            "\n",
            "[0.87454068 0.56701031 0.50564334]\n",
            "Epoch: 18/19, Step: 1/167, Training Loss: 0.1435, Elapsed time: 11386 sec [0.9108791  0.90794097 0.86513158]\n",
            "Epoch: 18/19, Step: 2/167, Training Loss: 0.1324, Elapsed time: 11397 sec [0.91696238 0.91637631 0.88905547]\n",
            "Epoch: 18/19, Step: 3/167, Training Loss: 0.0957, Elapsed time: 11405 sec [0.89994194 0.92749562 0.9020979 ]\n",
            "Epoch: 18/19, Step: 4/167, Training Loss: 0.0883, Elapsed time: 11415 sec [0.86750587 0.84494382 0.87061404]\n",
            "Epoch: 18/19, Step: 5/167, Training Loss: 0.1734, Elapsed time: 11426 sec [0.91117301 0.92799412 0.86956522]\n",
            "Epoch: 18/19, Step: 6/167, Training Loss: 0.1174, Elapsed time: 11435 sec [0.81485116 0.88990268 0.85894942]\n",
            "Epoch: 18/19, Step: 7/167, Training Loss: 0.1507, Elapsed time: 11447 sec [0.87893911 0.75146542 0.80073801]\n",
            "Epoch: 18/19, Step: 8/167, Training Loss: 0.1508, Elapsed time: 11455 sec [0.86860718 0.79620324 0.9001692 ]\n",
            "Epoch: 18/19, Step: 9/167, Training Loss: 0.1425, Elapsed time: 11465 sec [0.88414908 0.87230216 0.88029021]\n",
            "Epoch: 18/19, Step: 10/167, Training Loss: 0.1086, Elapsed time: 11476 sec [0.90593479 0.88707483 0.83007209]\n",
            "Epoch: 18/19, Step: 11/167, Training Loss: 0.1541, Elapsed time: 11486 sec [0.8934338  0.89302326 0.82768778]\n",
            "Epoch: 18/19, Step: 12/167, Training Loss: 0.1374, Elapsed time: 11494 sec [0.92083777 0.85736616 0.78421388]\n",
            "Epoch: 18/19, Step: 13/167, Training Loss: 0.2101, Elapsed time: 11504 sec [0.92504291 0.90410959 0.83911234]\n",
            "Epoch: 18/19, Step: 14/167, Training Loss: 0.0975, Elapsed time: 11514 sec [0.84437729 0.76932441 0.79507279]\n",
            "Epoch: 18/19, Step: 15/167, Training Loss: 0.1702, Elapsed time: 11524 sec [0.92634418 0.82527076 0.78253425]\n",
            "Epoch: 18/19, Step: 16/167, Training Loss: 0.2486, Elapsed time: 11533 sec [0.75575758 0.64137689 0.69754386]\n",
            "Epoch: 18/19, Step: 17/167, Training Loss: 0.3605, Elapsed time: 11542 sec [0.8868476  0.89260658 0.86717557]\n",
            "Epoch: 18/19, Step: 18/167, Training Loss: 0.1018, Elapsed time: 11554 sec [0.86627418 0.66877971 0.7887931 ]\n",
            "Epoch: 18/19, Step: 19/167, Training Loss: 0.1287, Elapsed time: 11561 sec [0.82725451 0.73480818 0.78425511]\n",
            "Epoch: 18/19, Step: 20/167, Training Loss: 0.1638, Elapsed time: 11570 sec [0.84183267 0.76916488 0.85814116]\n",
            "Epoch: 18/19, Step: 21/167, Training Loss: 0.1841, Elapsed time: 11581 sec [0.91503906 0.76579926 0.76438522]\n",
            "Epoch: 18/19, Step: 22/167, Training Loss: 0.2115, Elapsed time: 11589 sec [0.89061129 0.74716652 0.88253012]\n",
            "Epoch: 18/19, Step: 23/167, Training Loss: 0.1748, Elapsed time: 11599 sec [0.8413365  0.80786201 0.75480769]\n",
            "Epoch: 18/19, Step: 24/167, Training Loss: 0.3549, Elapsed time: 11611 sec [0.84925483 0.89552239 0.91161616]\n",
            "Epoch: 18/19, Step: 25/167, Training Loss: 0.1000, Elapsed time: 11619 sec [0.85244161 0.84210526 0.88841883]\n",
            "Epoch: 18/19, Step: 26/167, Training Loss: 0.1458, Elapsed time: 11629 sec [0.84962025 0.76674365 0.86469673]\n",
            "Epoch: 18/19, Step: 27/167, Training Loss: 0.1642, Elapsed time: 11639 sec [0.86678174 0.82308143 0.8003518 ]\n",
            "Epoch: 18/19, Step: 28/167, Training Loss: 0.2295, Elapsed time: 11648 sec [0.85111147 0.92646411 0.88717589]\n",
            "Epoch: 18/19, Step: 29/167, Training Loss: 0.1541, Elapsed time: 11657 sec [0.73611111 0.87862797 0.85653105]\n",
            "Epoch: 18/19, Step: 30/167, Training Loss: 0.1446, Elapsed time: 11667 sec [0.80691474 0.70609535 0.79321592]\n",
            "Epoch: 18/19, Step: 31/167, Training Loss: 0.3056, Elapsed time: 11676 sec [0.77387535 0.78636364 0.78460278]\n",
            "Epoch: 18/19, Step: 32/167, Training Loss: 0.2305, Elapsed time: 11687 sec [0.85958993 0.53614458 0.67980296]\n",
            "Epoch: 18/19, Step: 33/167, Training Loss: 0.1890, Elapsed time: 11695 sec [0.88424717 0.86849468 0.84516681]\n",
            "Epoch: 18/19, Step: 34/167, Training Loss: 0.1471, Elapsed time: 11704 sec [0.86231743 0.81675875 0.81175536]\n",
            "Epoch: 18/19, Step: 35/167, Training Loss: 0.1068, Elapsed time: 11715 sec [0.8358837  0.72837897 0.66205943]\n",
            "Epoch: 18/19, Step: 36/167, Training Loss: 0.1843, Elapsed time: 11724 sec [0.89328851 0.88857645 0.84732824]\n",
            "Epoch: 18/19, Step: 37/167, Training Loss: 0.1835, Elapsed time: 11736 sec [0.85807442 0.7793911  0.68070652]\n",
            "Epoch: 18/19, Step: 38/167, Training Loss: 0.2075, Elapsed time: 11746 sec [0.86120043 0.89259878 0.84353741]\n",
            "Epoch: 18/19, Step: 39/167, Training Loss: 0.0773, Elapsed time: 11755 sec [0.86646758 0.75931352 0.74931632]\n",
            "Epoch: 18/19, Step: 40/167, Training Loss: 0.1354, Elapsed time: 11764 sec [0.89094085 0.81048035 0.8115942 ]\n",
            "Epoch: 18/19, Step: 41/167, Training Loss: 0.1080, Elapsed time: 11774 sec [0.86042437 0.87125091 0.78651685]\n",
            "Epoch: 18/19, Step: 42/167, Training Loss: 0.1355, Elapsed time: 11783 sec [0.84101749 0.91784152 0.88711819]\n",
            "Epoch: 18/19, Step: 43/167, Training Loss: 0.1519, Elapsed time: 11794 sec [0.89849277 0.88537005 0.8396803 ]\n",
            "Epoch: 18/19, Step: 44/167, Training Loss: 0.0929, Elapsed time: 11802 sec [0.88129103 0.88152327 0.87591241]\n",
            "Epoch: 18/19, Step: 45/167, Training Loss: 0.0664, Elapsed time: 11811 sec [0.85163551 0.91805556 0.87349398]\n",
            "Epoch: 18/19, Step: 46/167, Training Loss: 0.0934, Elapsed time: 11822 sec [0.86707832 0.88974855 0.80711354]\n",
            "Epoch: 18/19, Step: 47/167, Training Loss: 0.0956, Elapsed time: 11830 sec [0.63316583 0.83345247 0.83839212]\n",
            "Epoch: 18/19, Step: 48/167, Training Loss: 0.2216, Elapsed time: 11840 sec [0.86021505 0.88223749 0.86285714]\n",
            "Epoch: 18/19, Step: 49/167, Training Loss: 0.1481, Elapsed time: 11851 sec [0.90996271 0.87579774 0.85877863]\n",
            "Epoch: 18/19, Step: 50/167, Training Loss: 0.1534, Elapsed time: 11860 sec [0.88282337 0.78925144 0.81687133]\n",
            "Epoch: 18/19, Step: 51/167, Training Loss: 0.2706, Elapsed time: 11869 sec [0.84149681 0.62527076 0.50798722]\n",
            "Epoch: 18/19, Step: 52/167, Training Loss: 0.1494, Elapsed time: 11879 sec [0.88404095 0.92960526 0.88411215]\n",
            "Epoch: 18/19, Step: 53/167, Training Loss: 0.1463, Elapsed time: 11888 sec [0.86039742 0.79384911 0.77092139]\n",
            "Epoch: 18/19, Step: 54/167, Training Loss: 0.5601, Elapsed time: 11898 sec [0.90317654 0.8249833  0.74754477]\n",
            "Epoch: 18/19, Step: 55/167, Training Loss: 0.2706, Elapsed time: 11908 sec [0.87544867 0.90990099 0.89539749]\n",
            "Epoch: 18/19, Step: 56/167, Training Loss: 0.1085, Elapsed time: 11918 sec [0.85734523 0.70083433 0.51195219]\n",
            "Epoch: 18/19, Step: 57/167, Training Loss: 0.1904, Elapsed time: 11929 sec [0.86768846 0.69454919 0.52180809]\n",
            "Epoch: 18/19, Step: 58/167, Training Loss: 0.2625, Elapsed time: 11937 sec [0.79327862 0.71637694 0.82096664]\n",
            "Epoch: 18/19, Step: 59/167, Training Loss: 0.1499, Elapsed time: 11947 sec [0.86500925 0.7831669  0.83489097]\n",
            "Epoch: 18/19, Step: 60/167, Training Loss: 0.1820, Elapsed time: 11957 sec [0.80766284 0.8839957  0.87482219]\n",
            "Epoch: 18/19, Step: 61/167, Training Loss: 0.1222, Elapsed time: 11967 sec [0.8024712  0.83584396 0.7866354 ]\n",
            "Epoch: 18/19, Step: 62/167, Training Loss: 0.1738, Elapsed time: 11977 sec [0.85143033 0.86936937 0.85657895]\n",
            "Epoch: 18/19, Step: 63/167, Training Loss: 0.1619, Elapsed time: 11987 sec [0.84973404 0.86904762 0.85447761]\n",
            "Epoch: 18/19, Step: 64/167, Training Loss: 0.1279, Elapsed time: 11996 sec [0.90443114 0.93367089 0.90072321]\n",
            "Epoch: 18/19, Step: 65/167, Training Loss: 0.1030, Elapsed time: 12007 sec [0.85964215 0.79120879 0.87450462]\n",
            "Epoch: 18/19, Step: 66/167, Training Loss: 0.0996, Elapsed time: 12015 sec [0.86334311 0.78409091 0.83819629]\n",
            "Epoch: 18/19, Step: 67/167, Training Loss: 0.1527, Elapsed time: 12026 sec [0.94070553 0.91620112 0.92996109]\n",
            "Epoch: 18/19, Step: 68/167, Training Loss: 0.0845, Elapsed time: 12035 sec [0.8743344  0.94036697 0.91176471]\n",
            "Epoch: 18/19, Step: 69/167, Training Loss: 0.1288, Elapsed time: 12044 sec [0.91004437 0.89368591 0.90390707]\n",
            "Epoch: 18/19, Step: 70/167, Training Loss: 0.1491, Elapsed time: 12054 sec [0.8787738  0.94407159 0.86486486]\n",
            "Epoch: 18/19, Step: 71/167, Training Loss: 0.2894, Elapsed time: 12063 sec [0.91104231 0.85008237 0.89507154]\n",
            "Epoch: 18/19, Step: 72/167, Training Loss: 0.1316, Elapsed time: 12073 sec [0.89647391 0.85537919 0.80329557]\n",
            "Epoch: 18/19, Step: 73/167, Training Loss: 0.0888, Elapsed time: 12082 sec [0.88965123 0.94072448 0.88888889]\n",
            "Epoch: 18/19, Step: 74/167, Training Loss: 0.0923, Elapsed time: 12091 sec [0.84951304 0.88372093 0.84665793]\n",
            "Epoch: 18/19, Step: 75/167, Training Loss: 0.1114, Elapsed time: 12102 sec [0.86284115 0.89450355 0.68210151]\n",
            "Epoch: 18/19, Step: 76/167, Training Loss: 0.2581, Elapsed time: 12114 sec [0.89495523 0.92558984 0.85485164]\n",
            "Epoch: 18/19, Step: 77/167, Training Loss: 0.1073, Elapsed time: 12122 sec [0.79807336 0.76708861 0.78942014]\n",
            "Epoch: 18/19, Step: 78/167, Training Loss: 0.1886, Elapsed time: 12132 sec [0.79405352 0.7115903  0.89090909]\n",
            "Epoch: 18/19, Step: 79/167, Training Loss: 0.3976, Elapsed time: 12142 sec [0.73676681 0.88542544 0.80820106]\n",
            "Epoch: 18/19, Step: 80/167, Training Loss: 0.1847, Elapsed time: 12151 sec [0.722177   0.90841911 0.89217895]\n",
            "Epoch: 18/19, Step: 81/167, Training Loss: 0.2017, Elapsed time: 12161 sec [0.8661287  0.82312253 0.83046358]\n",
            "Epoch: 18/19, Step: 82/167, Training Loss: 0.1292, Elapsed time: 12172 sec [0.6711613  0.81931633 0.77754386]\n",
            "Epoch: 18/19, Step: 83/167, Training Loss: 0.1807, Elapsed time: 12181 sec [0.86744894 0.7457847  0.83242259]\n",
            "Epoch: 18/19, Step: 84/167, Training Loss: 0.1185, Elapsed time: 12192 sec [0.86174164 0.72191838 0.79271357]\n",
            "Epoch: 18/19, Step: 85/167, Training Loss: 0.2224, Elapsed time: 12199 sec [0.89446267 0.74121406 0.78799489]\n",
            "Epoch: 18/19, Step: 86/167, Training Loss: 0.1757, Elapsed time: 12209 sec [0.8997255  0.76589595 0.61660777]\n",
            "Epoch: 18/19, Step: 87/167, Training Loss: 0.2881, Elapsed time: 12221 sec [0.93003167 0.73786408 0.72113677]\n",
            "Epoch: 18/19, Step: 88/167, Training Loss: 0.0872, Elapsed time: 12229 sec [0.87811321 0.85326087 0.87347561]\n",
            "Epoch: 18/19, Step: 89/167, Training Loss: 0.1599, Elapsed time: 12238 sec [0.93666667 0.87089552 0.88809947]\n",
            "Epoch: 18/19, Step: 90/167, Training Loss: 0.1464, Elapsed time: 12249 sec [0.93144424 0.83133971 0.87905935]\n",
            "Epoch: 18/19, Step: 91/167, Training Loss: 0.2047, Elapsed time: 12259 sec [0.91987454 0.91533181 0.88577154]\n",
            "Epoch: 18/19, Step: 92/167, Training Loss: 0.1630, Elapsed time: 12269 sec [0.92474263 0.81782249 0.87962419]\n",
            "Epoch: 18/19, Step: 93/167, Training Loss: 0.1424, Elapsed time: 12278 sec [0.90288551 0.90142276 0.87346025]\n",
            "Epoch: 18/19, Step: 94/167, Training Loss: 0.1458, Elapsed time: 12288 sec [0.90930199 0.91454139 0.88904796]\n",
            "Epoch: 18/19, Step: 95/167, Training Loss: 0.1287, Elapsed time: 12299 sec [0.87062691 0.8992416  0.85524126]\n",
            "Epoch: 18/19, Step: 96/167, Training Loss: 0.1484, Elapsed time: 12307 sec [0.90038314 0.90439932 0.879046  ]\n",
            "Epoch: 18/19, Step: 97/167, Training Loss: 0.1119, Elapsed time: 12316 sec [0.85608856 0.77200277 0.81419624]\n",
            "Epoch: 18/19, Step: 98/167, Training Loss: 0.1136, Elapsed time: 12327 sec [0.86597938 0.83397683 0.83538634]\n",
            "Epoch: 18/19, Step: 99/167, Training Loss: 0.1441, Elapsed time: 12335 sec [0.82540449 0.77867903 0.75936719]\n",
            "Epoch: 18/19, Step: 100/167, Training Loss: 0.1866, Elapsed time: 12346 sec [0.86603807 0.875412   0.84097035]\n",
            "Epoch: 18/19, Step: 101/167, Training Loss: 0.1229, Elapsed time: 12356 sec [0.90653484 0.92448191 0.9025544 ]\n",
            "Epoch: 18/19, Step: 102/167, Training Loss: 0.1147, Elapsed time: 12366 sec [0.87398374 0.91706648 0.87344029]\n",
            "Epoch: 18/19, Step: 103/167, Training Loss: 0.0782, Elapsed time: 12376 sec [0.85576703 0.85100287 0.83333333]\n",
            "Epoch: 18/19, Step: 104/167, Training Loss: 0.1034, Elapsed time: 12384 sec [0.83706356 0.87774295 0.86297004]\n",
            "Epoch: 18/19, Step: 105/167, Training Loss: 0.1023, Elapsed time: 12393 sec [0.89642375 0.87735004 0.8708134 ]\n",
            "Epoch: 18/19, Step: 106/167, Training Loss: 0.1530, Elapsed time: 12404 sec [0.91542694 0.84427408 0.79229122]\n",
            "Epoch: 18/19, Step: 107/167, Training Loss: 0.1258, Elapsed time: 12412 sec [0.84800741 0.6035627  0.77705977]\n",
            "Epoch: 18/19, Step: 108/167, Training Loss: 0.4714, Elapsed time: 12423 sec [0.89626363 0.89785933 0.88888889]\n",
            "Epoch: 18/19, Step: 109/167, Training Loss: 0.1037, Elapsed time: 12432 sec [0.91429936 0.88308351 0.79379845]\n",
            "Epoch: 18/19, Step: 110/167, Training Loss: 0.1509, Elapsed time: 12439 sec [0.79071023 0.56514914 0.68956407]\n",
            "Epoch: 18/19, Step: 111/167, Training Loss: 0.1567, Elapsed time: 12448 sec [0.80919765 0.78967066 0.86525555]\n",
            "Epoch: 18/19, Step: 112/167, Training Loss: 0.2391, Elapsed time: 12459 sec [0.86993997 0.80643372 0.89216447]\n",
            "Epoch: 18/19, Step: 113/167, Training Loss: 0.1489, Elapsed time: 12469 sec [0.87743823 0.87857143 0.85677252]\n",
            "Epoch: 18/19, Step: 114/167, Training Loss: 0.1493, Elapsed time: 12479 sec [0.80892338 0.89101498 0.84497041]\n",
            "Epoch: 18/19, Step: 115/167, Training Loss: 0.1242, Elapsed time: 12489 sec [0.86302368 0.84567901 0.875     ]\n",
            "Epoch: 18/19, Step: 116/167, Training Loss: 0.1273, Elapsed time: 12498 sec [0.83450109 0.77601411 0.82791817]\n",
            "Epoch: 18/19, Step: 117/167, Training Loss: 0.1186, Elapsed time: 12509 sec [0.84459624 0.88923802 0.87608696]\n",
            "Epoch: 18/19, Step: 118/167, Training Loss: 0.1076, Elapsed time: 12517 sec [0.85927367 0.8197106  0.83357664]\n",
            "Epoch: 18/19, Step: 119/167, Training Loss: 0.2281, Elapsed time: 12526 sec [0.87685629 0.8063475  0.86220647]\n",
            "Epoch: 18/19, Step: 120/167, Training Loss: 0.3041, Elapsed time: 12537 sec [0.8444728  0.91507937 0.89704997]\n",
            "Epoch: 18/19, Step: 121/167, Training Loss: 0.1988, Elapsed time: 12545 sec [0.89350574 0.87065053 0.91932203]\n",
            "Epoch: 18/19, Step: 122/167, Training Loss: 0.1269, Elapsed time: 12555 sec [0.79962937 0.88696904 0.85998014]\n",
            "Epoch: 18/19, Step: 123/167, Training Loss: 0.1361, Elapsed time: 12565 sec [0.90802978 0.89066667 0.8807947 ]\n",
            "Epoch: 18/19, Step: 124/167, Training Loss: 0.0743, Elapsed time: 12574 sec [0.80856621 0.87123746 0.87628244]\n",
            "Epoch: 18/19, Step: 125/167, Training Loss: 0.2206, Elapsed time: 12586 sec [0.87516779 0.9112426  0.89443997]\n",
            "Epoch: 18/19, Step: 126/167, Training Loss: 0.1235, Elapsed time: 12595 sec [0.81678552 0.89556801 0.85734463]\n",
            "Epoch: 18/19, Step: 127/167, Training Loss: 0.1736, Elapsed time: 12604 sec [0.86347992 0.94256575 0.89612403]\n",
            "Epoch: 18/19, Step: 128/167, Training Loss: 0.2054, Elapsed time: 12614 sec [0.87825223 0.86759957 0.83441982]\n",
            "Epoch: 18/19, Step: 129/167, Training Loss: 0.1795, Elapsed time: 12623 sec [0.9105919  0.9019286  0.84718826]\n",
            "Epoch: 18/19, Step: 130/167, Training Loss: 0.1136, Elapsed time: 12632 sec [0.88790479 0.93503669 0.9057454 ]\n",
            "Epoch: 18/19, Step: 131/167, Training Loss: 0.1482, Elapsed time: 12643 sec [0.8663325  0.70384866 0.73916737]\n",
            "Epoch: 18/19, Step: 132/167, Training Loss: 0.3340, Elapsed time: 12651 sec [0.88421053 0.90271636 0.80990191]\n",
            "Epoch: 18/19, Step: 133/167, Training Loss: 0.2364, Elapsed time: 12661 sec [0.81382828 0.73042862 0.65196342]\n",
            "Epoch: 18/19, Step: 134/167, Training Loss: 0.1724, Elapsed time: 12673 sec [0.81692573 0.84311377 0.76089517]\n",
            "Epoch: 18/19, Step: 135/167, Training Loss: 0.1621, Elapsed time: 12683 sec [0.87264974 0.60240964 0.57515204]\n",
            "Epoch: 18/19, Step: 136/167, Training Loss: 0.1897, Elapsed time: 12691 sec [0.8692607  0.80289093 0.74682396]\n",
            "Epoch: 18/19, Step: 137/167, Training Loss: 0.2110, Elapsed time: 12701 sec [0.87894103 0.89255319 0.84113475]\n",
            "Epoch: 18/19, Step: 138/167, Training Loss: 0.0862, Elapsed time: 12713 sec [0.9117984 0.8997996 0.8332064]\n",
            "Epoch: 18/19, Step: 139/167, Training Loss: 0.1075, Elapsed time: 12724 sec [0.88702687 0.86082751 0.85245902]\n",
            "Epoch: 18/19, Step: 140/167, Training Loss: 0.1512, Elapsed time: 12732 sec [0.89146522 0.85076923 0.80481513]\n",
            "Epoch: 18/19, Step: 141/167, Training Loss: 0.1308, Elapsed time: 12745 sec [0.92336854 0.91360477 0.87817259]\n",
            "Epoch: 18/19, Step: 142/167, Training Loss: 0.0955, Elapsed time: 12754 sec [0.91280014 0.83610962 0.85690093]\n",
            "Epoch: 18/19, Step: 143/167, Training Loss: 0.1569, Elapsed time: 12764 sec [0.91886269 0.88254535 0.86129754]\n",
            "Epoch: 18/19, Step: 144/167, Training Loss: 0.2305, Elapsed time: 12774 sec [0.88786531 0.81688534 0.82034976]\n",
            "Epoch: 18/19, Step: 145/167, Training Loss: 0.1562, Elapsed time: 12783 sec [0.90212183 0.87576375 0.87433359]\n",
            "Epoch: 18/19, Step: 146/167, Training Loss: 0.1542, Elapsed time: 12792 sec [0.82096664 0.85960709 0.87932159]\n",
            "Epoch: 18/19, Step: 147/167, Training Loss: 0.1863, Elapsed time: 12802 sec [0.85971529 0.72173609 0.83595019]\n",
            "Epoch: 18/19, Step: 148/167, Training Loss: 0.1696, Elapsed time: 12811 sec [0.91056911 0.91739311 0.8957476 ]\n",
            "Epoch: 18/19, Step: 149/167, Training Loss: 0.0984, Elapsed time: 12823 sec [0.87277896 0.88384513 0.7962963 ]\n",
            "Epoch: 18/19, Step: 150/167, Training Loss: 0.1425, Elapsed time: 12833 sec [0.89311232 0.8525971  0.87701863]\n",
            "Epoch: 18/19, Step: 151/167, Training Loss: 0.1542, Elapsed time: 12842 sec [0.93001422 0.89685792 0.87192366]\n",
            "Epoch: 18/19, Step: 152/167, Training Loss: 0.1151, Elapsed time: 12854 sec [0.89381741 0.89649682 0.8478803 ]\n",
            "Epoch: 18/19, Step: 153/167, Training Loss: 0.1543, Elapsed time: 12862 sec [0.76205561 0.68467037 0.79632063]\n",
            "Epoch: 18/19, Step: 154/167, Training Loss: 0.2252, Elapsed time: 12872 sec [0.9102635  0.86672052 0.87407407]\n",
            "Epoch: 18/19, Step: 155/167, Training Loss: 0.1860, Elapsed time: 12881 sec [0.90744498 0.90687437 0.90775862]\n",
            "Epoch: 18/19, Step: 156/167, Training Loss: 0.1111, Elapsed time: 12891 sec [0.81790476 0.89256198 0.88421053]\n",
            "Epoch: 18/19, Step: 157/167, Training Loss: 0.0823, Elapsed time: 12901 sec [0.89298124 0.85896269 0.84042553]\n",
            "Epoch: 18/19, Step: 158/167, Training Loss: 0.0826, Elapsed time: 12911 sec [0.90428844 0.94892996 0.90471607]\n",
            "Epoch: 18/19, Step: 159/167, Training Loss: 0.1632, Elapsed time: 12920 sec [0.92037747 0.92452122 0.89349398]\n",
            "Epoch: 18/19, Step: 160/167, Training Loss: 0.1207, Elapsed time: 12931 sec [0.74278019 0.70232558 0.71620831]\n",
            "Epoch: 18/19, Step: 161/167, Training Loss: 0.2622, Elapsed time: 12941 sec [0.91007033 0.90699734 0.88207298]\n",
            "Epoch: 18/19, Step: 162/167, Training Loss: 0.0917, Elapsed time: 12952 sec [0.89185229 0.87880936 0.87345385]\n",
            "Epoch: 18/19, Step: 163/167, Training Loss: 0.1411, Elapsed time: 12963 sec [0.8742127  0.9067954  0.88606195]\n",
            "Epoch: 18/19, Step: 164/167, Training Loss: 0.0773, Elapsed time: 12972 sec [0.85415013 0.7802154  0.72904884]\n",
            "Epoch: 18/19, Step: 165/167, Training Loss: 0.2558, Elapsed time: 12983 sec [0.89531112 0.92109635 0.89366391]\n",
            "Epoch: 18/19, Step: 166/167, Training Loss: 0.0933, Elapsed time: 12993 sec [0.89166314 0.81078046 0.81672026]\n",
            "Epoch: 18/19, Step: 167/167, Training Loss: 0.1452, Elapsed time: 12999 sec \n",
            "train_wt_dice: 0.8661\n",
            "train_ct_dice: 0.8375\n",
            "train_at_dice: 0.8276\n",
            "Epoch: 18/19, Mean Training Loss: 0.1627, Epoch elapsed time: 1622 sec\n",
            "Epoch: 18/19, Validation Step: 22/21, Validation Loss: 0.8309, Elapsed time: 13201 sec \n",
            "val_wt_dice:  0.8819\n",
            "val_ct_dice:    0.8452\n",
            "val_at_dice:    0.8689\n",
            "Epoch: 18/19, Mean Validation Loss: 0.2140, Elapsed time: 1824 sec\n",
            "\n",
            "[0.87628594 0.578564   0.4989154 ]\n",
            "Epoch: 19/19, Step: 1/167, Training Loss: 0.1431, Elapsed time: 13211 sec [0.91057649 0.91384181 0.8684645 ]\n",
            "Epoch: 19/19, Step: 2/167, Training Loss: 0.1291, Elapsed time: 13221 sec [0.91175775 0.91024899 0.88143177]\n",
            "Epoch: 19/19, Step: 3/167, Training Loss: 0.0958, Elapsed time: 13230 sec [0.90322581 0.93019993 0.89922481]\n",
            "Epoch: 19/19, Step: 4/167, Training Loss: 0.0881, Elapsed time: 13240 sec [0.89135441 0.89611082 0.86129754]\n",
            "Epoch: 19/19, Step: 5/167, Training Loss: 0.1403, Elapsed time: 13251 sec [0.91505912 0.93413174 0.88518739]\n",
            "Epoch: 19/19, Step: 6/167, Training Loss: 0.1214, Elapsed time: 13259 sec [0.83503944 0.89017341 0.8650755 ]\n",
            "Epoch: 19/19, Step: 7/167, Training Loss: 0.1429, Elapsed time: 13270 sec [0.89257159 0.78184034 0.84565499]\n",
            "Epoch: 19/19, Step: 8/167, Training Loss: 0.1449, Elapsed time: 13281 sec [0.87959127 0.814562   0.90520922]\n",
            "Epoch: 19/19, Step: 9/167, Training Loss: 0.1431, Elapsed time: 13290 sec [0.87927928 0.90436397 0.90144231]\n",
            "Epoch: 19/19, Step: 10/167, Training Loss: 0.1028, Elapsed time: 13301 sec [0.90150094 0.89426399 0.84199584]\n",
            "Epoch: 19/19, Step: 11/167, Training Loss: 0.1477, Elapsed time: 13309 sec [0.90220684 0.90460307 0.84985836]\n",
            "Epoch: 19/19, Step: 12/167, Training Loss: 0.1317, Elapsed time: 13320 sec [0.91876451 0.83294118 0.75268817]\n",
            "Epoch: 19/19, Step: 13/167, Training Loss: 0.2188, Elapsed time: 13330 sec [0.9259962  0.89939024 0.84275862]\n",
            "Epoch: 19/19, Step: 14/167, Training Loss: 0.1020, Elapsed time: 13338 sec [0.86419753 0.82214984 0.83080513]\n",
            "Epoch: 19/19, Step: 15/167, Training Loss: 0.1545, Elapsed time: 13349 sec [0.92625869 0.83321247 0.74535166]\n",
            "Epoch: 19/19, Step: 16/167, Training Loss: 0.2529, Elapsed time: 13358 sec [0.74519088 0.60264026 0.66486121]\n",
            "Epoch: 19/19, Step: 17/167, Training Loss: 0.3558, Elapsed time: 13367 sec [0.88428038 0.88543897 0.85090361]\n",
            "Epoch: 19/19, Step: 18/167, Training Loss: 0.1042, Elapsed time: 13378 sec [0.8929385  0.71246819 0.77765608]\n",
            "Epoch: 19/19, Step: 19/167, Training Loss: 0.1241, Elapsed time: 13387 sec [0.82206311 0.72245236 0.75191571]\n",
            "Epoch: 19/19, Step: 20/167, Training Loss: 0.1626, Elapsed time: 13396 sec [0.83640738 0.78119508 0.85754784]\n",
            "Epoch: 19/19, Step: 21/167, Training Loss: 0.1909, Elapsed time: 13406 sec [0.91373183 0.76565657 0.77955665]\n",
            "Epoch: 19/19, Step: 22/167, Training Loss: 0.2052, Elapsed time: 13415 sec [0.89952858 0.78542141 0.88955672]\n",
            "Epoch: 19/19, Step: 23/167, Training Loss: 0.1731, Elapsed time: 13426 sec [0.84953204 0.82157018 0.75204678]\n",
            "Epoch: 19/19, Step: 24/167, Training Loss: 0.3383, Elapsed time: 13436 sec [0.85763042 0.91404255 0.8988764 ]\n",
            "Epoch: 19/19, Step: 25/167, Training Loss: 0.0945, Elapsed time: 13445 sec [0.8689878  0.86833013 0.88877174]\n",
            "Epoch: 19/19, Step: 26/167, Training Loss: 0.1354, Elapsed time: 13455 sec [0.85049389 0.76887661 0.8791894 ]\n",
            "Epoch: 19/19, Step: 27/167, Training Loss: 0.1606, Elapsed time: 13464 sec [0.87879269 0.81649245 0.77928693]\n",
            "Epoch: 19/19, Step: 28/167, Training Loss: 0.2375, Elapsed time: 13474 sec [0.86774036 0.92764045 0.88620199]\n",
            "Epoch: 19/19, Step: 29/167, Training Loss: 0.1475, Elapsed time: 13483 sec [0.74679407 0.89622642 0.86702703]\n",
            "Epoch: 19/19, Step: 30/167, Training Loss: 0.1391, Elapsed time: 13492 sec [0.809439   0.71372549 0.77281553]\n",
            "Epoch: 19/19, Step: 31/167, Training Loss: 0.3151, Elapsed time: 13501 sec [0.7874734  0.76760563 0.74882261]\n",
            "Epoch: 19/19, Step: 32/167, Training Loss: 0.2278, Elapsed time: 13512 sec [0.86618782 0.60743134 0.72105263]\n",
            "Epoch: 19/19, Step: 33/167, Training Loss: 0.1730, Elapsed time: 13520 sec [0.87564103 0.875      0.85353535]\n",
            "Epoch: 19/19, Step: 34/167, Training Loss: 0.1387, Elapsed time: 13529 sec [0.86469552 0.83380282 0.83252033]\n",
            "Epoch: 19/19, Step: 35/167, Training Loss: 0.1024, Elapsed time: 13540 sec [0.85680473 0.79212674 0.73940345]\n",
            "Epoch: 19/19, Step: 36/167, Training Loss: 0.1759, Elapsed time: 13548 sec [0.8863189  0.8960396  0.85033365]\n",
            "Epoch: 19/19, Step: 37/167, Training Loss: 0.1815, Elapsed time: 13561 sec [0.86575133 0.79433962 0.68758717]\n",
            "Epoch: 19/19, Step: 38/167, Training Loss: 0.2021, Elapsed time: 13571 sec [0.86050241 0.90660592 0.86656201]\n",
            "Epoch: 19/19, Step: 39/167, Training Loss: 0.0742, Elapsed time: 13580 sec [0.86554443 0.78482104 0.76837209]\n",
            "Epoch: 19/19, Step: 40/167, Training Loss: 0.1302, Elapsed time: 13589 sec [0.89119683 0.80964686 0.83698297]\n",
            "Epoch: 19/19, Step: 41/167, Training Loss: 0.1012, Elapsed time: 13598 sec [0.85526061 0.86905633 0.77812178]\n",
            "Epoch: 19/19, Step: 42/167, Training Loss: 0.1472, Elapsed time: 13608 sec [0.84108065 0.9097852  0.87491899]\n",
            "Epoch: 19/19, Step: 43/167, Training Loss: 0.1455, Elapsed time: 13618 sec [0.90177332 0.88606726 0.83828996]\n",
            "Epoch: 19/19, Step: 44/167, Training Loss: 0.0917, Elapsed time: 13627 sec [0.88165517 0.86482759 0.86187845]\n",
            "Epoch: 19/19, Step: 45/167, Training Loss: 0.0631, Elapsed time: 13638 sec [0.85790567 0.90871654 0.85742574]\n",
            "Epoch: 19/19, Step: 46/167, Training Loss: 0.0952, Elapsed time: 13648 sec [0.82958398 0.86654135 0.76883117]\n",
            "Epoch: 19/19, Step: 47/167, Training Loss: 0.1064, Elapsed time: 13655 sec [0.67754128 0.8355681  0.8564397 ]\n",
            "Epoch: 19/19, Step: 48/167, Training Loss: 0.2065, Elapsed time: 13666 sec [0.8672595  0.88376655 0.86913229]\n",
            "Epoch: 19/19, Step: 49/167, Training Loss: 0.1400, Elapsed time: 13677 sec [0.90773333 0.866      0.85659656]\n",
            "Epoch: 19/19, Step: 50/167, Training Loss: 0.1652, Elapsed time: 13687 sec [0.87934631 0.77955759 0.80771291]\n",
            "Epoch: 19/19, Step: 51/167, Training Loss: 0.2747, Elapsed time: 13696 sec [0.84101312 0.66615385 0.57404022]\n",
            "Epoch: 19/19, Step: 52/167, Training Loss: 0.1385, Elapsed time: 13705 sec [0.89216684 0.93050571 0.89003276]\n",
            "Epoch: 19/19, Step: 53/167, Training Loss: 0.1395, Elapsed time: 13716 sec [0.8610729  0.78827519 0.76734349]\n",
            "Epoch: 19/19, Step: 54/167, Training Loss: 0.5720, Elapsed time: 13724 sec [0.90255786 0.82886461 0.75130435]\n",
            "Epoch: 19/19, Step: 55/167, Training Loss: 0.2556, Elapsed time: 13734 sec [0.87240886 0.92532795 0.90768147]\n",
            "Epoch: 19/19, Step: 56/167, Training Loss: 0.0975, Elapsed time: 13745 sec [0.85268485 0.69891173 0.51302605]\n",
            "Epoch: 19/19, Step: 57/167, Training Loss: 0.1892, Elapsed time: 13754 sec [0.87013596 0.67962466 0.52755906]\n",
            "Epoch: 19/19, Step: 58/167, Training Loss: 0.2552, Elapsed time: 13764 sec [0.78834652 0.70483005 0.80879414]\n",
            "Epoch: 19/19, Step: 59/167, Training Loss: 0.1496, Elapsed time: 13774 sec [0.86064569 0.81962482 0.84542587]\n",
            "Epoch: 19/19, Step: 60/167, Training Loss: 0.1711, Elapsed time: 13783 sec [0.79697828 0.89283791 0.88936782]\n",
            "Epoch: 19/19, Step: 61/167, Training Loss: 0.1225, Elapsed time: 13795 sec [0.80415828 0.83740784 0.79883041]\n",
            "Epoch: 19/19, Step: 62/167, Training Loss: 0.1643, Elapsed time: 13803 sec [0.85137219 0.87424699 0.86809617]\n",
            "Epoch: 19/19, Step: 63/167, Training Loss: 0.1546, Elapsed time: 13812 sec [0.83905365 0.86214953 0.83272727]\n",
            "Epoch: 19/19, Step: 64/167, Training Loss: 0.1320, Elapsed time: 13824 sec [0.91341991 0.93968576 0.90429043]\n",
            "Epoch: 19/19, Step: 65/167, Training Loss: 0.0980, Elapsed time: 13831 sec [0.86725664 0.78842315 0.87417219]\n",
            "Epoch: 19/19, Step: 66/167, Training Loss: 0.1002, Elapsed time: 13841 sec [0.88346015 0.83891892 0.86697674]\n",
            "Epoch: 19/19, Step: 67/167, Training Loss: 0.1409, Elapsed time: 13852 sec [0.95095095 0.92706645 0.92941176]\n",
            "Epoch: 19/19, Step: 68/167, Training Loss: 0.0763, Elapsed time: 13861 sec [0.87351466 0.9145394  0.89556136]\n",
            "Epoch: 19/19, Step: 69/167, Training Loss: 0.1323, Elapsed time: 13871 sec [0.91634596 0.88406564 0.89240506]\n",
            "Epoch: 19/19, Step: 70/167, Training Loss: 0.1417, Elapsed time: 13880 sec [0.87835752 0.94529215 0.86389776]\n",
            "Epoch: 19/19, Step: 71/167, Training Loss: 0.2813, Elapsed time: 13889 sec [0.91765194 0.8569863  0.89260143]\n",
            "Epoch: 19/19, Step: 72/167, Training Loss: 0.1242, Elapsed time: 13900 sec [0.89478138 0.85638767 0.80745342]\n",
            "Epoch: 19/19, Step: 73/167, Training Loss: 0.0867, Elapsed time: 13909 sec [0.88609763 0.93167028 0.86881383]\n",
            "Epoch: 19/19, Step: 74/167, Training Loss: 0.0963, Elapsed time: 13920 sec [0.85238693 0.89098532 0.85862516]\n",
            "Epoch: 19/19, Step: 75/167, Training Loss: 0.0994, Elapsed time: 13930 sec [0.87369723 0.91573785 0.71518419]\n",
            "Epoch: 19/19, Step: 76/167, Training Loss: 0.2397, Elapsed time: 13940 sec [0.89494333 0.92779783 0.85463259]\n",
            "Epoch: 19/19, Step: 77/167, Training Loss: 0.1070, Elapsed time: 13950 sec [0.80832708 0.81132075 0.7966457 ]\n",
            "Epoch: 19/19, Step: 78/167, Training Loss: 0.1764, Elapsed time: 13959 sec [0.80023971 0.70608108 0.8969555 ]\n",
            "Epoch: 19/19, Step: 79/167, Training Loss: 0.3948, Elapsed time: 13970 sec [0.73627749 0.89350649 0.83056478]\n",
            "Epoch: 19/19, Step: 80/167, Training Loss: 0.1803, Elapsed time: 13979 sec [0.72731498 0.91133912 0.89360334]\n",
            "Epoch: 19/19, Step: 81/167, Training Loss: 0.1999, Elapsed time: 13988 sec [0.86721649 0.83912612 0.84048257]\n",
            "Epoch: 19/19, Step: 82/167, Training Loss: 0.1212, Elapsed time: 14000 sec [0.6714357  0.81681034 0.78556558]\n",
            "Epoch: 19/19, Step: 83/167, Training Loss: 0.1735, Elapsed time: 14009 sec [0.87543554 0.76626826 0.85447761]\n",
            "Epoch: 19/19, Step: 84/167, Training Loss: 0.1131, Elapsed time: 14018 sec [0.84673707 0.73073666 0.79398873]\n",
            "Epoch: 19/19, Step: 85/167, Training Loss: 0.2212, Elapsed time: 14029 sec [0.90045767 0.73408578 0.7905189 ]\n",
            "Epoch: 19/19, Step: 86/167, Training Loss: 0.1737, Elapsed time: 14039 sec [0.90579476 0.75727229 0.62286229]\n",
            "Epoch: 19/19, Step: 87/167, Training Loss: 0.2769, Elapsed time: 14050 sec [0.93466476 0.72851296 0.71914132]\n",
            "Epoch: 19/19, Step: 88/167, Training Loss: 0.0856, Elapsed time: 14058 sec [0.88127154 0.85451595 0.88396947]\n",
            "Epoch: 19/19, Step: 89/167, Training Loss: 0.1615, Elapsed time: 14068 sec [0.94078287 0.86548411 0.90777577]\n",
            "Epoch: 19/19, Step: 90/167, Training Loss: 0.1450, Elapsed time: 14079 sec [0.93600735 0.83604581 0.87683616]\n",
            "Epoch: 19/19, Step: 91/167, Training Loss: 0.2068, Elapsed time: 14088 sec [0.91794358 0.92342684 0.8927477 ]\n",
            "Epoch: 19/19, Step: 92/167, Training Loss: 0.1595, Elapsed time: 14099 sec [0.92928571 0.81994261 0.88410017]\n",
            "Epoch: 19/19, Step: 93/167, Training Loss: 0.1471, Elapsed time: 14109 sec [0.90308507 0.89503328 0.88839286]\n",
            "Epoch: 19/19, Step: 94/167, Training Loss: 0.1470, Elapsed time: 14118 sec [0.91100395 0.89707865 0.86647727]\n",
            "Epoch: 19/19, Step: 95/167, Training Loss: 0.1327, Elapsed time: 14130 sec [0.8771847  0.89356984 0.86468085]\n",
            "Epoch: 19/19, Step: 96/167, Training Loss: 0.1565, Elapsed time: 14138 sec [0.90342788 0.91302503 0.88774341]\n",
            "Epoch: 19/19, Step: 97/167, Training Loss: 0.1069, Elapsed time: 14151 sec [0.85820204 0.75549451 0.79375   ]\n",
            "Epoch: 19/19, Step: 98/167, Training Loss: 0.1172, Elapsed time: 14160 sec [0.85303244 0.82929936 0.8206278 ]\n",
            "Epoch: 19/19, Step: 99/167, Training Loss: 0.1438, Elapsed time: 14169 sec [0.8235012  0.78984652 0.78366638]\n",
            "Epoch: 19/19, Step: 100/167, Training Loss: 0.1876, Elapsed time: 14178 sec [0.86554922 0.88251001 0.83985765]\n",
            "Epoch: 19/19, Step: 101/167, Training Loss: 0.1206, Elapsed time: 14188 sec [0.91032987 0.92128486 0.90056818]\n",
            "Epoch: 19/19, Step: 102/167, Training Loss: 0.1138, Elapsed time: 14200 sec [0.87732558 0.90357384 0.86496028]\n",
            "Epoch: 19/19, Step: 103/167, Training Loss: 0.0768, Elapsed time: 14209 sec [0.84222423 0.86430678 0.84758364]\n",
            "Epoch: 19/19, Step: 104/167, Training Loss: 0.1025, Elapsed time: 14217 sec [0.83821884 0.8653169  0.84605757]\n",
            "Epoch: 19/19, Step: 105/167, Training Loss: 0.1044, Elapsed time: 14227 sec [0.90117647 0.86289632 0.85296075]\n",
            "Epoch: 19/19, Step: 106/167, Training Loss: 0.1490, Elapsed time: 14237 sec [0.91912657 0.84195539 0.79237288]\n",
            "Epoch: 19/19, Step: 107/167, Training Loss: 0.1191, Elapsed time: 14245 sec [0.86388498 0.63351499 0.79504132]\n",
            "Epoch: 19/19, Step: 108/167, Training Loss: 0.4392, Elapsed time: 14256 sec [0.89960215 0.90594059 0.90265487]\n",
            "Epoch: 19/19, Step: 109/167, Training Loss: 0.1095, Elapsed time: 14266 sec [0.92366779 0.88986014 0.80766161]\n",
            "Epoch: 19/19, Step: 110/167, Training Loss: 0.1468, Elapsed time: 14275 sec [0.81875    0.61394558 0.74857143]\n",
            "Epoch: 19/19, Step: 111/167, Training Loss: 0.1476, Elapsed time: 14284 sec [0.82282878 0.80046048 0.85743909]\n",
            "Epoch: 19/19, Step: 112/167, Training Loss: 0.2406, Elapsed time: 14294 sec [0.87039312 0.81935847 0.8776435 ]\n",
            "Epoch: 19/19, Step: 113/167, Training Loss: 0.1413, Elapsed time: 14304 sec [0.87375143 0.87939698 0.85638999]\n",
            "Epoch: 19/19, Step: 114/167, Training Loss: 0.1565, Elapsed time: 14313 sec [0.80923451 0.88103161 0.83442136]\n",
            "Epoch: 19/19, Step: 115/167, Training Loss: 0.1305, Elapsed time: 14324 sec [0.86302368 0.83661848 0.86224784]\n",
            "Epoch: 19/19, Step: 116/167, Training Loss: 0.1261, Elapsed time: 14333 sec [0.83164496 0.77835951 0.81533101]\n",
            "Epoch: 19/19, Step: 117/167, Training Loss: 0.1173, Elapsed time: 14342 sec [0.85018094 0.89559165 0.87578947]\n",
            "Epoch: 19/19, Step: 118/167, Training Loss: 0.1004, Elapsed time: 14353 sec [0.86006717 0.80790514 0.84072727]\n",
            "Epoch: 19/19, Step: 119/167, Training Loss: 0.2250, Elapsed time: 14362 sec [0.87206799 0.80042463 0.8678745 ]\n",
            "Epoch: 19/19, Step: 120/167, Training Loss: 0.2987, Elapsed time: 14373 sec [0.83930858 0.90502355 0.88399762]\n",
            "Epoch: 19/19, Step: 121/167, Training Loss: 0.2070, Elapsed time: 14384 sec [0.89491328 0.87504747 0.9260274 ]\n",
            "Epoch: 19/19, Step: 122/167, Training Loss: 0.1216, Elapsed time: 14393 sec [0.79990693 0.86858749 0.83965015]\n",
            "Epoch: 19/19, Step: 123/167, Training Loss: 0.1342, Elapsed time: 14403 sec [0.91442444 0.89319092 0.8846473 ]\n",
            "Epoch: 19/19, Step: 124/167, Training Loss: 0.0706, Elapsed time: 14413 sec [0.82301239 0.88548318 0.89010322]\n",
            "Epoch: 19/19, Step: 125/167, Training Loss: 0.2110, Elapsed time: 14421 sec [0.87534086 0.91188646 0.89139179]\n",
            "Epoch: 19/19, Step: 126/167, Training Loss: 0.1278, Elapsed time: 14432 sec [0.80649601 0.88798371 0.85289958]\n",
            "Epoch: 19/19, Step: 127/167, Training Loss: 0.1743, Elapsed time: 14441 sec [0.85876777 0.94577007 0.89753773]\n",
            "Epoch: 19/19, Step: 128/167, Training Loss: 0.2052, Elapsed time: 14452 sec [0.88119211 0.89438944 0.86613119]\n",
            "Epoch: 19/19, Step: 129/167, Training Loss: 0.1758, Elapsed time: 14460 sec [0.91103866 0.89877551 0.85731933]\n",
            "Epoch: 19/19, Step: 130/167, Training Loss: 0.1081, Elapsed time: 14470 sec [0.87770371 0.93597396 0.91045899]\n",
            "Epoch: 19/19, Step: 131/167, Training Loss: 0.1493, Elapsed time: 14481 sec [0.85199836 0.69483871 0.72667217]\n",
            "Epoch: 19/19, Step: 132/167, Training Loss: 0.3183, Elapsed time: 14490 sec [0.87923729 0.89879058 0.80411407]\n",
            "Epoch: 19/19, Step: 133/167, Training Loss: 0.2505, Elapsed time: 14501 sec [0.78075155 0.67962466 0.60425953]\n",
            "Epoch: 19/19, Step: 134/167, Training Loss: 0.1894, Elapsed time: 14513 sec [0.81128505 0.85241935 0.77672209]\n",
            "Epoch: 19/19, Step: 135/167, Training Loss: 0.1567, Elapsed time: 14523 sec [0.88070175 0.60486674 0.58303887]\n",
            "Epoch: 19/19, Step: 136/167, Training Loss: 0.1873, Elapsed time: 14533 sec [0.86581306 0.81785124 0.76500229]\n",
            "Epoch: 19/19, Step: 137/167, Training Loss: 0.1984, Elapsed time: 14542 sec [0.87560038 0.89314195 0.83994334]\n",
            "Epoch: 19/19, Step: 138/167, Training Loss: 0.0864, Elapsed time: 14554 sec [0.91819057 0.90361446 0.83678161]\n",
            "Epoch: 19/19, Step: 139/167, Training Loss: 0.1052, Elapsed time: 14564 sec [0.9005541  0.88109589 0.88153846]\n",
            "Epoch: 19/19, Step: 140/167, Training Loss: 0.1475, Elapsed time: 14574 sec [0.89575    0.8693137  0.82829181]\n",
            "Epoch: 19/19, Step: 141/167, Training Loss: 0.1299, Elapsed time: 14586 sec [0.92395748 0.8988989  0.84974093]\n",
            "Epoch: 19/19, Step: 142/167, Training Loss: 0.1031, Elapsed time: 14594 sec [0.90668599 0.8558952  0.85237698]\n",
            "Epoch: 19/19, Step: 143/167, Training Loss: 0.1530, Elapsed time: 14606 sec [0.92633773 0.8931523  0.84026258]\n",
            "Epoch: 19/19, Step: 144/167, Training Loss: 0.2090, Elapsed time: 14616 sec [0.88832648 0.80355161 0.80249805]\n",
            "Epoch: 19/19, Step: 145/167, Training Loss: 0.1571, Elapsed time: 14626 sec [0.90589367 0.87576375 0.87349398]\n",
            "Epoch: 19/19, Step: 146/167, Training Loss: 0.1577, Elapsed time: 14636 sec [0.82205259 0.85090218 0.86888454]\n",
            "Epoch: 19/19, Step: 147/167, Training Loss: 0.1812, Elapsed time: 14645 sec [0.85891813 0.71143251 0.82397004]\n",
            "Epoch: 19/19, Step: 148/167, Training Loss: 0.1724, Elapsed time: 14656 sec [0.90999448 0.92377796 0.893531  ]\n",
            "Epoch: 19/19, Step: 149/167, Training Loss: 0.0884, Elapsed time: 14665 sec [0.86487759 0.88098375 0.78947368]\n",
            "Epoch: 19/19, Step: 150/167, Training Loss: 0.1381, Elapsed time: 14674 sec [0.90027342 0.86296823 0.88025078]\n",
            "Epoch: 19/19, Step: 151/167, Training Loss: 0.1505, Elapsed time: 14686 sec [0.93394207 0.90136986 0.87366819]\n",
            "Epoch: 19/19, Step: 152/167, Training Loss: 0.1123, Elapsed time: 14696 sec [0.90052195 0.91589537 0.85732244]\n",
            "Epoch: 19/19, Step: 153/167, Training Loss: 0.1440, Elapsed time: 14705 sec [0.75888502 0.66931007 0.76349614]\n",
            "Epoch: 19/19, Step: 154/167, Training Loss: 0.2219, Elapsed time: 14715 sec [0.91105417 0.86495308 0.87087435]\n",
            "Epoch: 19/19, Step: 155/167, Training Loss: 0.1884, Elapsed time: 14724 sec [0.90875117 0.90368368 0.90798611]\n",
            "Epoch: 19/19, Step: 156/167, Training Loss: 0.1133, Elapsed time: 14737 sec [0.81578448 0.89799809 0.90163934]\n",
            "Epoch: 19/19, Step: 157/167, Training Loss: 0.0828, Elapsed time: 14746 sec [0.8925734  0.85125448 0.83783784]\n",
            "Epoch: 19/19, Step: 158/167, Training Loss: 0.0770, Elapsed time: 14756 sec [0.90066492 0.95231072 0.91666667]\n",
            "Epoch: 19/19, Step: 159/167, Training Loss: 0.1605, Elapsed time: 14767 sec [0.91964809 0.92537313 0.90433127]\n",
            "Epoch: 19/19, Step: 160/167, Training Loss: 0.1125, Elapsed time: 14776 sec [0.74041167 0.69923484 0.70157068]\n",
            "Epoch: 19/19, Step: 161/167, Training Loss: 0.2608, Elapsed time: 14786 sec [0.90638044 0.92064923 0.89518717]\n",
            "Epoch: 19/19, Step: 162/167, Training Loss: 0.0884, Elapsed time: 14796 sec [0.88879108 0.88193457 0.87729469]\n",
            "Epoch: 19/19, Step: 163/167, Training Loss: 0.1387, Elapsed time: 14808 sec [0.87631103 0.91694852 0.88802661]\n",
            "Epoch: 19/19, Step: 164/167, Training Loss: 0.0761, Elapsed time: 14817 sec [0.86040247 0.77972865 0.72875318]\n",
            "Epoch: 19/19, Step: 165/167, Training Loss: 0.2510, Elapsed time: 14826 sec [0.89528873 0.91611296 0.89319314]\n",
            "Epoch: 19/19, Step: 166/167, Training Loss: 0.0923, Elapsed time: 14837 sec [0.89515268 0.81981982 0.82038835]\n",
            "Epoch: 19/19, Step: 167/167, Training Loss: 0.1404, Elapsed time: 14844 sec \n",
            "train_wt_dice: 0.8682\n",
            "train_ct_dice: 0.842\n",
            "train_at_dice: 0.8297\n",
            "Epoch: 19/19, Mean Training Loss: 0.1597, Epoch elapsed time: 1643 sec\n",
            "Epoch: 19/19, Validation Step: 22/21, Validation Loss: 0.8295, Elapsed time: 15048 sec "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/01/28 20:38:18 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.0+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.1.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "val_wt_dice:  0.8846\n",
            "val_ct_dice:    0.8474\n",
            "val_at_dice:    0.8707\n",
            "Epoch: 19/19, Mean Validation Loss: 0.2099, Elapsed time: 1847 sec\n",
            "\n",
            "Total training time: 15048 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/01/28 20:38:33 WARNING mlflow.utils.requirements_utils: Found dgl version (2.0.0+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'dgl==2.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "###############################\n",
        "# Allenare sage con i nuovi dati\n",
        "#################################\n",
        "import torch.nn.functional as F\n",
        "INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/processed_15000_0.1_0' #\n",
        "\n",
        "TRAIN_PATH = os.path.join(INPUT_PATH,'train')\n",
        "VAL_PATH = os.path.join(INPUT_PATH,'val')\n",
        "TEST_PATH = os.path.join(INPUT_PATH,'test') # set to 'test' in production\n",
        "\n",
        "\n",
        "train_dataset = ImageGraphDataset(TRAIN_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True, glob_features=global_features)\n",
        "val_dataset = ImageGraphDataset(VAL_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True, glob_features=global_features)\n",
        "test_dataset = ImageGraphDataset(TEST_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True, glob_features=global_features)\n",
        "\n",
        "\n",
        "model_name = 'GraphSage'\n",
        "TRAIN_MODEL = True\n",
        "LOAD_MODEL = True # resume training\n",
        "\n",
        "supervised = True\n",
        "eval_metrics = None\n",
        "num_workers = 0\n",
        "\n",
        "batch_size = 6\n",
        "num_epochs = 8\n",
        "lr = 5e-4\n",
        "#input_feats = 20\n",
        "\n",
        "# val weights torch.Tensor([ 1.07, 23.0, 110.7, 72.2])\n",
        "# train weights torch.Tensor([ 1.06, 26.15, 127.05, 83.67]).to(device) #torch.Tensor([0.1, 2.2, 10.5, 7]).to(device)\n",
        "class_weights = torch.Tensor([0.2, 1, 3, 3]).to(device)\n",
        "layer_sizes=[256]*4\n",
        "n_classes=4\n",
        "aggregator_type='mean'\n",
        "\n",
        "k_val=3\n",
        "heads = [128]*3\n",
        "residuals = [128]*3\n",
        "heads = [8, 8, 8, 8, 8, 8]\n",
        "residuals = [False, True, True, False, True, True]\n",
        "activation = F.relu\n",
        "val_dropout = 0.1\n",
        "val_feat_drop = 0.2\n",
        "val_attn_drop = 0.2\n",
        "\n",
        "model = None\n",
        "\n",
        "if(model_name == 'GraphSage'):\n",
        "    #run_id = '42365d77ea8c47b6b6fbf9d938ea5f65'\n",
        "    dict_params = {k:eval(k) for k in ['val_dropout','input_feats', 'class_weights', 'layer_sizes', 'aggregator_type']}\n",
        "    model = GraphSage(in_feats=input_feats,\n",
        "                  layer_sizes=layer_sizes,\n",
        "                  n_classes=n_classes,\n",
        "                  aggregator_type=aggregator_type,\n",
        "                  dropout=val_dropout)\n",
        "elif(model_name == 'GIN'):\n",
        "    dict_params = {k:eval(k) for k in ['val_dropout','input_feats', 'class_weights', 'layer_sizes']}\n",
        "    model = GIN(in_feats=input_feats,\n",
        "                  layer_sizes=layer_sizes,\n",
        "                  n_classes=n_classes,\n",
        "                  dropout=val_dropout)\n",
        "elif(model_name == 'ChebNet'):\n",
        "    model = ChebNet(input_feats, layer_sizes, n_classes, k_val, val_dropout)\n",
        "    dict_params = {k:eval(k) for k in ['val_dropout','input_feats', 'class_weights', 'k_val', 'layer_sizes']}\n",
        "elif(model_name == 'GAT'):\n",
        "    dict_params = {k:eval(k) for k in ['val_dropout', 'val_feat_drop','val_attn_drop','input_feats', 'class_weights', 'layer_sizes', 'activation', 'heads', 'residuals']}\n",
        "    model = GAT(in_feats = input_feats, layer_sizes = layer_sizes, n_classes=n_classes,heads = heads, residuals = residuals, activation = activation,  feat_drop = val_feat_drop, attn_drop = val_attn_drop)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=lr,weight_decay=1e-4)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "#loss_fn = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
        "#loss_fn = LossBraTS(focal=False)\n",
        "\n",
        "wrapper = ModelWrapper(model = model,\n",
        "                            loss_fn = loss_fn,\n",
        "                            optimizer = optimizer,\n",
        "                            supervised = supervised,\n",
        "                            dict_params = dict_params,\n",
        "                            isgnn = True,\n",
        "                            num_epochs = num_epochs,\n",
        "                            LOAD_MODEL = LOAD_MODEL,\n",
        "                            eval_metrics = eval_metrics,\n",
        "                          )\n",
        "                            #run_id = '42365d77ea8c47b6b6fbf9d938ea5f65'\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset = train_dataset,\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers,\n",
        "                              collate_fn=minibatch_graphs)\n",
        "\n",
        "val_loader = DataLoader(dataset = val_dataset,\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers,\n",
        "                              collate_fn=minibatch_graphs)\n",
        "\n",
        "\n",
        "\n",
        "print('Elapsed epochs: ' + str(wrapper.elapsed_epochs))\n",
        "\n",
        "if(TRAIN_MODEL):\n",
        "    training_loss, validation_loss = wrapper.train(train_loader = train_loader, val_loader = val_loader )\n",
        "    #torch.cuda.empty_cache()\n",
        "\n",
        "if(LOAD_MODEL):\n",
        "  xx_train = range(len(wrapper.training_loss))\n",
        "  xx_val = range(len(wrapper.validation_loss))\n",
        "  plt.title('Loss curve')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(xx_train, wrapper.training_loss, '-o', label = 'Train')\n",
        "  plt.plot(xx_val, wrapper.validation_loss,'-o', label = 'Val')\n",
        "  plt.legend(loc='lower left')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xolTo38UlNnP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GCLW2sRSlxhW"
      },
      "outputs": [],
      "source": [
        "! rm -r '/content/drive/MyDrive/Lorusso/BraTS/data/segmentation/BraTS2023-ValidationData/'\n",
        "! mkdir '/content/drive/MyDrive/Lorusso/BraTS/data/segmentation/BraTS2023-ValidationData/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wVE18k6M5O3b"
      },
      "outputs": [],
      "source": [
        "#testdata2023 = ImageGraphDataset('/content/drive/MyDrive/Lorusso/BraTS/data/processed_15000_0.5_0/BraTS2023-ValidationData','BraTS-GLI-',read_image=False,read_graph=True,read_label=False)\n",
        "#predictions = wrapper.predict_graph(testdata2023)\n",
        "#seg_to_challenge()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_J8e-G2cJQxr"
      },
      "outputs": [],
      "source": [
        "#rm_run('BraTS_GraphSage',[\t'50144ee94c044e65b36faa631448a56a'\t,'ef211f890cfd40719edeb48128bf5ed0'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1_TRMmsm6jY"
      },
      "source": [
        "# TESTING SECTION 🚧"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "22bSOLKA_Z4r"
      },
      "outputs": [],
      "source": [
        "#from skimage.segmentation import mark_boundaries\n",
        "#dataset = ImageGraphDataset(TRAIN_PATH,'BraTS2021',read_image=True,read_graph=False,read_label=True, features=None)\n",
        "#img, lab = dataset.get_image(dataset.all_ids[12])\n",
        "#slic = dataset.get_supervoxel_partitioning(dataset.all_ids[12])\n",
        "#img = img.sum(axis=3)\n",
        "##img = img[90,:,:]\n",
        "##img = img[90,:,:]\n",
        "#img =   img[90,:,:].T\n",
        "#slic = slic[90,:,:].T\n",
        "#\n",
        "## Display result\n",
        "#fig, ax_arr = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(8, 8))\n",
        "#ax1, ax2, ax3,ax4 = ax_arr.ravel()\n",
        "#\n",
        "#ax1.imshow(img,cmap='gray')\n",
        "#ax1.set_title('Original image')\n",
        "#\n",
        "#ax3.imshow(mark_boundaries(img, slic))\n",
        "#ax3.contour(slic, colors='red', linewidths=1)\n",
        "#ax3.set_title('SLIC')\n",
        "#\n",
        "#for ax in ax_arr.ravel():\n",
        "#    ax.set_axis_off()\n",
        "#\n",
        "#plt.tight_layout()\n",
        "#plt.show()\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bC-ortIQ3vXX"
      },
      "outputs": [],
      "source": [
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled\n",
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/train\n",
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/test\n",
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/val\n",
        "\n",
        "\n",
        "\n",
        "#trainset = set()\n",
        "#valset = set()\n",
        "#testset = set()\n",
        "#for el in dp.get_status_ids()['Pending']:\n",
        "#    if(el in train_dataset.all_ids):\n",
        "#        trainset.add(el)\n",
        "#    if(el in val_dataset.all_ids):\n",
        "#        valset.add(el)\n",
        "#    if(el in test_dataset.all_ids):\n",
        "#        testset.add(el)\n",
        "#\n",
        "#\n",
        "#for el in valset:\n",
        "#    src = VAL_PATH+'/'+str(el)\n",
        "#    dst = '/content/drive/MyDrive/Lorusso/BraTS/data/processed_5000_0.5_10/brats/'+el\n",
        "#    shutil.copytree(src, dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJUctzmLazd9"
      },
      "source": [
        "## Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eHHzhDDF_3G9"
      },
      "outputs": [],
      "source": [
        "\n",
        "TEST_MODE = False\n",
        "\n",
        "\n",
        "if(TEST_MODE):\n",
        "\n",
        "    INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled'\n",
        "    INPUT_PATH_PARENT = '/'.join(INPUT_PATH.split('/')[:-1])\n",
        "    TRAIN_PATH = os.path.join(INPUT_PATH_PARENT,'train')\n",
        "    VAL_PATH = os.path.join(INPUT_PATH_PARENT,'val')\n",
        "    TEST_PATH = os.path.join(INPUT_PATH_PARENT,'test')\n",
        "\n",
        "    TRAIN_MODEL = False\n",
        "    LOAD_MODEL = True # resume training\n",
        "\n",
        "    num_workers = 0\n",
        "    batch_size = 1\n",
        "    num_epochs = 2\n",
        "    lr = 0.01\n",
        "    supervised = False\n",
        "    eval_metrics = [nn.MSELoss()]\n",
        "\n",
        "  # MONAI AUTOENCODER\n",
        "    model = AutoEncoder(\n",
        "           spatial_dims=3,\n",
        "           kernel_size = 3,\n",
        "           up_kernel_size = 3,\n",
        "           in_channels=4,\n",
        "           out_channels=4,\n",
        "           channels=(5,),\n",
        "           strides=(2,),\n",
        "           inter_channels=(8, 16, 32),\n",
        "           inter_dilations=(1, 2, 4),\n",
        "           num_inter_units=2\n",
        "       )\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-10)\n",
        "    loss_fn = nn.MSELoss() #SSIMLoss(spatial_dims=3)\n",
        "\n",
        "    wrapper = ModelWrapper(model = model,\n",
        "                              loss_fn = loss_fn,\n",
        "                              optimizer = optimizer,\n",
        "                              supervised = supervised,\n",
        "                              num_epochs = num_epochs,\n",
        "                              LOAD_MODEL = LOAD_MODEL,\n",
        "                              eval_metrics = eval_metrics\n",
        "                          )\n",
        "\n",
        "    dataset = DataPreprocessor(INPUT_PATH = INPUT_PATH)\n",
        "\n",
        "    # Split dataset if it's not\n",
        "    if(not os.path.exists(TRAIN_PATH)):\n",
        "      dataset.split_dataset()\n",
        "\n",
        "\n",
        "    train_dataset = DataPreprocessor(INPUT_PATH = TRAIN_PATH)\n",
        "    val_dataset = DataPreprocessor(INPUT_PATH = VAL_PATH)\n",
        "    test_dataset = DataPreprocessor(INPUT_PATH = TEST_PATH)\n",
        "\n",
        "    train_loader = DataLoader(dataset = train_dataset,\n",
        "                             sampler = SeqSampler(train_dataset),\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers)\n",
        "\n",
        "    val_loader = DataLoader(dataset = val_dataset,\n",
        "                             sampler = SeqSampler(val_dataset),\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers)\n",
        "\n",
        "    test_loader = DataLoader(dataset = test_dataset,\n",
        "                             sampler = SeqSampler(test_dataset),\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers)\n",
        "\n",
        "\n",
        "    print('Elapsed epochs: ' + str(wrapper.elapsed_epochs))\n",
        "\n",
        "    if(TRAIN_MODEL):\n",
        "        training_loss, validation_loss = wrapper.train(train_loader = train_loader, val_loader = val_loader, experiment_prefix = 'Test' )\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CcuxwsmW9OYS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "86220e26-88ad-4da1-990f-08fda86a3e4a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "  xx = range(1,wrapper.elapsed_epochs+1)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(xx, wrapper.training_loss, '-o', label = 'Train')\n",
        "  plt.plot(xx, wrapper.validation_loss,'-o', label = 'Val')\n",
        "  plt.legend(loc='lower left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "V3TKi4A9O4NA"
      },
      "outputs": [],
      "source": [
        "if(False):\n",
        "\n",
        "  slice_index = 90\n",
        "  ax = 0\n",
        "\n",
        "  im_test, lab = next(iter(test_loader))\n",
        "  output = wrapper.predict(data = im_test)\n",
        "  out = output[0]\n",
        "  out_numpy = out.cpu().detach().numpy()\n",
        "  im_test_numpy = im_test.cpu().detach().numpy()\n",
        "\n",
        "  plot_reconstruction(im_orig = np.sum(im_test_numpy[0], axis=0), im_rec = np.sum(out_numpy, axis = 0), ax = ax, slice_index = slice_index)\n",
        "  plot_brain_sections([im_test[0], lab[0]], ax = ax, slice_index = slice_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "s5fwjWaWOhXN"
      },
      "outputs": [],
      "source": [
        "if(False):\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(ker[1], allkernels=True)\n",
        "\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(torch.sum(ker, dim=(0)), allkernels=True)\n",
        "\n",
        "  kk = torch.sum(ker, dim= 0)\n",
        "  kk = torch.sum(kk, dim = 0)\n",
        "  plt.imshow(kk[0,:,:].cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGPCqWQcrPqb"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-lpiQp3XNxn"
      },
      "source": [
        "## GNN training ⛽"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "wVWsu1gEXV9j"
      },
      "outputs": [],
      "source": [
        "\n",
        "TEST_MODE = False\n",
        "\n",
        "\n",
        "if(TEST_MODE):\n",
        "\n",
        "    INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/processed_5000_0.5_10'\n",
        "    TRAIN_PATH = os.path.join(INPUT_PATH,'train')\n",
        "    VAL_PATH = os.path.join(INPUT_PATH,'val')\n",
        "    TEST_PATH = os.path.join(INPUT_PATH,'val') # set again to 'test' in production\n",
        "\n",
        "    TRAIN_MODEL = True\n",
        "    LOAD_MODEL = False # resume training\n",
        "\n",
        "    num_workers = 0\n",
        "    batch_size = 1\n",
        "    num_epochs = 2\n",
        "    lr = 0.005\n",
        "    supervised = True\n",
        "    eval_metrics = []\n",
        "\n",
        "    dropout = 0\n",
        "    input_feats = 20\n",
        "    class_weights = torch.Tensor([0.1,1,2,2])\n",
        "    layer_sizes=[256]*4\n",
        "    n_classes=4\n",
        "    aggregator_type='pool'\n",
        "\n",
        "    dict_params = {k:eval(k) for k in ['input_feats', 'class_weights', 'layer_sizes', 'n_classes', 'aggregator_type', 'n_classes']}\n",
        "\n",
        "    model = GraphSage(in_feats=input_feats,layer_sizes=layer_sizes,n_classes=n_classes,aggregator_type=aggregator_type,dropout=dropout)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(),lr=lr,weight_decay=1e-10)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    wrapper = ModelWrapper(model = model,\n",
        "                            loss_fn = loss_fn,\n",
        "                            optimizer = optimizer,\n",
        "                            supervised = supervised,\n",
        "                            dict_params = dict_params,\n",
        "                            isgnn = True,\n",
        "                            num_epochs = num_epochs,\n",
        "                            LOAD_MODEL = LOAD_MODEL,\n",
        "                            eval_metrics = eval_metrics\n",
        "                          )\n",
        "\n",
        "\n",
        "    train_dataset = ImageGraphDataset(VAL_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True)\n",
        "    #val_dataset = ImageGraphDataset(VAL_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True)\n",
        "    #test_dataset = ImageGraphDataset(TEST_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True)\n",
        "\n",
        "    train_loader = DataLoader(dataset = train_dataset,\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers,\n",
        "                              collate_fn=minibatch_graphs)\n",
        "\n",
        "\n",
        "    print('Elapsed epochs: ' + str(wrapper.elapsed_epochs))\n",
        "\n",
        "    if(TRAIN_MODEL):\n",
        "        training_loss, validation_loss = wrapper.train(train_loader = train_loader, val_loader = train_loader, experiment_prefix = 'Test_GNN' )\n",
        "        torch.cuda.empty_cache()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mwFJcAuef07T",
        "wE7OwGBES7sa",
        "q7ygOSsiEAWs",
        "aKEY_j_lCzCa",
        "YBt4O3t3uD3V",
        "cgINpL8BK3hL",
        "0u3q4OhjYyGq",
        "he-C8aZXQkry",
        "vsRN0nOwDif5",
        "59tr9TkL0zF3",
        "n1_TRMmsm6jY",
        "kJUctzmLazd9",
        "T-lpiQp3XNxn"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}