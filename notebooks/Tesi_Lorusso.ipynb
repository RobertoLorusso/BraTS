{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## README â—\n",
        "\n",
        "Set a manual_seed for reproducibility.\n",
        "\n",
        "References:\n",
        "\n",
        "- See [Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ub1h-Y7DIpYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### TO-DO âœ…\n",
        "\n",
        "\n",
        "#### FIX ðŸ§¯\n",
        "\n",
        "\n",
        "Quando viene sollevata un'eccezione in DataPreprocessor.__getitem __ viene restituito uno stack di immagini vuote. Aggiungere il relativo ID ad una lista e in fase di training evitare che queste immagini nere vengano incluse nel batch. Magari sostituendo a queste una delle immagini valide presenti nello stesso batch.\n"
      ],
      "metadata": {
        "id": "Yj5kNWe7UKif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment setup ðŸ›\n"
      ],
      "metadata": {
        "id": "mwFJcAuef07T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install python-dotenv\n",
        "! pip install monai\n",
        "! pip install shutil\n",
        "! pip install mlflow --quiet\n",
        "! pip install pyngrok --quiet\n",
        "! pip install torchmetrics\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "from mlflow.models.signature import infer_signature\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from pyngrok import ngrok\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dotenv import load_dotenv\n",
        "import tarfile\n",
        "import nibabel as nib\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import SequentialSampler\n",
        "from torchvision import utils\n",
        "\n",
        "from monai.networks.nets import AutoEncoder\n",
        "from monai.losses import DiceCELoss, DiceFocalLoss, DiceLoss, FocalLoss\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "#torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "45BU03HjgAgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe0e4b8-9223-4311-c7e0-282699f375c4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n",
            "Collecting monai\n",
            "  Downloading monai-1.3.0-202310121228-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from monai) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->monai) (1.3.0)\n",
            "Installing collected packages: monai\n",
            "Successfully installed monai-1.3.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement shutil (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for shutil\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m230.6/230.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.10.0 torchmetrics-1.2.1\n",
            "Mounted at /content/drive/\n",
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "if [ ! -f '/content/drive/MyDrive/Lorusso/BraTS/.env' ]; then\n",
        "    echo \"Creating .env file...\"\n",
        "    echo \"INPUT_DATA_DIR='/content/drive/MyDrive/Lorusso/BraTS/data/raw/brats'\" > '/content/drive/MyDrive/Lorusso/BraTS/.env'\n",
        "    echo \"PROCESSED_DATA_DIR='/content/drive/MyDrive/Lorusso/BraTS/data/processed'\" >> '/content/drive/MyDrive/Lorusso/BraTS/.env'\n",
        "fi"
      ],
      "metadata": {
        "id": "MkZwkHg4Wn_e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "if [ ! -f '/content/drive/MyDrive/Lorusso/BraTS/data/raw/BraTS2021_Training_Data.tar'  ]; then\n",
        "    echo \"Downloading BraTS dataset at /content/drive/MyDrive/Lorusso/BraTS/data/raw ...\"\n",
        "    mkdir /root/.kaggle/\n",
        "    cp '/content/drive/MyDrive/Lorusso/kaggle.json' /root/.kaggle\n",
        "    chmod 600 '/root/.kaggle/kaggle.json'\n",
        "    cd '/content/drive/MyDrive/Lorusso/BraTS/data/raw' && kaggle datasets download -d dschettler8845/brats-2021-task1\n",
        "    ls '/content/drive/MyDrive/Lorusso/BraTS/data/raw'\n",
        "    unzip '/content/drive/MyDrive/Lorusso/BraTS/data/raw/brats-2021-task1.zip' -d '/content/drive/MyDrive/Lorusso/BraTS/data/raw/'\n",
        "    rm -r '/content/drive/MyDrive/Lorusso/BraTS/data/raw/brats-2021-task1.zip'\n",
        "fi"
      ],
      "metadata": {
        "id": "0u_tX1q-hh-L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "path='/content/drive/MyDrive/Lorusso/BraTS/data/raw/brats'\n",
        "dst='/content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled/'\n",
        "\n",
        "if [ ! -d '/content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled' ]; then\n",
        "\n",
        "    mkdir '/content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled'\n",
        "\n",
        "    rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/train\n",
        "    rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/test\n",
        "    rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/val\n",
        "\n",
        "    for el in $(ls $path | head -n 50);\n",
        "        do\n",
        "            echo \"$path/$el -> $dst\"\n",
        "            cp -R \"$path/$el\" $dst\n",
        "        done\n",
        "fi\n"
      ],
      "metadata": {
        "id": "2dm-FkQK1izU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLFlow server"
      ],
      "metadata": {
        "id": "wE7OwGBES7sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_tracking_uri('file:///content/drive/MyDrive/Lorusso/BraTS/mlruns')\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"2Yliuv8VnNyKNcljxgEv6NpZgz8_6ZDBYmEcebUeoX93eGJAE\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)\n",
        "\n",
        "# run tracking UI in the background\n",
        "get_ipython().system_raw(\"mlflow ui --backend-store-uri file:///content/drive/MyDrive/Lorusso/BraTS/mlruns --port 5000 & \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4-kysoZS-i4",
        "outputId": "911404d6-5aa5-4dd1-87cb-35e3830b4238"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow Tracking UI: https://1432-34-41-213-52.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils ðŸ› "
      ],
      "metadata": {
        "id": "q7ygOSsiEAWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def untar_brats(tar_path, extract_path):\n",
        "    tar = tarfile.open(tar_path)\n",
        "    tar.extractall(extract_path)\n",
        "    tar.close()\n",
        "\n",
        "\n",
        "\n",
        "def plot_reconstruction(im_orig, im_rec, ax:int = 0, slice_index:int = 100):\n",
        "\n",
        "    f, ax_array = plt.subplots(1,2, figsize=(10,10))\n",
        "    ax_array[0].imshow(np.take(im_orig, indices = slice_index, axis = ax), cmap='gray')\n",
        "    ax_array[1].imshow(np.take( im_rec , indices=slice_index, axis = ax), cmap='gray')\n",
        "\n",
        "def plot_brain_sections(images,ax = 1,slice_index = 90):\n",
        "\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    d1,d2,d3 = np.shape(images[1])\n",
        "    dims = [d1,d2,d3]\n",
        "    dims.pop(ax)\n",
        "    color_segmentation = np.zeros((dims[0],dims[1],3))\n",
        "\n",
        "    gray_segmentation = np.take(images[1],slice_index,axis = ax)\n",
        "    color_segmentation[gray_segmentation == 1] = [255,0,0] # Red (necrotic tumor core)\n",
        "    color_segmentation[gray_segmentation == 2] = [0,255,0] # Green (peritumoral edematous/invaded tissue)\n",
        "    color_segmentation[gray_segmentation == 4] = [0,0,255] # Blue (enhancing tumor)\n",
        "\n",
        "    t1 = images[0][0]\n",
        "    flair = images[0][1]\n",
        "    t2 = images[0][2]\n",
        "    t1ce = images[0][3]\n",
        "\n",
        "    image = t1+t2+flair+t1ce\n",
        "\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.subplot(1,4,1)\n",
        "    plt.imshow(np.take(image,slice_index,axis = 0),cmap='gray')\n",
        "\n",
        "    plt.subplot(1,4,2)\n",
        "    plt.imshow(np.take(image,slice_index,axis = 1),cmap='gray')\n",
        "\n",
        "    plt.subplot(1,4,3)\n",
        "    plt.imshow(np.take(image,slice_index,axis = 2),cmap='gray')\n",
        "\n",
        "    plt.subplot(1,4,4)\n",
        "    plt.imshow(color_segmentation,cmap='gray')\n",
        "    plt.xlabel('Segmentation')\n",
        "\n",
        "\n",
        "def visTensor(tensor, ch=0, allkernels=False, nrow=8, padding=1):\n",
        "    n,c,w,h = tensor.shape\n",
        "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
        "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
        "    rows = np.min((tensor.shape[0] // nrow + 1, 64))\n",
        "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
        "    plt.figure( figsize=(nrow,rows) )\n",
        "    plt.imshow(grid.cpu().permute((1, 2, 0)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lrs_wCJEAJpS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset class  ðŸ’¾\n"
      ],
      "metadata": {
        "id": "aKEY_j_lCzCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Preprocessing script to convert from data provided by BraTS to data used by our model. Should be the first thing you run.\n",
        "Fulfills the following four functions:\n",
        "1. Normalize and standardize each image of each MRI modality\n",
        "2. Combine multiple MRI modalitities into one image array\n",
        "3. Swap labels from BraTS order (0,2,1,4) to more intuitive order (0,1,2,3)\n",
        "4. Convert image into a graph\n",
        "    Using Simple Linear Iterative Clustering algorithm\n",
        "    Parameters passed on command line\n",
        "\n",
        "If no labels are present (e.g. at test time, in deployment) can also build graph without labels.\n",
        "\n",
        "Saves the following in the specified output directory for each sample\n",
        "MRI_ID/\n",
        "    _input.nii.gz (processed and combined modalities for a sample as nifti file)\n",
        "    _label.nii.gz\n",
        "    _nxgraph.json (networkx graph containing both graph topography and features and labels for each node)\n",
        "    _supervoxels.nii.gz (supervoxel partitioning produced by SLIC)\n",
        "    _crop.npy (optionally the crop of the processed data relative to the original data) (crops out empty image planes)\n",
        "'''\n",
        "\n",
        "\n",
        "class DataPreprocessor(Dataset):\n",
        "  def __init__(self, dotenv_path = \"/content/drive/MyDrive/Lorusso/BraTS/.env\", transform = True, INPUT_PATH = None):\n",
        "\n",
        "    load_dotenv(dotenv_path)\n",
        "    # Data mean and variance\n",
        "    data_stats = ([0.4645, 0.6625, 0.4064, 0.3648], [0.1593, 0.1703, 0.1216, 0.1627])\n",
        "\n",
        "    if(INPUT_PATH is not None and os.path.exists(INPUT_PATH)):\n",
        "        self.data_dir = INPUT_PATH\n",
        "    else:\n",
        "        self.data_dir = os.getenv('INPUT_DATA_DIR')\n",
        "    self.output_dir = os.getenv('PROCESSED_DATA_DIR')\n",
        "    self.mri_prefix = 'BraTS2021'\n",
        "    self.modality_extensions = [\"_flair.nii.gz\", \"_t1.nii.gz\", \"_t1ce.nii.gz\", \"_t2.nii.gz\"]\n",
        "    self.label_extension = \"_seg.nii.gz\"\n",
        "    self.include_labels = self.label_extension is not None\n",
        "    self.all_ids, self.id_to_fp = self.get_all_mris_in_dataset()\n",
        "    self.LABEL_MAP = {4: 3, 2: 1, 1: 2}\n",
        "    self.dataset_mean = np.array(data_stats[0], dtype=np.float32)\n",
        "    self.dataset_std = np.array(data_stats[1], dtype=np.float32)\n",
        "    self.transform = transform if isinstance(transform, bool) else True\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.all_ids)\n",
        "\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    images = []\n",
        "    # Load the image corresponding to idx\n",
        "    try:\n",
        "      fp = [self.id_to_fp[k] for k in self.all_ids if k.split('_')[-1] == idx][0]\n",
        "      bn = os.path.basename(os.path.split(fp)[0])\n",
        "      images.append([nib.load(os.path.join(fp, bn + level)).get_fdata(dtype=np.float32).T\n",
        "                     for level in self.modality_extensions])\n",
        "      labels = nib.load(os.path.join(fp, bn + self.label_extension)).get_fdata(dtype=np.float32).T\n",
        "\n",
        "      # Convert to numpy array otherwise you'll experience RAM leak\n",
        "      images = np.asarray(images[0])\n",
        "      imstack = np.stack(np.array(images, dtype=np.float32), axis = 0)\n",
        "      imstack, labels = self.padding(imstack, labels)\n",
        "\n",
        "      if (self.transform):\n",
        "          imstack,labels = self.get_standardized_image(imstack, labels)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Exception thrown in class {self.__class__.__name__ }, method __getitem__\")\n",
        "      print(e)\n",
        "      imstack = np.zeros([4,240,240,240],dtype=np.float32)\n",
        "      labels = np.zeros([240, 240, 240])\n",
        "\n",
        "    return np.array(imstack), labels\n",
        "\n",
        "\n",
        "  def get_all_mris_in_dataset(self):\n",
        "    mri_folders = glob.glob(f\"{self.data_dir}**/{self.mri_prefix}*/\",\n",
        "                            recursive=True)\n",
        "    mri_folders = self.remove_incomplete_mris(mri_folders)\n",
        "    scan_dic = {os.path.split(fp)[0].split(\"/\")[-1]: fp for fp in mri_folders}\n",
        "    if(len(mri_folders) == 0):\n",
        "        print(\"No MRI found at \" + self.data_dir)\n",
        "    return list(scan_dic.keys()), scan_dic\n",
        "\n",
        "\n",
        "  def remove_incomplete_mris(self, mri_folders):\n",
        "    # if there are any you want to ignore just add them to this list\n",
        "    removed_mris = []\n",
        "    return [fp for fp in mri_folders if fp.split(\"/\")[-2] not in removed_mris]\n",
        "\n",
        "\n",
        "  def split_dataset(self, ratio = (.6,.2,.2),seed = 42):\n",
        "\n",
        "    random.seed(seed)\n",
        "    if(np.sum(ratio) != 1 or ratio is None):\n",
        "      print(\"Error: ratio does not sum up to one.\\nSwitching to default (.6,.2,.2))\")\n",
        "      ratio = (.6,.2,.2)\n",
        "\n",
        "    train_length = int(len(self.all_ids)*ratio[0])\n",
        "    val_length = int(len(self.all_ids)*ratio[1])\n",
        "    test_length = int(len(self.all_ids)*ratio[2])\n",
        "    pos = random.sample(range(0,len(self.all_ids)), len(self.all_ids))\n",
        "\n",
        "    split_dict = {\n",
        "        'train': [self.all_ids[i] for i in pos[:train_length]],\n",
        "        'val': [self.all_ids[i] for i in pos[train_length :train_length + val_length]],\n",
        "        'test': [self.all_ids[i] for i in pos[train_length + val_length:]]\n",
        "    }\n",
        "\n",
        "    for k in split_dict.keys():\n",
        "      parent = '/'.join(self.data_dir.split('/')[:-1])\n",
        "      dst = os.path.join(parent,k)\n",
        "\n",
        "      try:\n",
        "        # create train,val,test dirs\n",
        "        if(not os.path.exists(dst)):\n",
        "          os.mkdir(dst)\n",
        "\n",
        "        # copy splitted data inside folders\n",
        "        for id in split_dict[k]:\n",
        "          if(not os.path.exists(os.path.join(dst,id))):\n",
        "             os.mkdir(os.path.join(dst,id))\n",
        "          copy_tree(self.id_to_fp[id],os.path.join(dst,id))\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"Exception thrown in class {self.__class__.__name__ }, method split_dataset\")\n",
        "        print(e)\n",
        "\n",
        "\n",
        "  def padding(self,image, labels):\n",
        "    n_channels = np.shape(image)[0]\n",
        "    max_val = max(np.shape(image))\n",
        "    pad_list = np.zeros([n_channels,max_val,max_val,max_val],dtype=np.float32)\n",
        "\n",
        "    for channel in range(0, n_channels): # pad every channel\n",
        "        pad_list[channel] = np.pad(image[channel],[(42,43),(0,0),(0,0)],'constant')\n",
        "    labels = np.pad(labels, [(42,43),(0,0),(0,0)],'constant')\n",
        "\n",
        "    return pad_list, labels\n",
        "\n",
        "\n",
        "  def get_standardized_image(self, image_data, label_data):\n",
        "\n",
        "    standardized_labels = self.swap_labels_from_brats(label_data)\n",
        "    #standardized_data = self.standardize_img(\n",
        "    #    image_data, self.dataset_mean, self.dataset_std)\n",
        "    #normalized_data = self.standardize_img(image_data)\n",
        "    normalized_data = self.normalize_img(image_data)\n",
        "    return normalized_data, standardized_labels\n",
        "\n",
        "\n",
        "  def normalize_img(self, img_array):\n",
        "    new_image = np.zeros(img_array.shape, dtype=np.float32)\n",
        "    n_channel = img_array.shape[0] # channel-first images\n",
        "\n",
        "    for channel in range(0, n_channel): # normalize every channel\n",
        "\n",
        "        maxval, minval= np.max(img_array[channel]), np.min(img_array[channel])\n",
        "        new_image[channel] = (img_array[channel] - minval)/(maxval-minval)\n",
        "\n",
        "    return new_image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def normalize_img_quantile(self, img_array):\n",
        "    quantile = np.quantile(img_array, 0.995, axis = (0,1,2,3) )\n",
        "    #print(quantile)\n",
        "    return img_array/quantile\n",
        "\n",
        "    #return img_array\n",
        "\n",
        "\n",
        "\n",
        "  def standardize_img(self,img_array):\n",
        "    img_array = img_array.T # Align shapes\n",
        "    centered = img_array-self.dataset_mean\n",
        "    standardized = centered/self.dataset_std\n",
        "    return standardized.T\n",
        "\n",
        "\n",
        "  def swap_labels_from_brats(self,label_data):\n",
        "    uniques = np.unique(label_data)\n",
        "    for u in uniques:\n",
        "        if u not in [0, 1, 2, 4]:\n",
        "            raise RuntimeError('unexpected label')\n",
        "    new_label_data = np.zeros_like(label_data, dtype=np.int16)\n",
        "    new_label_data[label_data == 4] = self.LABEL_MAP[4]\n",
        "    new_label_data[label_data == 2] = self.LABEL_MAP[2]\n",
        "    new_label_data[label_data == 1] = self.LABEL_MAP[1]\n",
        "    return new_label_data\n",
        "\n",
        "\n",
        "  def swap_labels_to_brats(self,label_data):\n",
        "    uniques = np.unique(label_data)\n",
        "    for u in uniques:\n",
        "        if u not in [0, 1, 2, 3]:\n",
        "            raise RuntimeError('unexpected label')\n",
        "    new_label_data = np.zeros_like(label_data, dtype=np.int16)\n",
        "    new_label_data[label_data == self.LABEL_MAP[4]] = 4\n",
        "    new_label_data[label_data == self.LABEL_MAP[2]] = 2\n",
        "    new_label_data[label_data == self.LABEL_MAP[1]] = 1\n",
        "    return new_label_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SeqSampler(SequentialSampler):\n",
        "  \"\"\"Samples elements sequentially, always in the same order.\n",
        "\n",
        "    Args:\n",
        "        data_source (Dataset): dataset to sample from\n",
        "  \"\"\"\n",
        "  def __init__(self, data_source:Dataset):\n",
        "    self.data_source = data_source\n",
        "    self.indexDict = [id.split('_')[1] for id in data_source.all_ids]\n",
        "  def __iter__(self):\n",
        "    return iter(self.indexDict)\n",
        "  def __len__(self):\n",
        "    return len(self.indexDict)\n"
      ],
      "metadata": {
        "id": "-Pa_KNzpRi0d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models ðŸ“ª\n"
      ],
      "metadata": {
        "id": "cgINpL8BK3hL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder3D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder3D, self).__init__()\n",
        "        # Encoder\n",
        "        self.activation = nn.ReLU()\n",
        "        #self.activation = nn.LogSigmoid()# nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.conv1 = nn.Conv3d(4, 8, 3)\n",
        "        self.conv2 = nn.Conv3d(8, 32, 3)\n",
        "        self.conv3 = nn.Conv3d(64, 256, 2)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2, return_indices=True)\n",
        "        self.enc_linear = nn.Linear(381216, 512)\n",
        "\n",
        "        # Decoder\n",
        "        self.deconv1 = nn.ConvTranspose3d(256, 64, 2)\n",
        "        self.deconv2 = nn.ConvTranspose3d(32, 8, 3)\n",
        "        self.deconv3 = nn.ConvTranspose3d(8, 4, 3)\n",
        "        self.unpool1 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "        self.unpool2 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "        self.unpool3 = nn.MaxUnpool3d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.dec_linear = nn.Linear(512, 381216)\n",
        "\n",
        "    def encode(self, x, return_partials=True):\n",
        "        # Encoder\n",
        "        x = self.conv1(x)\n",
        "        x = self.activation(x)\n",
        "        up3out_shape = x.shape\n",
        "        x, indices1 = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.activation(x)\n",
        "        up2out_shape = x.shape\n",
        "        x, indices2 = self.pool2(x)\n",
        "\n",
        "        #x = self.conv3(x)\n",
        "        #x = self.activation(x)\n",
        "        #up1out_shape = x.shape\n",
        "        #x, indices3 = self.pool3(x)\n",
        "\n",
        "\n",
        "\n",
        "        #print(x.shape)\n",
        "        #x = x.view((x.size(0), -1))\n",
        "        #print(x.shape)\n",
        "        #x = self.enc_linear(x)\n",
        "\n",
        "        # required for unpool\n",
        "        pool_par = {\n",
        "            \"P1\": [indices1, up3out_shape],\n",
        "            \"P2\": [indices2, up2out_shape],\n",
        "            #\"P3\": [indices3, up1out_shape]\n",
        "                   }\n",
        "\n",
        "        if return_partials:\n",
        "            return x, pool_par\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def decode(self, x, pool_par):\n",
        "        #x = self.dec_linear(x)\n",
        "        #x = x.view((x.size(0), 96, 11, 19, 19))\n",
        "\n",
        "        #x = self.unpool1(x, output_size=pool_par[\"P3\"][1], indices=pool_par[\"P3\"][0])\n",
        "        ##print(x.shape)\n",
        "        #x = self.deconv1(x)\n",
        "        #x = self.activation(x)\n",
        "\n",
        "        x = self.unpool2(x, output_size=pool_par[\"P2\"][1], indices=pool_par[\"P2\"][0])\n",
        "        x = self.deconv2(x)\n",
        "        x = self.activation(x)\n",
        "#\n",
        "        x = self.unpool3(x, output_size=pool_par[\"P1\"][1], indices=pool_par[\"P1\"][0])\n",
        "        x = self.deconv3(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.feature, pool_par = self.encode(x)\n",
        "        out = self.decode(self.feature, pool_par)\n",
        "        return out"
      ],
      "metadata": {
        "id": "O4dkf94oMgZz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, slope = 0.2):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv3d(4, 16, kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv3d(16, 64 , kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.MaxPool3d(kernel_size=2, stride=2),\n",
        "\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose3d(64, 16, kernel_size=(3,3,3), stride=2, padding=1,output_padding=1, bias=False),\n",
        "\n",
        "            nn.ConvTranspose3d(16, 4, kernel_size=(3,3,3), stride=2, padding=1, output_padding=1, bias=False),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n"
      ],
      "metadata": {
        "id": "dj9rmyw_LXS6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Wrapper ðŸ“¨\n"
      ],
      "metadata": {
        "id": "0u3q4OhjYyGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelWrapper():\n",
        "  \"\"\"\n",
        "  Allows train, evaluation, prediction and I/O operations on generic PyTorch models\n",
        "  The model is saved at every epoch.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, model: nn.Module, optimizer: nn.Module, loss_fn: nn.Module,\n",
        "               num_epochs: int, supervised: bool = True, LOAD_MODEL: bool = False,\n",
        "               model_path: str  = '/content/drive/MyDrive/Lorusso/models'):\n",
        "\n",
        "    self.device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "    self.model = model\n",
        "    self.model = self.model.to(self.device)\n",
        "    self.model = self.model.to(torch.float)\n",
        "    self.num_epochs = num_epochs\n",
        "    self.loss_fn = loss_fn\n",
        "    self.optimizer = optimizer\n",
        "    self.model_path = model_path\n",
        "    self.save_path = self.model_path + '/' + self.model.__class__.__name__ + '/model.pt'\n",
        "\n",
        "    self.supervised = supervised\n",
        "    self.training_loss = []\n",
        "    self.validation_loss = []\n",
        "    self.elapsed_epochs = 0\n",
        "    self.elapsed_seconds = 0\n",
        "\n",
        "    if(LOAD_MODEL):\n",
        "      self.load_checkpoint()\n",
        "\n",
        "    # Create directory for model loading\n",
        "    try:\n",
        "      if(not os.path.exists(self.model_path + '/' + self.model.__class__.__name__)):\n",
        "        os.mkdir(self.model_path + '/' + self.model.__class__.__name__)\n",
        "    except Exception as e:\n",
        "      print(f\"Exception thrown in class {self.model.__class__.__name__ }, method __init__\")\n",
        "      print(e)\n",
        "      print('\\n')\n",
        "\n",
        "\n",
        "  def log_checkpoint(self, info: dict):\n",
        "    mlflow.pytorch.log_state_dict(info, artifact_path='checkpoint')\n",
        "    #torch.save(info, self.save_path)\n",
        "\n",
        "  def load_checkpoint(self):\n",
        "    \"\"\"\n",
        "    Loads the last checkpoint for the given model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        run_id = mlflow.search_runs(experiment_names=['BraTS_'+type(self.model).__name__],\n",
        "                                    order_by=[\"start_time DESC\"]).iloc[0].run_id\n",
        "\n",
        "        checkpoint = mlflow.pytorch.load_state_dict('runs:/'+run_id+'/checkpoint', map_location=torch.device(self.device))\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.elapsed_epochs = len(checkpoint['training_loss'])\n",
        "        self.training_loss = checkpoint['training_loss']\n",
        "        self.validation_loss = checkpoint['validation_loss']\n",
        "        self.elapsed_seconds = checkpoint['elapsed_seconds']\n",
        "\n",
        "        # Track metrics in the current run\n",
        "        mlflow.start_run()\n",
        "        for i in range(len(training_loss)):\n",
        "            mlflow.log_metric('train_loss', training_loss[i], step=i)\n",
        "\n",
        "        for i in range(len(validation_loss)):\n",
        "            mlflow.log_metric('val_loss', validation_loss[i], step=i)\n",
        "\n",
        "        #checkpoint = torch.load(self.save_path, map_location=torch.device(self.device))\n",
        "        #self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        #self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        #self.elapsed_epochs = checkpoint['epochs']\n",
        "        #self.training_loss = checkpoint['training_loss']\n",
        "        #self.validation_loss = checkpoint['validation_loss']\n",
        "        #self.elapsed_seconds = checkpoint['elapsed_seconds']\n",
        "    except Exception as e:\n",
        "        print(f\"Exception thrown in class {self.model.__class__.__name__ }, method load_checkpoint\")\n",
        "        print(e)\n",
        "        print('\\n')\n",
        "        if(mlflow.active_run):\n",
        "            mlflow.end_run()\n",
        "\n",
        "\n",
        "  def train(self, train_loader, val_loader = None):\n",
        "\n",
        "    try:\n",
        "        # Set MLFlow experiment\n",
        "        mlflow.set_experiment('BraTS_'+self.model.__class__.__name__)\n",
        "\n",
        "        # Start a new run if the model wasn't loaded\n",
        "        if(not mlflow.active_run()):\n",
        "            mlflow.start_run()\n",
        "\n",
        "        param_dict = {\n",
        "            'batch_size':train_loader.batch_size,\n",
        "            'loss_fn':self.loss_fn.__class__.__name__,\n",
        "            'optimizer':self.optimizer.__class__.__name__,\n",
        "            'learning_rate':self.optimizer.state_dict()['param_groups'][0]['lr'],\n",
        "            'weight_decay':self.optimizer.state_dict()['param_groups'][0]['weight_decay'],\n",
        "        }\n",
        "        mlflow.log_params(param_dict)\n",
        "\n",
        "        training_loss = self.training_loss\n",
        "        validation_loss = self.validation_loss\n",
        "\n",
        "        self.tot_epochs = self.elapsed_epochs + self.num_epochs+1\n",
        "        self.tot_time = time.time()\n",
        "\n",
        "        # Train\n",
        "        for epoch in range(self.elapsed_epochs+1, self.tot_epochs):\n",
        "            start = time.time() # track time\n",
        "\n",
        "          # Evaluate first if loaded model missed the evaluation during an epoch\n",
        "            if(len(training_loss) > len(validation_loss)):\n",
        "                if(val_loader is not None):\n",
        "\n",
        "                  ## COMPLETE EVALUATION OF PREVIOUS EPOCH ##\n",
        "                  # NB: epoch = epoch - 1\n",
        "                    val_batch_loss = self.__eval(val_loader, (epoch-1) )\n",
        "                    validation_loss.append(np.array(val_batch_loss).mean())\n",
        "\n",
        "                    # Log metric\n",
        "                    mlflow.log_metric('val_loss',validation_loss[-1], step=epoch-1)\n",
        "\n",
        "                    print(f\"Epoch: {epoch-1}/{self.tot_epochs-1}, Validation Loss: {validation_loss[-1]:.4f}, Elapsed time: {time.time() - start:.0f} sec \")\n",
        "\n",
        "                  # Update training time\n",
        "                    epoch_time = int(time.time() - self.tot_time) + self.elapsed_seconds\n",
        "\n",
        "                  #Create checkpoint\n",
        "                    val_dict = {\n",
        "                              'model_state_dict': self.model.state_dict(),\n",
        "                              'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                              'training_loss': training_loss,\n",
        "                              'validation_loss': validation_loss,\n",
        "                              'elapsed_seconds': epoch_time\n",
        "                              }\n",
        "                    self.log_checkpoint(val_dict)\n",
        "                else:\n",
        "                    # Kind of exception, needed to keep the vectors of the same size\n",
        "                    validation_loss.append(np.mean(validation_loss))\n",
        "\n",
        "                    #Log metric\n",
        "                    mlflow.log_metric('val_loss',validation_loss[-1], step=epoch-1)\n",
        "\n",
        "            #### TRAIN ######\n",
        "            train_batch_loss = self.__train(train_loader, epoch)\n",
        "            training_loss.append(np.array(train_batch_loss).mean())\n",
        "\n",
        "            # Log metric\n",
        "            mlflow.log_metric('train_loss',training_loss[-1], step=epoch) # MLFLOW tracking\n",
        "            print(f\"\\nEpoch: {epoch}/{self.tot_epochs-1}, Loss: {training_loss[-1]:.4f}, Epoch elapsed time: {time.time() - start:.0f} sec \\n\")\n",
        "\n",
        "            #Save model every elapsed epoch\n",
        "            self.elapsed_epochs = epoch\n",
        "            epoch_time = int(time.time() - self.tot_time) + self.elapsed_seconds\n",
        "\n",
        "            train_dict = {\n",
        "                      'model_state_dict': self.model.state_dict(),\n",
        "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                      'training_loss': training_loss,\n",
        "                      'validation_loss': validation_loss,\n",
        "                      'elapsed_seconds': epoch_time\n",
        "                      }\n",
        "            self.log_checkpoint(train_dict)\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            time.sleep(5)\n",
        "            if(val_loader is not None):\n",
        "\n",
        "            #### EVALUATE ######\n",
        "                val_batch_loss = self.__eval(val_loader, epoch)\n",
        "                validation_loss.append(np.array(val_batch_loss).mean())\n",
        "\n",
        "                # Log metric\n",
        "                mlflow.log_metric('val_loss',validation_loss[-1], step=epoch)\n",
        "                print(f\"Epoch: {epoch}/{self.tot_epochs-1}, Validation Loss: {validation_loss[-1]:.4f}, Elapsed time: {time.time() - start:.0f} sec \")\n",
        "\n",
        "                #Checkpoint\n",
        "                epoch_time = int(time.time() - self.tot_time) + self.elapsed_seconds\n",
        "                val_dict = {\n",
        "                          'model_state_dict': self.model.state_dict(),\n",
        "                          'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                          'training_loss': training_loss,\n",
        "                          'validation_loss': validation_loss,\n",
        "                          'elapsed_seconds': epoch_time\n",
        "                          }\n",
        "                self.log_checkpoint(val_dict)\n",
        "\n",
        "            print(f\"Total training time: {time.time()-self.tot_time:.0f} sec\")\n",
        "\n",
        "        # Log model --> end run\n",
        "        mlflow.pytorch.log_model(self.model, artifact_path='model')\n",
        "        mlflow.end_run()\n",
        "    except Exception as e:\n",
        "        print(f\"Exception thrown in class {self.model.__class__.__name__ }, method train:\")\n",
        "        print(e)\n",
        "        print('\\n')\n",
        "        mlflow.end_run()\n",
        "\n",
        "\n",
        "    return training_loss, validation_loss\n",
        "\n",
        "\n",
        "\n",
        "  def __train(self, train_loader: DataLoader, epoch:int):\n",
        "\n",
        "    \"\"\" Train for an epoch \"\"\"\n",
        "\n",
        "    self.model.train()\n",
        "    train_batch_loss = []\n",
        "    train_steps = int(len(train_loader.dataset.all_ids)/train_loader.batch_size)\n",
        "\n",
        "    for i, (data,labels) in enumerate(train_loader):\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "      if self.device == 'cuda':\n",
        "        data = data.type(torch.cuda.FloatTensor)\n",
        "      else:\n",
        "        data = data.type(torch.FloatTensor)\n",
        "\n",
        "      data = data.to(self.device)\n",
        "\n",
        "      outputs = self.model(data)\n",
        "\n",
        "      if(self.supervised):\n",
        "        loss = self.loss_fn(outputs,labels)\n",
        "      else:\n",
        "        loss = self.loss_fn(outputs, data)\n",
        "\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      train_batch_loss.append(loss.detach().item())\n",
        "\n",
        "      out = f\"Epoch: {epoch}/{self.tot_epochs-1}, Step: {i+1}/{train_steps}, Loss: {loss.item():.4f}, Elapsed time: {time.time() - self.tot_time:.0f} sec \"\n",
        "      sys.stdout.write(\"\\r\" + out)\n",
        "      sys.stdout.flush()\n",
        "\n",
        "    return train_batch_loss\n",
        "\n",
        "\n",
        "  def __eval(self, val_loader: DataLoader, epoch:int):\n",
        "\n",
        "    \"\"\" Evaluate for an epoch \"\"\"\n",
        "\n",
        "    val_steps = int(len(val_loader.dataset.all_ids)/val_loader.batch_size)\n",
        "    self.model.eval()\n",
        "    val_batch_loss = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i, (data,labels) in enumerate(val_loader):\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        if self.device == 'cuda':\n",
        "          data = data.type(torch.cuda.FloatTensor)\n",
        "        else:\n",
        "          data = data.type(torch.FloatTensor)\n",
        "\n",
        "        data = data.to(self.device)\n",
        "        outputs = self.model(data)\n",
        "\n",
        "        if(self.supervised):\n",
        "            val_loss = self.loss_fn(outputs, labels)\n",
        "        else:\n",
        "            val_loss = self.loss_fn(outputs, data)\n",
        "\n",
        "        val_batch_loss.append(val_loss.detach().item())\n",
        "\n",
        "        out = f\"Epoch: {epoch}/{self.tot_epochs-1}, Validation Step: {i+1}/{val_steps}, Validation Loss: {val_loss.item():.4f}, Elapsed time: {time.time() - self.tot_time:.0f} sec \"\n",
        "        sys.stdout.write(\"\\r\" + out)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    return val_batch_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def predict_batch(self, data_loader):\n",
        "\n",
        "    output = []\n",
        "    self.model = self.model.to(self.device)\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "          for i, batch in enumerate(data_loader):\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if(len(batch) == 1):\n",
        "              data = batch\n",
        "            else:\n",
        "              data,label = batch\n",
        "\n",
        "            if self.device == 'cuda':\n",
        "              data = data.type(torch.cuda.FloatTensor)\n",
        "            else:\n",
        "              data = data.type(torch.FloatTensor)\n",
        "\n",
        "            data.to(self.device)\n",
        "\n",
        "            out = self.model(data)\n",
        "            out = out.cpu().detach().numpy()\n",
        "            output.append(out)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Exception thrown in class {self.model.__class__.__name__ }, method predict_batch:\")\n",
        "            print(e)\n",
        "            print('\\n')\n",
        "\n",
        "    return np.array(output)\n",
        "\n",
        "\n",
        "  def predict(self, data):\n",
        "\n",
        "    if device == 'cuda':\n",
        "      data = data.type(torch.cuda.FloatTensor)\n",
        "    else:\n",
        "      data = data.type(torch.FloatTensor)\n",
        "\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "        data.to(device)\n",
        "        output = self.model(data)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "biiQzha9JP8G"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss ðŸ•³"
      ],
      "metadata": {
        "id": "he-C8aZXQkry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss(nn.Module):\n",
        "    def __init__(self, focal):\n",
        "        super(Loss, self).__init__()\n",
        "        if focal:\n",
        "            self.loss_fn = DiceFocalLoss(\n",
        "                include_background=False, softmax=True, to_onehot_y=True, batch=True, gamma=2.0\n",
        "            )\n",
        "        else:\n",
        "            self.loss_fn = DiceCELoss(include_background=False, softmax=True, to_onehot_y=True, batch=True)\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        return self.loss_fn(y_pred, y_true)\n",
        "\n",
        "\n",
        "class LossBraTS(nn.Module):\n",
        "    def __init__(self, focal):\n",
        "        super(LossBraTS, self).__init__()\n",
        "        self.dice = DiceLoss(sigmoid=True, batch=True)\n",
        "        self.ce = FocalLoss(gamma=2.0, to_onehot_y=False) if focal else nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def _loss(self, p, y):\n",
        "        return self.dice(p, y) + self.ce(p, y.float())\n",
        "\n",
        "    def forward(self, p, y):\n",
        "        y_wt, y_tc, y_et = y > 0, ((y == 1) + (y == 3)) > 0, y == 3\n",
        "        p_wt, p_tc, p_et = p[:, 0].unsqueeze(1), p[:, 1].unsqueeze(1), p[:, 2].unsqueeze(1)\n",
        "        l_wt, l_tc, l_et = self._loss(p_wt, y_wt), self._loss(p_tc, y_tc), self._loss(p_et, y_et)\n",
        "        return l_wt + l_tc + l_et"
      ],
      "metadata": {
        "id": "pUxP6_HiQiP8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build dataset ðŸ—"
      ],
      "metadata": {
        "id": "vsRN0nOwDif5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dotenv_path = \"/content/drive/MyDrive/Lorusso/BraTS/.env\"\n",
        "load_dotenv(dotenv_path)\n",
        "\n",
        "\n",
        "INPUT_PATH = os.getenv(\"INPUT_DATA_DIR\")\n",
        "PROCESSED_PATH = os.getenv('PROCESSED_DATA_DIR')\n",
        "INPUT_PATH_PARENT = '/'.join(INPUT_PATH.split('/')[:-1])\n",
        "\n",
        "TRAIN_PATH = os.path.join(INPUT_PATH_PARENT,'train')\n",
        "VAL_PATH = os.path.join(INPUT_PATH_PARENT,'val')\n",
        "TEST_PATH = os.path.join(INPUT_PATH_PARENT,'test')\n",
        "\n",
        "TAR_PATH = os.path.join(INPUT_PATH_PARENT,'BraTS2021_Training_Data.tar')\n",
        "BUILD_DATASET = False\n",
        "\n",
        "if(BUILD_DATASET):\n",
        "  untar_brats(tar_path = '/content/drive/MyDrive/Lorusso/BraTS/data/raw/BraTS2021_Training_Data.tar', extract_path = INPUT_PATH )\n",
        "  dataset = DataPreprocessor()\n",
        "  dataset.split_dataset()\n",
        "\n",
        "#dataset = DataPreprocessor()\n",
        "#train_loader = DataLoader(dataset, sampler = SeqSampler(dataset), batch_size = 1, num_workers = 0)\n",
        "#images, labels= next(iter(train_loader))\n",
        "#plot_brain_sections([images[0], labels[0]])\n",
        "#del images, labels, train_loader, dataset"
      ],
      "metadata": {
        "id": "9m9XrfCJDqWy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and predict âŒ›"
      ],
      "metadata": {
        "id": "mz-6k-tLLDbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_MODEL = False\n",
        "LOAD_MODEL = True\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "time.sleep(10)\n",
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "  num_workers = 1\n",
        "  batch_size = 1\n",
        "  num_epochs = 1\n",
        "  lr = 0.005\n",
        "  supervised = False\n",
        "\n",
        "  # MONAI AUTOENCODER\n",
        "  model = AutoEncoder(\n",
        "         spatial_dims=3,\n",
        "         kernel_size = 3,\n",
        "         up_kernel_size = 3,\n",
        "         in_channels=4,\n",
        "         out_channels=4,\n",
        "         channels=(5,),\n",
        "         strides=(2,),\n",
        "         inter_channels=(8, 16, 32),\n",
        "         inter_dilations=(1, 2, 4),\n",
        "         num_inter_units=2\n",
        "     )\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-10)\n",
        "  loss_fn = nn.MSELoss() #SSIMLoss(spatial_dims=3)\n",
        "\n",
        "  wrapper = ModelWrapper(model = model,\n",
        "                            loss_fn = loss_fn,\n",
        "                            optimizer = optimizer,\n",
        "                            supervised = supervised,\n",
        "                            num_epochs = num_epochs,\n",
        "                            LOAD_MODEL = LOAD_MODEL\n",
        "                        )\n",
        "\n",
        "  dataset = DataPreprocessor(INPUT_PATH = INPUT_PATH)\n",
        "\n",
        "  # Split dataset if it's not\n",
        "  if(not os.path.exists(TRAIN_PATH)):\n",
        "    dataset.split_dataset()\n",
        "\n",
        "\n",
        "  train_dataset = DataPreprocessor(INPUT_PATH = TRAIN_PATH)\n",
        "  val_dataset = DataPreprocessor(INPUT_PATH = VAL_PATH)\n",
        "  test_dataset = DataPreprocessor(INPUT_PATH = TEST_PATH)\n",
        "\n",
        "  train_loader = DataLoader(dataset = train_dataset,\n",
        "                           sampler = SeqSampler(train_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "  val_loader = DataLoader(dataset = val_dataset,\n",
        "                           sampler = SeqSampler(val_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "  test_loader = DataLoader(dataset = test_dataset,\n",
        "                           sampler = SeqSampler(test_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "\n",
        "  print('Elapsed epochs: ' + str(wrapper.elapsed_epochs))\n",
        "\n",
        "if(TRAIN_MODEL):\n",
        "      training_loss, validation_loss = wrapper.train(train_loader = train_loader, val_loader = val_loader )\n",
        "      torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "UZ5ItOcISTzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5074ef3a-8f91-4d3a-dd19-2c36c101bca5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/12/07 18:20:34 WARNING mlflow.tracking.fluent: Cannot retrieve experiment by name BraTS_AutoEncoder\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception thrown in class AutoEncoder, method load_checkpoint\n",
            "single positional indexer is out-of-bounds\n",
            "\n",
            "\n",
            "Elapsed epochs: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "  xx = range(1,wrapper.elapsed_epochs+1)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(xx, wrapper.training_loss, '-o', label = 'Train')\n",
        "  plt.plot(xx, wrapper.validation_loss,'-o', label = 'Val')\n",
        "  plt.legend(loc='lower left')"
      ],
      "metadata": {
        "id": "Bo0aQf08rHvn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "\n",
        "  slice_index = 150\n",
        "  ax = 0\n",
        "\n",
        "  im_test, lab = next(iter(test_loader))\n",
        "  output = wrapper.predict(data = im_test)\n",
        "  out = output[0]\n",
        "  out_numpy = out.cpu().detach().numpy()\n",
        "  im_test_numpy = im_test.cpu().detach().numpy()\n",
        "\n",
        "  plot_reconstruction(im_orig = np.sum(im_test_numpy[0], axis=0), im_rec = np.sum(out_numpy, axis = 0), ax = ax, slice_index = slice_index)\n",
        "  plot_brain_sections([im_test[0], lab[0]], ax = ax, slice_index = slice_index)"
      ],
      "metadata": {
        "id": "DvvSdXEvIw5w"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(ker[0], allkernels=True)\n",
        "\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(torch.sum(ker, dim=(0)), allkernels=True)\n",
        "\n",
        "  kk = torch.sum(ker, dim= 0)\n",
        "  kk = torch.sum(kk, dim = 0)\n",
        "  plt.imshow(kk[0,:,:].cpu().detach().numpy())"
      ],
      "metadata": {
        "id": "ETwFBvf1Zjyp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING SECTION ðŸš§"
      ],
      "metadata": {
        "id": "n1_TRMmsm6jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled\n",
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/train\n",
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/test\n",
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/val"
      ],
      "metadata": {
        "id": "bC-ortIQ3vXX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TEST_MODE = False\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "if(TEST_MODE):\n",
        "\n",
        "  INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled'\n",
        "  INPUT_PATH_PARENT = '/'.join(INPUT_PATH.split('/')[:-1])\n",
        "  TRAIN_PATH = os.path.join(INPUT_PATH_PARENT,'train')\n",
        "  VAL_PATH = os.path.join(INPUT_PATH_PARENT,'val')\n",
        "  TEST_PATH = os.path.join(INPUT_PATH_PARENT,'test')\n",
        "\n",
        "  TRAIN_MODEL = False\n",
        "  LOAD_MODEL = True # resume training\n",
        "\n",
        "  num_workers = 1\n",
        "  batch_size = 1\n",
        "  num_epochs = 1\n",
        "  lr = 0.01\n",
        "  supervised = False\n",
        "\n",
        "  # MONAI AUTOENCODER\n",
        "  model = AutoEncoder(\n",
        "         spatial_dims=3,\n",
        "         kernel_size = 3,\n",
        "         up_kernel_size = 3,\n",
        "         in_channels=4,\n",
        "         out_channels=4,\n",
        "         channels=(5,),\n",
        "         strides=(2,),\n",
        "         inter_channels=(8, 16, 32),\n",
        "         inter_dilations=(1, 2, 4),\n",
        "         num_inter_units=2\n",
        "     )\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-10)\n",
        "  loss_fn = nn.MSELoss() #SSIMLoss(spatial_dims=3)\n",
        "\n",
        "  wrapper = ModelWrapper(model = model,\n",
        "                            loss_fn = loss_fn,\n",
        "                            optimizer = optimizer,\n",
        "                            supervised = supervised,\n",
        "                            num_epochs = num_epochs,\n",
        "                            LOAD_MODEL = LOAD_MODEL\n",
        "                        )\n",
        "\n",
        "  dataset = DataPreprocessor(INPUT_PATH = INPUT_PATH)\n",
        "\n",
        "  # Split dataset if it's not\n",
        "  if(not os.path.exists(TRAIN_PATH)):\n",
        "    dataset.split_dataset()\n",
        "\n",
        "\n",
        "  train_dataset = DataPreprocessor(INPUT_PATH = TRAIN_PATH)\n",
        "  val_dataset = DataPreprocessor(INPUT_PATH = VAL_PATH)\n",
        "  test_dataset = DataPreprocessor(INPUT_PATH = TEST_PATH)\n",
        "\n",
        "  train_loader = DataLoader(dataset = train_dataset,\n",
        "                           sampler = SeqSampler(train_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "  val_loader = DataLoader(dataset = val_dataset,\n",
        "                           sampler = SeqSampler(val_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "  test_loader = DataLoader(dataset = test_dataset,\n",
        "                           sampler = SeqSampler(test_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "\n",
        "  print('Elapsed epochs: ' + str(wrapper.elapsed_epochs))\n",
        "\n",
        "  if(TRAIN_MODEL):\n",
        "      training_loss, validation_loss = wrapper.train(train_loader = train_loader, val_loader = None )\n",
        "      torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "eHHzhDDF_3G9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "  xx = range(1,wrapper.elapsed_epochs+1)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(xx, wrapper.training_loss, '-o', label = 'Train')\n",
        "  plt.plot(xx, wrapper.validation_loss,'-o', label = 'Val')\n",
        "  plt.legend(loc='lower left')"
      ],
      "metadata": {
        "id": "CcuxwsmW9OYS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "7c081d68-ea68-4149-ff73-3390ab49aa69"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "\n",
        "  slice_index = 90\n",
        "  ax = 0\n",
        "\n",
        "  im_test, lab = next(iter(test_loader))\n",
        "  output = wrapper.predict(data = im_test)\n",
        "  out = output[0]\n",
        "  out_numpy = out.cpu().detach().numpy()\n",
        "  im_test_numpy = im_test.cpu().detach().numpy()\n",
        "\n",
        "  plot_reconstruction(im_orig = np.sum(im_test_numpy[0], axis=0), im_rec = np.sum(out_numpy, axis = 0), ax = ax, slice_index = slice_index)\n",
        "  plot_brain_sections([im_test[0], lab[0]], ax = ax, slice_index = slice_index)"
      ],
      "metadata": {
        "id": "V3TKi4A9O4NA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(ker[1], allkernels=True)\n",
        "\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(torch.sum(ker, dim=(0)), allkernels=True)\n",
        "\n",
        "  kk = torch.sum(ker, dim= 0)\n",
        "  kk = torch.sum(kk, dim = 0)\n",
        "  plt.imshow(kk[0,:,:].cpu().detach().numpy())"
      ],
      "metadata": {
        "id": "s5fwjWaWOhXN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLFlow transitioner\n",
        "\n",
        "Helps to bring a model saved with pickle to MLFlow"
      ],
      "metadata": {
        "id": "kVH1teeglI4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(False):\n",
        "    checkpoint = torch.load(wrapper.save_path, map_location=torch.device(wrapper.device))\n",
        "    wrapper.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    wrapper.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    wrapper.elapsed_epochs = checkpoint['epochs']\n",
        "    wrapper.training_loss = checkpoint['training_loss']\n",
        "    wrapper.validation_loss = checkpoint['validation_loss']\n",
        "    wrapper.elapsed_seconds = checkpoint['elapsed_seconds']\n",
        "    try:\n",
        "        mlflow.set_experiment('BraTS_'+wrapper.model.__class__.__name__)\n",
        "        mlflow.start_run()\n",
        "        param_dict = {\n",
        "            'batch_size':train_loader.batch_size,\n",
        "            'optimizer':wrapper.optimizer.__class__.__name__,\n",
        "            'learning_rate':wrapper.optimizer.state_dict()['param_groups'][0]['lr'],\n",
        "            'loss_fn':wrapper.loss_fn.__class__.__name__\n",
        "        }\n",
        "        mlflow.log_params(param_dict)\n",
        "\n",
        "        #sample_input,_ = next(iter(train_loader))\n",
        "        #sample_output = wrapper.predict(sample_input)\n",
        "        #signature = infer_signature(sample_input.numpy(), sample_output.numpy())\n",
        "    #\n",
        "        #print(\"Model signature:\", signature)\n",
        "\n",
        "        for i in range(len(wrapper.training_loss)):\n",
        "            mlflow.log_metric('train_loss',wrapper.training_loss[i], step =i+1)\n",
        "        for i in range(len(wrapper.validation_loss)):\n",
        "            mlflow.log_metric('val_loss',wrapper.validation_loss[i], step =i+1)\n",
        "\n",
        "        val_dict = {\n",
        "              'model_state_dict': wrapper.model.state_dict(),\n",
        "              'optimizer_state_dict': wrapper.optimizer.state_dict(),\n",
        "              'elapsed_seconds': wrapper.elapsed_seconds\n",
        "              }\n",
        "\n",
        "        mlflow.pytorch.log_state_dict(val_dict,artifact_path=\"checkpoint\")\n",
        "\n",
        "        mlflow.pytorch.log_model(wrapper.model, artifact_path='model')\n",
        "\n",
        "        mlflow.end_run()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        mlflow.end_run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xjPxt_lhQ7z",
        "outputId": "a949bb51-3ed9-4f47-a769-573dde81229d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/12/07 18:35:37 INFO mlflow.tracking.fluent: Experiment with name 'BraTS_AutoEncoder' does not exist. Creating a new experiment.\n",
            "2023/12/07 18:35:38 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.0+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2023/12/07 18:35:50 WARNING mlflow.utils.requirements_utils: Found torchaudio version (2.1.0+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchaudio==2.1.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2023/12/07 18:35:50 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.16.0+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.16.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2023/12/07 18:35:50 WARNING mlflow.utils.requirements_utils: Found jaxlib version (0.4.20+cuda11.cudnn86) contains a local version label (+cuda11.cudnn86). MLflow logged a pip requirement for this package as 'jaxlib==0.4.20' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ub1h-Y7DIpYf",
        "mwFJcAuef07T",
        "q7ygOSsiEAWs",
        "aKEY_j_lCzCa",
        "cgINpL8BK3hL",
        "he-C8aZXQkry",
        "vsRN0nOwDif5"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}