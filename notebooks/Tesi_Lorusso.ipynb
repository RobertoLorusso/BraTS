{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub1h-Y7DIpYf"
      },
      "source": [
        "## README ❗\n",
        "\n",
        "Set a manual_seed for reproducibility.\n",
        "\n",
        "References:\n",
        "\n",
        "- See [Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwFJcAuef07T"
      },
      "source": [
        "## Environment setup 🏛\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45BU03HjgAgT",
        "outputId": "d2d983db-692b-456f-af25-27d7361464d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n",
            "Collecting monai\n",
            "  Downloading monai-1.3.0-202310121228-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from monai) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->monai) (1.3.0)\n",
            "Installing collected packages: monai\n",
            "Successfully installed monai-1.3.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement shutil (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for shutil\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.10.0 torchmetrics-1.3.0.post0\n",
            "Looking in links: https://data.dgl.ai/wheels/cu121/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/cu121/dgl-2.0.0%2Bcu121-cp310-cp310-manylinux1_x86_64.whl (926.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.0/926.0 MB\u001b[0m \u001b[31m737.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.11.17)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata>=0.5.0->dgl) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchdata>=0.5.0->dgl) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-2.0.0+cu121\n",
            "Looking in links: https://data.dgl.ai/wheels-test/repo.html\n",
            "Collecting dglgo\n",
            "  Downloading dglgo-0.0.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (0.9.0)\n",
            "Collecting isort>=5.10.1 (from dglgo)\n",
            "  Downloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autopep8>=1.6.0 (from dglgo)\n",
            "  Downloading autopep8-2.0.4-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpydoc>=1.1.0 (from dglgo)\n",
            "  Downloading numpydoc-1.6.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.10.13)\n",
            "Collecting ruamel.yaml>=0.17.20 (from dglgo)\n",
            "  Downloading ruamel.yaml-0.18.5-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from dglgo) (6.0.1)\n",
            "Collecting ogb>=1.3.3 (from dglgo)\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit-pypi (from dglgo)\n",
            "  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.2.2)\n",
            "Collecting pycodestyle>=2.10.0 (from autopep8>=1.6.0->dglgo)\n",
            "  Downloading pycodestyle-2.11.1-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from autopep8>=1.6.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: sphinx>=5 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (5.0.2)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (3.1.3)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (0.9.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (4.66.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb>=1.3.3->dglgo)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.0->dglgo) (4.5.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.20->dglgo)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (3.2.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.4.0->dglgo) (8.1.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi->dglgo) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.1.3)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb>=1.3.3->dglgo)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2023.3.post1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.0.6)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.16.1)\n",
            "Requirement already satisfied: docutils<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (0.18.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=5->numpydoc>=1.1.0->dglgo) (23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb>=1.3.3->dglgo) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7026 sha256=88bac9e14bd37b6f2e21093a48b0a02293bfe595c1bc0f7f9ff6253ddc093e4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, ruamel.yaml.clib, rdkit-pypi, pycodestyle, isort, ruamel.yaml, outdated, autopep8, ogb, numpydoc, dglgo\n",
            "Successfully installed autopep8-2.0.4 dglgo-0.0.2 isort-5.13.2 littleutils-0.2.2 numpydoc-1.6.0 ogb-1.3.6 outdated-0.2.2 pycodestyle-2.11.1 rdkit-pypi-2022.9.5 ruamel.yaml-0.18.5 ruamel.yaml.clib-0.2.8\n",
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "! pip install python-dotenv\n",
        "! pip install monai\n",
        "! pip install shutil\n",
        "! pip install mlflow --quiet\n",
        "! pip install pyngrok --quiet\n",
        "! pip install torchmetrics\n",
        "#! pip install dgl\n",
        "! pip install  dgl -f https://data.dgl.ai/wheels/cu121/repo.html\n",
        "! pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "sys.path.append('/content/drive/MyDrive/Lorusso/BraTS/')\n",
        "\n",
        "from src.preprocess import evaluation\n",
        "from src.preprocess.image_processing import *\n",
        "from src.preprocess.nifti_io import *\n",
        "from src.preprocess.graphgen import *\n",
        "from src.preprocess.graph_io import *\n",
        "\n",
        "from mlflow.models.signature import infer_signature\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from pyngrok import ngrok\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dotenv import load_dotenv\n",
        "import concurrent.futures\n",
        "import tarfile\n",
        "import nibabel as nib\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "from distutils.dir_util import copy_tree\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import SequentialSampler\n",
        "from torchvision import utils\n",
        "\n",
        "from monai.networks.nets import AutoEncoder\n",
        "from monai.losses import DiceCELoss, DiceFocalLoss, DiceLoss, FocalLoss\n",
        "\n",
        "from dgl import from_networkx as to_dgl_graph\n",
        "from dgl import batch as dgl_batch\n",
        "\n",
        "#torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MkZwkHg4Wn_e"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "if [ ! -f '/content/drive/MyDrive/Lorusso/BraTS/.env' ]; then\n",
        "    echo \"Creating .env file...\"\n",
        "    echo \"INPUT_DATA_DIR='/content/drive/MyDrive/Lorusso/BraTS/data/raw/brats'\" > '/content/drive/MyDrive/Lorusso/BraTS/.env'\n",
        "    echo \"PROCESSED_DATA_DIR='/content/drive/MyDrive/Lorusso/BraTS/data/processed'\" >> '/content/drive/MyDrive/Lorusso/BraTS/.env'\n",
        "    echo \"LOG_PATH='/content/drive/MyDrive/Lorusso/BraTS/logs'\" >> '/content/drive/MyDrive/Lorusso/BraTS/.env'\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0u_tX1q-hh-L"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "if [ ! -f '/content/drive/MyDrive/Lorusso/BraTS/data/raw/BraTS2021_Training_Data.tar'  ]; then\n",
        "    echo \"Downloading BraTS dataset at /content/drive/MyDrive/Lorusso/BraTS/data/raw ...\"\n",
        "    mkdir /root/.kaggle/\n",
        "    cp '/content/drive/MyDrive/Lorusso/kaggle.json' /root/.kaggle\n",
        "    chmod 600 '/root/.kaggle/kaggle.json'\n",
        "    cd '/content/drive/MyDrive/Lorusso/BraTS/data/raw' && kaggle datasets download -d dschettler8845/brats-2021-task1\n",
        "    ls '/content/drive/MyDrive/Lorusso/BraTS/data/raw'\n",
        "    unzip '/content/drive/MyDrive/Lorusso/BraTS/data/raw/brats-2021-task1.zip' -d '/content/drive/MyDrive/Lorusso/BraTS/data/raw/'\n",
        "    rm -r '/content/drive/MyDrive/Lorusso/BraTS/data/raw/brats-2021-task1.zip'\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2dm-FkQK1izU"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "path='/content/drive/MyDrive/Lorusso/BraTS/data/raw/brats'\n",
        "dst='/content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled/'\n",
        "\n",
        "\n",
        "if [ ! -d '/content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled' ]; then\n",
        "\n",
        "    mkdir '/content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled'\n",
        "\n",
        "    rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/train\n",
        "    rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/test\n",
        "    rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/val\n",
        "\n",
        "    for el in $(ls $path | head -n 24);\n",
        "        do\n",
        "            echo \"$path/$el -> $dst\"\n",
        "            cp -R \"$path/$el\" $dst\n",
        "        done\n",
        "fi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE7OwGBES7sa"
      },
      "source": [
        "## MLFlow server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4-kysoZS-i4",
        "outputId": "580856ac-4474-4fd3-d69d-1dfad50f644e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow Tracking UI: https://7b4b-35-227-161-184.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "mlflow.set_tracking_uri('file:///content/drive/MyDrive/Lorusso/BraTS/mlruns')\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "NGROK_AUTH_TOKEN = \"2Yliuv8VnNyKNcljxgEv6NpZgz8_6ZDBYmEcebUeoX93eGJAE\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)\n",
        "\n",
        "# run tracking UI in the background\n",
        "get_ipython().system_raw(\"mlflow ui --backend-store-uri file:///content/drive/MyDrive/Lorusso/BraTS/mlruns --port 5000 & \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7ygOSsiEAWs"
      },
      "source": [
        "## Utils 🛠"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Lrs_wCJEAJpS"
      },
      "outputs": [],
      "source": [
        "class Logger:\n",
        "    def __init__(self,filename):\n",
        "        self.filename = filename\n",
        "        try:  #try to open file in 'r' (read) mode, if file does not exists the statement will throw an IOexception\n",
        "            log_file = open(self.filename, \"r\")\n",
        "            log_file.close()\n",
        "        except Exception as e: #catch the Exception raised from the block above and create the missing file in the specified path\n",
        "            log_file = open(self.filename, \"w\")\n",
        "            log_file.close()\n",
        "\n",
        "    def log_msg(self,*args):\n",
        "        try:\n",
        "            with open(self.filename, \"a+\") as log_file:\n",
        "                for el in args:\n",
        "                    if(type(el) is list or type(el) is tuple):\n",
        "                        for subel in el:\n",
        "                            log_file.write(subel)\n",
        "                            log_file.write('\\n')\n",
        "                    else:\n",
        "                        log_file.write(el)\n",
        "                        log_file.write('\\n')\n",
        "        except Exception as e:\n",
        "            print('Exception in Logger.log_msg')\n",
        "            print(e)\n",
        "\n",
        "    def read_msg(self):\n",
        "        content = []\n",
        "        try:\n",
        "            with open(self.filename) as file:\n",
        "                for el in file:\n",
        "                    content.append(el)\n",
        "        except Exception as e:\n",
        "            print('Exception in Logger.read_msg')\n",
        "            print(e)\n",
        "        finally:\n",
        "            return content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def untar_brats(tar_path, extract_path):\n",
        "    tar = tarfile.open(tar_path)\n",
        "    tar.extractall(extract_path)\n",
        "    tar.close()\n",
        "\n",
        "def plot_reconstruction(im_orig, im_rec, ax:int = 0, slice_index:int = 100):\n",
        "\n",
        "    f, ax_array = plt.subplots(1,2, figsize=(10,10))\n",
        "    ax_array[0].imshow(np.take(im_orig, indices = slice_index, axis = ax), cmap='gray')\n",
        "    ax_array[1].imshow(np.take( im_rec , indices=slice_index, axis = ax), cmap='gray')\n",
        "\n",
        "def plot_brain_sections(images,ax = 1,slice_index = 90):\n",
        "\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    d1,d2,d3 = np.shape(images[1])\n",
        "    dims = [d1,d2,d3]\n",
        "    dims.pop(ax)\n",
        "    color_segmentation = np.zeros((dims[0],dims[1],3))\n",
        "\n",
        "    gray_segmentation = np.take(images[1],slice_index,axis = ax)\n",
        "    color_segmentation[gray_segmentation == 1] = [255,0,0] # Red (necrotic tumor core)\n",
        "    color_segmentation[gray_segmentation == 2] = [0,255,0] # Green (peritumoral edematous/invaded tissue)\n",
        "    color_segmentation[gray_segmentation == 4] = [0,0,255] # Blue (enhancing tumor)\n",
        "\n",
        "    t1 = images[0][0]\n",
        "    flair = images[0][1]\n",
        "    t2 = images[0][2]\n",
        "    t1ce = images[0][3]\n",
        "\n",
        "    image = t1+t2+flair+t1ce\n",
        "\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.subplot(1,4,1)\n",
        "    plt.imshow(np.take(image,slice_index,axis = 0),cmap='gray')\n",
        "\n",
        "    plt.subplot(1,4,2)\n",
        "    plt.imshow(np.take(image,slice_index,axis = 1),cmap='gray')\n",
        "\n",
        "    plt.subplot(1,4,3)\n",
        "    plt.imshow(np.take(image,slice_index,axis = 2),cmap='gray')\n",
        "\n",
        "    plt.subplot(1,4,4)\n",
        "    plt.imshow(color_segmentation,cmap='gray')\n",
        "    plt.xlabel('Segmentation')\n",
        "\n",
        "\n",
        "def visTensor(tensor, ch=0, allkernels=False, nrow=8, padding=1):\n",
        "    n,c,w,h = tensor.shape\n",
        "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
        "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
        "    rows = np.min((tensor.shape[0] // nrow + 1, 64))\n",
        "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
        "    plt.figure( figsize=(nrow,rows) )\n",
        "    plt.imshow(grid.cpu().permute((1, 2, 0)))\n",
        "\n",
        "\n",
        "def count_labels(triple_list):\n",
        "    label_counts = {}\n",
        "    labels = [triple[3] for triple in triple_list]\n",
        "\n",
        "    # Concatenate all the arrays into one\n",
        "    all_labels = np.concatenate(labels)\n",
        "\n",
        "    # Count the occurrences of each label\n",
        "    counter = Counter(all_labels)\n",
        "\n",
        "    # Create a dict with the counts for labels 1 to 4\n",
        "    counts_dict = {i: counter[i] for i in counter.keys()}\n",
        "    return counts_dict\n",
        "\n",
        "\n",
        "def class_weights_tensor(label_weights):\n",
        "    num_classes = max(label_weights.keys())+1\n",
        "    weight_tensor = torch.zeros(num_classes, dtype=torch.float32)\n",
        "    # Sort the dictionary by keys (labels)\n",
        "    sorted_label_weights = sorted(label_weights.items(), key=lambda x: x[0])\n",
        "    for label, weight in sorted_label_weights:\n",
        "        weight_tensor[label] = weight  # Subtract 1 from label if your labels start from 1\n",
        "    return weight_tensor\n",
        "\n",
        "\n",
        "def compute_average_weights(graphs):\n",
        "    label_counts = count_labels(graphs)\n",
        "    total_count = sum(label_counts.values())\n",
        "    class_weights = {label: total_count / count for label, count in label_counts.items()}\n",
        "    weight_tensor = class_weights_tensor(class_weights)\n",
        "    return weight_tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKEY_j_lCzCa"
      },
      "source": [
        "## Dataset class  💾\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-Pa_KNzpRi0d"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Preprocessing script to convert from data provided by BraTS to data used by our model. Should be the first thing you run.\n",
        "Fulfills the following four functions:\n",
        "1. Normalize and standardize each image of each MRI modality\n",
        "2. Combine multiple MRI modalitities into one image array\n",
        "3. Swap labels from BraTS order (0,2,1,4) to more intuitive order (0,1,2,3)\n",
        "4. Convert image into a graph\n",
        "    Using Simple Linear Iterative Clustering algorithm\n",
        "    Parameters passed on command line\n",
        "\n",
        "If no labels are present (e.g. at test time, in deployment) can also build graph without labels.\n",
        "\n",
        "Saves the following in the specified output directory for each sample\n",
        "MRI_ID/\n",
        "    _input.nii.gz (processed and combined modalities for a sample as nifti file)\n",
        "    _label.nii.gz\n",
        "    _nxgraph.json (networkx graph containing both graph topography and features and labels for each node)\n",
        "    _supervoxels.nii.gz (supervoxel partitioning produced by SLIC)\n",
        "    _crop.npy (optionally the crop of the processed data relative to the original data) (crops out empty image planes)\n",
        "'''\n",
        "\n",
        "LABEL_MAP = {4: 3, 2: 1, 1: 2}\n",
        "\n",
        "\n",
        "def swap_labels_from_brats(label_data):\n",
        "    uniques = np.unique(label_data)\n",
        "    for u in uniques:\n",
        "        if u not in [0, 1, 2, 4]:\n",
        "            raise RuntimeError('unexpected label')\n",
        "    new_label_data = np.zeros_like(label_data, dtype=np.int16)\n",
        "    new_label_data[label_data == 4] = LABEL_MAP[4]\n",
        "    new_label_data[label_data == 2] = LABEL_MAP[2]\n",
        "    new_label_data[label_data == 1] = LABEL_MAP[1]\n",
        "    return new_label_data\n",
        "\n",
        "def swap_labels_to_brats(label_data):\n",
        "    uniques = np.unique(label_data)\n",
        "    for u in uniques:\n",
        "        if u not in [0, 1, 2, 3]:\n",
        "            raise RuntimeError('unexpected label')\n",
        "    new_label_data = np.zeros_like(label_data, dtype=np.int16)\n",
        "    new_label_data[label_data == LABEL_MAP[4]] = 4\n",
        "    new_label_data[label_data == LABEL_MAP[2]] = 2\n",
        "    new_label_data[label_data == LABEL_MAP[1]] = 1\n",
        "    return new_label_data\n",
        "\n",
        "\n",
        "def swap_labels_to_brats_2023(label_data):\n",
        "    uniques = np.unique(label_data)\n",
        "    for u in uniques:\n",
        "        if u not in [0, 1, 2, 3]:\n",
        "            raise RuntimeError('unexpected label')\n",
        "    new_label_data = np.zeros_like(label_data, dtype=np.int16)\n",
        "    new_label_data[label_data == LABEL_MAP[4]] = 3\n",
        "    new_label_data[label_data == LABEL_MAP[2]] = 2\n",
        "    new_label_data[label_data == LABEL_MAP[1]] = 1\n",
        "    return new_label_data\n",
        "\n",
        "class DataPreprocessor(Dataset):\n",
        "    def __init__(self, dotenv_path = \"/content/drive/MyDrive/Lorusso/BraTS/.env\", transform:bool = True, INPUT_PATH = None,\n",
        "                 num_nodes = 15000, boxiness_coef = 0.5, num_neighbors = 0, **kwargs):\n",
        "\n",
        "        load_dotenv(dotenv_path)\n",
        "        # Data mean and variance\n",
        "        data_stats = ([0.4645, 0.6625, 0.4064, 0.3648],\n",
        "                      [0.1593, 0.1703, 0.1216, 0.1627])\n",
        "        self.N_THREADS = 8\n",
        "        self.num_nodes = num_nodes\n",
        "        self.boxiness_coef = boxiness_coef\n",
        "        self.num_neighbors = num_neighbors\n",
        "\n",
        "        if(INPUT_PATH is not None and os.path.exists(INPUT_PATH)):\n",
        "            self.data_dir = INPUT_PATH\n",
        "        else:\n",
        "            self.data_dir = os.getenv('INPUT_DATA_DIR')\n",
        "\n",
        "        self.output_dir = os.getenv('PROCESSED_DATA_DIR')\n",
        "        self.graph_dir = f\"{self.output_dir}_{self.num_nodes}_{self.boxiness_coef}_{self.num_neighbors}{os.sep}{os.path.basename(self.data_dir)}\"\n",
        "        self.logger = Logger(filename=os.path.join(os.getenv('LOG_PATH'), os.path.basename(self.data_dir)+'_logs.txt'))\n",
        "\n",
        "        self.mri_prefix = 'BraTS2021_'\n",
        "        self.modality_extensions = [\"_flair.nii.gz\", \"_t1.nii.gz\", \"_t1ce.nii.gz\", \"_t2.nii.gz\"]\n",
        "        self.label_extension = \"_seg.nii.gz\"\n",
        "        self.dataset_mean = np.array(data_stats[0], dtype=np.float32)\n",
        "        self.dataset_std = np.array(data_stats[1], dtype=np.float32)\n",
        "        self.transform = transform\n",
        "        self.force_conversion = False\n",
        "\n",
        "        # Set or overwrite additional attributes\n",
        "        for el in kwargs.keys():\n",
        "            setattr(self,str(el),kwargs[el])\n",
        "\n",
        "        self.include_labels = self.label_extension is not None\n",
        "        self.all_ids, self.id_to_fp = self.get_all_mris_in_dataset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_ids)\n",
        "\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        # DO NOT APPLY PADDING!! See this issue:\n",
        "        # https://github.com/RobertoLorusso/BraTS/issues/10#issue-2065431255\n",
        "        #imstack, labels = self.padding(imstack, labels)\n",
        "\n",
        "        imstack = self.read_in_patient_sample(idx)\n",
        "        if(self.include_labels):\n",
        "            labels = self.read_in_labels(idx)\n",
        "        else:\n",
        "            labels = None\n",
        "        crop_idxs = None\n",
        "\n",
        "        if (self.transform):\n",
        "            imstack,labels,crop_idxs = self.get_standardized_image(imstack, labels)\n",
        "        return imstack, labels, crop_idxs\n",
        "\n",
        "\n",
        "    def read_in_patient_sample(self, mri_id):\n",
        "\n",
        "        scan_dir = self.data_dir + os.sep + mri_id\n",
        "        modality_exts = self.modality_extensions\n",
        "        num_modalities=len(modality_exts)\n",
        "        modality_imgs = []\n",
        "        for root, _, files in os.walk(scan_dir):\n",
        "            for ext in modality_exts:\n",
        "                for filename in files:\n",
        "                    if filename.endswith(ext):\n",
        "                        filepath = os.path.join(root, filename)\n",
        "                        mod_img = nib.load(filepath)\n",
        "                        #data is actually stored as int16\n",
        "                        img_data = np.array(mod_img.dataobj,dtype=np.float32)\n",
        "                        modality_imgs.append(img_data)\n",
        "        #check that all the modalities were present in the folder\n",
        "        assert(len(modality_imgs)==num_modalities)\n",
        "\n",
        "        patient_sample = np.stack(modality_imgs,3) if num_modalities>1 else modality_imgs[0]\n",
        "        return patient_sample\n",
        "\n",
        "\n",
        "    def read_in_labels(self,mri_id):\n",
        "        scan_dir = self.data_dir + os.sep + mri_id\n",
        "        for filename in os.listdir(scan_dir):\n",
        "            if filename.endswith(self.label_extension):\n",
        "                label_nib = nib.load(scan_dir+os.sep+filename)\n",
        "                #potentially also return affine if they are different between images (which they are not for brats)\n",
        "                return np.array(label_nib.dataobj,dtype=np.int16)\n",
        "        raise FileNotFoundError(f\"Label image not found in folder: {scan_dir}\")\n",
        "\n",
        "\n",
        "    def get_all_mris_in_dataset(self):\n",
        "        mri_folders = glob.glob(f\"{self.data_dir}**/{self.mri_prefix}*/\",\n",
        "                                recursive=True)\n",
        "        mri_folders = self.remove_incomplete_mris(mri_folders)\n",
        "        scan_dic = {os.path.split(fp)[0].split(\"/\")[-1]: fp for fp in mri_folders}\n",
        "        if(len(mri_folders) == 0):\n",
        "            print(\"No MRI found at \" + self.data_dir)\n",
        "        return list(scan_dic.keys()), scan_dic\n",
        "\n",
        "\n",
        "    def remove_incomplete_mris(self, mri_folders):\n",
        "        # if there are any you want to ignore just add them to this list\n",
        "        removed_mris = []\n",
        "        return [fp for fp in mri_folders if fp.split(\"/\")[-2] not in removed_mris]\n",
        "\n",
        "    def padding(self,image, labels):\n",
        "        n_channels = np.shape(image)[0]\n",
        "        max_val = max(np.shape(image))\n",
        "        pad_list = np.zeros([n_channels,max_val,max_val,max_val],dtype=np.float32)\n",
        "\n",
        "        for channel in range(0, n_channels): # pad every channel\n",
        "            pad_list[channel] = np.pad(image[channel],[(42,43),(0,0),(0,0)],'constant')\n",
        "        labels = np.pad(labels, [(42,43),(0,0),(0,0)],'constant')\n",
        "\n",
        "        return pad_list, labels\n",
        "\n",
        "\n",
        "    def get_standardized_image(self, image_data, label_data):\n",
        "\n",
        "        #standardized_labels = self.swap_labels_from_brats(label_data)\n",
        "\n",
        "        crop_idxs = self.determine_brain_crop(image_data)\n",
        "        cropped_data = image_data[crop_idxs]\n",
        "        if(self.include_labels):\n",
        "            cropped_labels = label_data[crop_idxs]\n",
        "            standardized_labels= swap_labels_from_brats(cropped_labels)\n",
        "        else:\n",
        "            standardized_labels = None\n",
        "\n",
        "        normalized_data = self.normalize_img_quantile(cropped_data)\n",
        "        standardized_data = self.standardize_img(normalized_data)\n",
        "        return standardized_data,standardized_labels,crop_idxs\n",
        "\n",
        "\n",
        "    def determine_brain_crop(self,multi_modal_data):\n",
        "        if(len(multi_modal_data.shape)==4):\n",
        "            max_intensity_vals = np.amax(multi_modal_data,axis=3)\n",
        "        elif(len(multi_modal_data.shape)==3):\n",
        "            max_intensity_vals = multi_modal_data\n",
        "        else:\n",
        "            raise Exception(f\"Expected input shape of either nxmxr or nxmxrxC. Instead got {multi_modal_data.shape}\")\n",
        "        mask = max_intensity_vals>0.01\n",
        "        ix = np.ix_(mask.any(axis=(1,2)),mask.any(axis=(0,2)),mask.any(axis=(0,1)))\n",
        "\n",
        "        return ix\n",
        "\n",
        "    def normalize_img(self, img_array):\n",
        "        new_image = np.zeros(img_array.shape, dtype=np.float32)\n",
        "        n_channel = img_array.shape[3] # channel-first images\n",
        "\n",
        "        for channel in range(0, n_channel): # normalize every channel\n",
        "\n",
        "            maxval, minval= np.max(img_array[channel]), np.min(img_array[channel])\n",
        "            new_image[channel] = (img_array[channel] - minval)/(maxval-minval)\n",
        "        return new_image\n",
        "\n",
        "\n",
        "    def normalize_img_quantile(self, img_array):\n",
        "        # Exclude the channel axis, for channel-first images is zero\n",
        "        quantile = np.quantile(img_array, 0.995, axis = (0,1,2) ).astype(np.float32)\n",
        "        img_array = img_array/quantile\n",
        "        return img_array\n",
        "\n",
        "\n",
        "\n",
        "    def standardize_img(self,img_array):\n",
        "        centered = img_array-self.dataset_mean\n",
        "        standardized = centered/self.dataset_std\n",
        "        return standardized\n",
        "\n",
        "\n",
        "    def get_status_ids(self):\n",
        "\n",
        "        common_ids = set()\n",
        "        converting_ids = set()\n",
        "        finished_ids = set()\n",
        "\n",
        "        finished = []\n",
        "        pending = []\n",
        "\n",
        "        try:\n",
        "            regex = r'(Converting|Finished) ' + self.mri_prefix + '(\\d+)([-_]\\d+)?'\n",
        "\n",
        "            content = self.logger.read_msg()\n",
        "\n",
        "            # Define the regular expression pattern\n",
        "            pattern = re.compile(regex)\n",
        "            # Find all occurrences in the list\n",
        "            matches = [match.groups() for s in content if (match := pattern.match(s))]\n",
        "\n",
        "            for action, brats_id, suffix in matches:\n",
        "                if action == 'Converting':\n",
        "                    if(suffix is not None):\n",
        "                        converting_ids.add(self.mri_prefix + brats_id + suffix)\n",
        "                    else:\n",
        "                        converting_ids.add(self.mri_prefix + brats_id)\n",
        "                elif action == 'Finished':\n",
        "                    if(suffix is not None):\n",
        "                        finished_ids.add(self.mri_prefix + brats_id + suffix)\n",
        "                    else:\n",
        "                        finished_ids.add(self.mri_prefix + brats_id)\n",
        "            # Find finished and pending conversions\n",
        "            common_ids = converting_ids.intersection(finished_ids)\n",
        "            finished = [el for el in common_ids]\n",
        "            pending = [el for el in converting_ids.difference(common_ids)]\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Exception in DataPreprocessor.get_status_ids')\n",
        "            print(e)\n",
        "\n",
        "        return {'Finished':finished, 'Pending':pending}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def remove_pending_graphs(self):\n",
        "\n",
        "        pending = self.get_status_ids()['Pending']\n",
        "\n",
        "        for mri_id in pending:\n",
        "            remove_path = f\"{self.graph_dir}{os.sep}{mri_id}\"\n",
        "            try:\n",
        "                print('Removing pending graph: ' + remove_path)\n",
        "                shutil.rmtree(remove_path)\n",
        "            except Exception as e:\n",
        "                print('Exception in DataPreprocessor.remove_pending_graphs:')\n",
        "                print(e)\n",
        "\n",
        "\n",
        "\n",
        "    def split_dataset(self, fixed = (1001, 125, 125),seed = 42):\n",
        "\n",
        "        random.seed(seed)\n",
        "        pos = random.sample(range(0,len(self.all_ids)), len(self.all_ids))\n",
        "\n",
        "        if fixed:\n",
        "            if(np.sum(fixed) != len(self.all_ids)):\n",
        "                print(\"Error: fixed ratio does not sum up to one.\\nSwitching to default (1001,125,125))\")\n",
        "                fixed = (1001,125,125)\n",
        "\n",
        "            train_length = fixed[0]\n",
        "            val_length = fixed[1]\n",
        "            test_length = fixed[2]\n",
        "\n",
        "\n",
        "        split_dict = {\n",
        "            'train': [self.all_ids[i] for i in pos[:train_length]],\n",
        "            'val': [self.all_ids[i] for i in pos[train_length :train_length + val_length]],\n",
        "            'test': [self.all_ids[i] for i in pos[train_length + val_length:]]\n",
        "        }\n",
        "\n",
        "        for k in split_dict.keys():\n",
        "            parent = '/'.join(self.data_dir.split('/')[:-1])\n",
        "            dst = os.path.join(parent,k)\n",
        "\n",
        "            try:\n",
        "              # create train,val,test dirs\n",
        "              if(not os.path.exists(dst)):\n",
        "                os.mkdir(dst)\n",
        "\n",
        "              # copy splitted data inside folders\n",
        "              for id in split_dict[k]:\n",
        "                if(not os.path.exists(os.path.join(dst,id))):\n",
        "                   os.mkdir(os.path.join(dst,id))\n",
        "                copy_tree(self.id_to_fp[id],os.path.join(dst,id))\n",
        "\n",
        "            except Exception as e:\n",
        "              print(f\"Exception thrown in class {self.__class__.__name__ }, method split_dataset\")\n",
        "              print(e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def image_to_graph(self, mri_id):\n",
        "\n",
        "\n",
        "        save_path = f\"{self.graph_dir}{os.sep}{mri_id}\"\n",
        "        finished = self.get_status_ids()['Finished']\n",
        "\n",
        "        if(mri_id not in finished):\n",
        "            self.logger.log_msg('Converting ' + str(mri_id))\n",
        "        print('Converting ' + str(mri_id))\n",
        "\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "\n",
        "        if(not os.path.exists(f\"{save_path}{os.sep}{mri_id}_input.nii.gz\") or self.force_conversion == True):\n",
        "            imstack, labels, crop_idxs = self.__getitem__(mri_id)\n",
        "        else:\n",
        "            imstack, labels = self.get_image(mri_id)\n",
        "\n",
        "        # Load supervoxels, if already exist, and save computational time avoiding the runnning of slic\n",
        "        sv_partitioning = None\n",
        "        if(os.path.exists(f\"{save_path}{os.sep}{mri_id}_supervoxels.nii.gz\")):\n",
        "            sv_partitioning = self.get_supervoxel_partitioning(mri_id)\n",
        "            print('Loading supervoxels...')\n",
        "\n",
        "        nx_graph,node_feats,region_img = img2graph(imstack,labels,sv_partitioning,self.num_nodes,self.boxiness_coef,self.num_neighbors)\n",
        "\n",
        "        save_networkx_graph(nx_graph, f\"{save_path}{os.sep}{mri_id}_nxgraph.json\")\n",
        "        save_as_nifti(imstack,f\"{save_path}{os.sep}{mri_id}_input.nii.gz\")\n",
        "        if(self.include_labels):\n",
        "            save_as_nifti(labels,f\"{save_path}{os.sep}{mri_id}_label.nii.gz\")\n",
        "        save_as_nifti(region_img,f\"{save_path}{os.sep}{mri_id}_supervoxels.nii.gz\")\n",
        "\n",
        "        with open(f\"{save_path}{os.sep}{mri_id}_crop.pkl\", \"wb\") as f:\n",
        "            pickle.dump(crop_idxs, f)\n",
        "\n",
        "        return mri_id\n",
        "\n",
        "    def get_voxel_labels(self,mri_id):\n",
        "        fp=f\"{self.graph_dir}{os.sep}{mri_id}{os.sep}{mri_id}_label.nii.gz\"\n",
        "        return read_nifti(fp,np.int16)\n",
        "\n",
        "    def get_image(self,mri_id):\n",
        "        fp = f\"{self.graph_dir}{os.sep}{mri_id}{os.sep}{mri_id}_input.nii.gz\"\n",
        "        img = read_nifti(fp,np.float32)\n",
        "        return img,self.get_voxel_labels(mri_id)\n",
        "\n",
        "\n",
        "    def get_supervoxel_partitioning(self,mri_id):\n",
        "        fp=f\"{self.graph_dir}{os.sep}{mri_id}{os.sep}{mri_id}_supervoxels.nii.gz\"\n",
        "        return read_nifti(fp,np.int16)\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        self.remove_pending_graphs()\n",
        "\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.N_THREADS) as executor:\n",
        "            futures = [executor.submit(self.image_to_graph, mri_id) for mri_id in self.all_ids]\n",
        "            print(\"Set up Threads, starting execution\")\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                try:\n",
        "                    mri_id = future.result()\n",
        "                except Exception as exc:\n",
        "                    print(\"Exception caught in DataPreprocessor.run\")\n",
        "                    print(f\"{exc}\")\n",
        "                else:\n",
        "                    if(mri_id not in self.get_status_ids()['Finished']):\n",
        "                        # Log message\n",
        "                        self.logger.log_msg('Finished ' + str(mri_id))\n",
        "                    print(\"Finished \"+ str(mri_id))\n",
        "\n",
        "\n",
        "'''\n",
        "A Dataset similar to a torch dataset which iterates over all samples in a directory and returns the contents as numpy arrays.\n",
        "Expects to receive a filepath to the output of the preprocess script which should have the following:\n",
        "1.processed image (nifti)\n",
        "2.label image (nifti)\n",
        "3.networkx graph (json)\n",
        "4.supervoxel partitioning (nifti)\n",
        "5. (optionally) a .npy file containing the crop of the processed image relative to the original image\n",
        "\n",
        "\n",
        "#Input#\n",
        "dataset_root_dir: filepath to preprocessed dataset (generated by running preprocess script)\n",
        "mri_start_string: a prefix that every image folder starts with (can be empty string)\n",
        "read_image: whether to read in and return preprocessed images for each sample (only necessary for CNN model)\n",
        "read_graph: whether to return graphs for each sample (for training GNN)\n",
        "read_label: whether to read in labels. Will be returned in vector form (one label per node if )\n",
        "\n",
        "#Output#\n",
        "\n",
        "If graph:\n",
        "Returns a DGL Graph, features for each node, and (optionally) labels for each node\n",
        "If image:\n",
        "Returns a numpy image array and (optionally) a numpy label array\n",
        "\n",
        "'''\n",
        "\n",
        "class ImageGraphDataset(Dataset):\n",
        "    def __init__(self, dataset_root_dir,mri_start_string,read_image=True,read_graph=True,read_label=True, features = None):\n",
        "        self.dataset_root_dir=dataset_root_dir\n",
        "        self.all_ids = self.get_all_mris_in_dataset(dataset_root_dir,mri_start_string)\n",
        "        self.read_image=read_image\n",
        "        self.read_graph=read_graph\n",
        "        self.read_label = read_label\n",
        "        self.features = features\n",
        "        assert(self.read_graph or self.read_image)\n",
        "\n",
        "    def get_all_mris_in_dataset(self,dataset_root_dir,mri_start_string):\n",
        "        mri_folders = glob.glob(f\"{dataset_root_dir}**/{mri_start_string}*/\",recursive=True)\n",
        "        mri_ids = [fp.split(os.sep)[-2] for fp in mri_folders]\n",
        "        print(f\"Found {len(mri_folders)} MRIs\")\n",
        "        return mri_ids\n",
        "\n",
        "    def get_one(self,mri_id):\n",
        "        if(self.read_graph and not self.read_image):\n",
        "            return (mri_id, *self.get_graph(mri_id))\n",
        "        elif(self.read_image  and not self.read_graph):\n",
        "            return (mri_id, *self.get_image(mri_id))\n",
        "        elif(self.read_image and self.read_graph):\n",
        "            return (mri_id, *self.get_graph(mri_id), *self.get_image(mri_id))\n",
        "        else:\n",
        "            print(\"Invalid combination of flags\")\n",
        "\n",
        "    def add_features(self,features):\n",
        "        feats_len = len(features)\n",
        "        new_features = np.zeros([feats_len + len(self.features)])\n",
        "        new_features[:feats_len] = features\n",
        "\n",
        "        new_features[:feats_len] = np.copy(features)\n",
        "        new_features[feats_len:] = np.copy(self.features)\n",
        "\n",
        "        return np.array(new_features)\n",
        "\n",
        "    '''\n",
        "    Reads in the saved networkx graph, converts it to a DGLGraph, normalizes the graph (not actually sure how useful this is),\n",
        "    and returns the DGLGraph as well as a vector of node features and optionally labels.\n",
        "    '''\n",
        "    def get_graph(self,mri_id):\n",
        "        nx_graph = load_networkx_graph(f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_nxgraph.json\")\n",
        "        #features = np.array([nx_graph.nodes[n]['features'] for n in nx_graph.nodes])\n",
        "        features = []\n",
        "        for n in nx_graph.nodes:\n",
        "            node_features = np.array(nx_graph.nodes[n]['features'])\n",
        "            if(self.features is not None):\n",
        "                node_features = self.add_features(node_features)\n",
        "            features.append(node_features)\n",
        "        features = np.array(features, dtype=np.float32)\n",
        "\n",
        "        if(self.read_label):\n",
        "            labels = np.array([nx_graph.nodes[n]['label'] for n in nx_graph.nodes])\n",
        "\n",
        "        G = to_dgl_graph(nx_graph)\n",
        "        n_edges = G.number_of_edges()\n",
        "        # normalization\n",
        "        degs = G.in_degrees().float()\n",
        "        norm = torch.pow(degs, -0.5)\n",
        "        norm[torch.isinf(norm)] = 0\n",
        "        G.ndata['norm'] = norm.unsqueeze(1)\n",
        "        #G.ndata['feat'] = features\n",
        "        if(self.read_label):\n",
        "            #G.ndata['label'] = labels\n",
        "            return G, features, labels\n",
        "        return G, features\n",
        "\n",
        "    def get_voxel_labels(self,mri_id):\n",
        "        fp=f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_label.nii.gz\"\n",
        "        return read_nifti(fp,np.int16)\n",
        "\n",
        "    def get_image(self,mri_id):\n",
        "        fp = f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_input.nii.gz\"\n",
        "        img = read_nifti(fp,np.float32)\n",
        "        if(self.read_label):\n",
        "            return img,self.get_voxel_labels(mri_id)\n",
        "        else:\n",
        "            return (img,)\n",
        "\n",
        "    def get_supervoxel_partitioning(self,mri_id):\n",
        "        fp=f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_supervoxels.nii.gz\"\n",
        "        return read_nifti(fp,np.int16)\n",
        "\n",
        "    def get_crop(self,mri_id):\n",
        "        fp=f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_crop.pkl\"\n",
        "        return tuple(np.load(fp,allow_pickle=True))\n",
        "\n",
        "    def save_voxel_preds(self, mri_id, predicted_nodes):\n",
        "        supervoxel_partitioning = self.get_supervoxel_partitioning(mri_id)\n",
        "        raw_data_crop = self.get_crop(mri_id)\n",
        "        predicted_voxels = project_nodes_to_img(supervoxel_partitioning,predicted_nodes)\n",
        "        predicted_voxels = uncrop_to_brats_size(raw_data_crop,predicted_voxels)\n",
        "        predicted_voxels = swap_labels_to_brats_2023(predicted_voxels)\n",
        "        save_as_nifti(predicted_voxels,f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_seg.nii.gz\")\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        for mri_id in self.all_ids:\n",
        "            yield self.get_one(mri_id)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        mri_id = self.all_ids[index]\n",
        "        #print(index)\n",
        "        #mri_id = [el for el in self.all_ids if el == index][0]\n",
        "        return self.get_one(mri_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_ids)\n",
        "\n",
        "\n",
        "\n",
        "class SeqSampler(SequentialSampler):\n",
        "    \"\"\"Samples elements sequentially, always in the same order.\n",
        "\n",
        "    Args:\n",
        "        data_source (Dataset): dataset to sample from\n",
        "    \"\"\"\n",
        "    def __init__(self, data_source:Dataset):\n",
        "        self.data_source = data_source\n",
        "        self.indexDict = [id for id in data_source.all_ids]\n",
        "    def __iter__(self):\n",
        "        return iter(self.indexDict)\n",
        "    def __len__(self):\n",
        "        return len(self.indexDict)\n",
        "\n",
        "\n",
        "def minibatch_graphs(samples):\n",
        "    mri_ids,graphs,features, labels = map(list, zip(*samples))\n",
        "    #print(\"Batch Mri Ids:\",mri_ids)\n",
        "    batched_graph = dgl_batch(graphs)\n",
        "    return mri_ids,batched_graph, torch.FloatTensor(np.concatenate(features)), torch.LongTensor(np.concatenate(labels))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgINpL8BK3hL"
      },
      "source": [
        "## Models 📪\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "78WIjwSZRkGN"
      },
      "outputs": [],
      "source": [
        "\n",
        "from dgl.nn.pytorch import GATConv, GraphConv\n",
        "from dgl.nn.pytorch.conv import SAGEConv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "'''\n",
        "Contains the actual neural network architectures.\n",
        "Supports GraphSAGE with either the pool,mean,gcn, or lstm aggregator as well as GAT.\n",
        "The input, output, and intermediate layer sizes can all be specified.\n",
        "Typically will call init_graph_net and pass along the desired model and hyperparameters.\n",
        "\n",
        "Also contains the CNN Refinement net which is a very simple 2 layer 3D convolutional neural network.\n",
        "As input, it expects 8 channels, which are the concatenated 4 input modalities and 4 output logits of the GNN predictions.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "class GraphSage(nn.Module):\n",
        "    def __init__(self,in_feats,layer_sizes,n_classes,aggregator_type,dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        # input layer\n",
        "        self.layers.append(SAGEConv(in_feats, layer_sizes[0], aggregator_type, feat_drop=dropout, activation=F.relu))\n",
        "        # hidden layers\n",
        "        for i in range(1,len(layer_sizes)):\n",
        "            self.layers.append(SAGEConv(layer_sizes[i-1], layer_sizes[i], aggregator_type, feat_drop=dropout, activation=F.relu))\n",
        "        # output layer\n",
        "        self.layers.append(SAGEConv(layer_sizes[-1], n_classes, aggregator_type, feat_drop=0, activation=None))\n",
        "\n",
        "    def forward(self,graph,features):\n",
        "        h = features\n",
        "        for layer in self.layers:\n",
        "            h = layer(graph, h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self,in_feats,layer_sizes,n_classes,heads,residuals,\n",
        "                activation=F.elu,feat_drop=0,attn_drop=0,negative_slope=0.2):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        # input projection (no residual)\n",
        "        self.layers.append(GATConv(\n",
        "            in_feats, layer_sizes[0], heads[0],\n",
        "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
        "        # hidden layers\n",
        "        for i in range(1, len(layer_sizes)):\n",
        "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "            self.layers.append(GATConv(\n",
        "                layer_sizes[i-1] * heads[i-1], layer_sizes[i], heads[i],\n",
        "                feat_drop, attn_drop, negative_slope, residuals[i], self.activation))\n",
        "        # output projection\n",
        "        self.layers.append(GATConv(\n",
        "            layer_sizes[-1] * heads[-1], n_classes, 1,\n",
        "            feat_drop, attn_drop, negative_slope, False, None))\n",
        "\n",
        "    def forward(self,g, inputs):\n",
        "        h = inputs\n",
        "        for l in range(len(self.layers)-1):\n",
        "            h = self.layers[l](g, h).flatten(1)\n",
        "        # output projection\n",
        "        logits = self.layers[-1](g, h).mean(1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class GIN(nn.Module):\n",
        "    def __init__(self, in_feats, layer_sizes, n_classes, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # input layer\n",
        "        self.layers.append(GINConv(apply_func=nn.Linear(in_feats, layer_sizes[0]), aggregator_type='sum'))\n",
        "        # hidden layers\n",
        "        for i in range(1, len(layer_sizes)):\n",
        "            self.layers.append(GINConv(apply_func=nn.Linear(layer_sizes[i-1], layer_sizes[i]), aggregator_type='sum'))\n",
        "        # output layer\n",
        "        self.layers.append(GINConv(apply_func=nn.Linear(layer_sizes[-1], n_classes), aggregator_type='sum'))\n",
        "\n",
        "    def forward(self, g, feat):\n",
        "        h = feat\n",
        "        for layer in self.layers[:-1]:\n",
        "            h = layer(g, h)\n",
        "            h = F.relu(h)\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = self.layers[-1](g, h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class ChebNet(nn.Module):\n",
        "    def __init__(self, in_feats, layer_sizes, n_classes, k, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Input layer\n",
        "        self.layers.append(ChebConv(in_feats, layer_sizes[0], k))\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(layer_sizes)):\n",
        "            self.layers.append(ChebConv(layer_sizes[i-1], layer_sizes[i], k))\n",
        "\n",
        "        # Output layer\n",
        "        self.layers.append(ChebConv(layer_sizes[-1], n_classes, k))\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            h = layer(g, h)\n",
        "            if i != len(self.layers) - 1: # No activation and dropout on the output layer\n",
        "                h = F.relu(h)\n",
        "                h = self.dropout(h)\n",
        "        return h\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u3q4OhjYyGq"
      },
      "source": [
        "## Model Wrapper 📨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "biiQzha9JP8G"
      },
      "outputs": [],
      "source": [
        "class ModelWrapper():\n",
        "    \"\"\"\n",
        "    Allows train, evaluation, prediction and I/O operations on generic PyTorch models\n",
        "    The model is saved at every epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, optimizer: nn.Module, loss_fn: nn.Module,\n",
        "                 num_epochs: int, supervised: bool = True, dict_params:dict = {}, eval_metrics = None, isgnn:bool = True, LOAD_MODEL: bool = False,\n",
        "                 model_path: str  = '/content/drive/MyDrive/Lorusso/models',):\n",
        "\n",
        "        self.device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "        self.model = model\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model = self.model.to(torch.float)\n",
        "        self.num_epochs = num_epochs\n",
        "        self.loss_fn = loss_fn\n",
        "        self.loss_fn = self.loss_fn.to(self.device)\n",
        "        self.eval_metrics = eval_metrics\n",
        "        self.optimizer = optimizer\n",
        "        self.model_path = model_path\n",
        "        self.save_path = self.model_path + '/' + self.model.__class__.__name__ + '/model.pt'\n",
        "\n",
        "        self.run_id = None\n",
        "        self.LOAD_MODEL = LOAD_MODEL\n",
        "        self.isgnn = isgnn\n",
        "        self.supervised = supervised\n",
        "        self.training_loss = []\n",
        "        self.validation_loss = []\n",
        "        self.dict_metrics = {}\n",
        "        self.elapsed_epochs = 0\n",
        "        self.elapsed_seconds = 0\n",
        "\n",
        "        self.dict_params = dict_params\n",
        "        self.update_params({'loss_fn':self.loss_fn.__class__.__name__})\n",
        "        self.update_params({'optimizer':self.optimizer.__class__.__name__})\n",
        "        self.update_params({'learning_rate':self.optimizer.state_dict()['param_groups'][0]['lr']})\n",
        "        self.update_params({'weight_decay':self.optimizer.state_dict()['param_groups'][0]['weight_decay']})\n",
        "\n",
        "        if(self.LOAD_MODEL):\n",
        "          self.load_checkpoint()\n",
        "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma = 0.98)\n",
        "        # Create directory for model loading\n",
        "        #try:\n",
        "        #  if(not os.path.exists(self.model_path + '/' + self.model.__class__.__name__)):\n",
        "        #    os.mkdir(self.model_path + '/' + self.model.__class__.__name__)\n",
        "        #except Exception as e:\n",
        "        #  print(f\"Exception thrown in class {self.__class__.__name__ }, method __init__\")\n",
        "        #  print(e)\n",
        "        #  print('\\n')\n",
        "\n",
        "\n",
        "    def update_params(self, new_params):\n",
        "        try:\n",
        "            self.dict_params.update(new_params)\n",
        "        except Exception as e:\n",
        "            print('Exception raised in WrapperModel.update_params')\n",
        "            print(e)\n",
        "\n",
        "    def log_checkpoint(self, info: dict):\n",
        "        mlflow.pytorch.log_state_dict(info, artifact_path='checkpoint')\n",
        "        #torch.save(info, self.save_path)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        \"\"\" Loads the last checkpoint for the given model \"\"\"\n",
        "        try:\n",
        "            if(self.run_id is None):\n",
        "                run_id = mlflow.search_runs(experiment_names=['BraTS_'+type(self.model).__name__],\n",
        "                                        order_by=[\"start_time DESC\"]).iloc[0].run_id\n",
        "            else:\n",
        "                run_id = self.run_id\n",
        "\n",
        "            checkpoint = mlflow.pytorch.load_state_dict('runs:/'+run_id+'/checkpoint',\n",
        "                                                        map_location=torch.device(self.device))\n",
        "\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.elapsed_epochs = len(checkpoint['training_loss'])\n",
        "            self.training_loss = checkpoint['training_loss']\n",
        "            self.validation_loss = checkpoint['validation_loss']\n",
        "            self.dict_metrics = checkpoint['dict_metrics']\n",
        "            self.elapsed_seconds = checkpoint['elapsed_seconds']\n",
        "            #print(self.elapsed_epochs)\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Exception thrown in class {self.model.__class__.__name__ }, method load_checkpoint\")\n",
        "            print(e)\n",
        "            print('\\n')\n",
        "            if(mlflow.active_run()):\n",
        "                mlflow.end_run()\n",
        "\n",
        "\n",
        "    #Calculates a slew of different metrics that might be interesting such as the number of nodes of each label and voxel and node Dice scores\n",
        "    def calculate_all_metrics_for_brain(self,mri_id,dataset,node_preds,node_labels):\n",
        "        label_counts = np.concatenate([evaluation.count_node_labels(node_preds),evaluation.count_node_labels(node_labels)])\n",
        "        node_dices = evaluation.calculate_node_dices(node_preds,node_labels)\n",
        "        #read in voxel_labels and supervoxel mapping to compute the image metrics\n",
        "        sv_partitioning = dataset.get_supervoxel_partitioning(mri_id)\n",
        "        true_voxels = dataset.get_voxel_labels(mri_id)\n",
        "        pred_voxels = project_nodes_to_img(sv_partitioning,node_preds)\n",
        "        voxel_metrics = evaluation.calculate_brats_metrics(pred_voxels,true_voxels)\n",
        "        return label_counts,np.concatenate([node_dices,voxel_metrics])\n",
        "\n",
        "\n",
        "    def train(self, train_loader, val_loader = None,  experiment_prefix = ''):\n",
        "\n",
        "        try:\n",
        "            # Set MLFlow experiment\n",
        "            if(experiment_prefix):\n",
        "                mlflow.set_experiment(experiment_prefix + self.model.__class__.__name__)\n",
        "            else:\n",
        "                mlflow.set_experiment('BraTS_'+self.model.__class__.__name__)\n",
        "\n",
        "            # Start a new run if the model wasn't loaded\n",
        "            if(not mlflow.active_run()):\n",
        "                # Track metrics in the current run\n",
        "                mlflow.start_run()\n",
        "            elif(self.LOAD_MODEL == False and mlflow.active_run()):\n",
        "                mlflow.end_run()\n",
        "                mlflow.start_run()\n",
        "\n",
        "            for i in range(len(self.training_loss)):\n",
        "                mlflow.log_metric('train_loss', self.training_loss[i], step=i)\n",
        "\n",
        "            for i in range(len(self.validation_loss)):\n",
        "                mlflow.log_metric('val_loss', self.validation_loss[i], step=i)\n",
        "\n",
        "            self.update_params({'batch_size':train_loader.batch_size})\n",
        "            mlflow.log_params(self.dict_params)\n",
        "\n",
        "            training_loss = self.training_loss\n",
        "            validation_loss = self.validation_loss\n",
        "\n",
        "            self.tot_epochs = self.elapsed_epochs + self.num_epochs+1\n",
        "            self.tot_time = time.time()\n",
        "\n",
        "            # Train\n",
        "            for epoch in range(self.elapsed_epochs+1, self.tot_epochs):\n",
        "                start = time.time() # track time\n",
        "\n",
        "              # Evaluate first if loaded model missed the evaluation during an epoch\n",
        "                if(len(training_loss) > len(validation_loss)):\n",
        "\n",
        "                    # COMPLETE EVALUATION OF PREVIOUS EPOCH\n",
        "                    # NB: epoch = epoch - 1\n",
        "                    if(val_loader is not None):\n",
        "\n",
        "                        val_batch_loss = self.__eval(val_loader, (epoch-1) )\n",
        "                        validation_loss.append(np.array(val_batch_loss).mean())\n",
        "\n",
        "                        # Log additional metrics\n",
        "                        self.dict_metrics = self.__eval_metrics(val_loader)\n",
        "                        for k in self.dict_metrics.keys():\n",
        "                            mlflow.log_metric(str(k), self.dict_metrics[k], step=epoch-1)\n",
        "\n",
        "                        mlflow.log_metric('val_loss',validation_loss[-1], step=epoch-1)\n",
        "\n",
        "                        print(f\"Epoch: {epoch-1}/{self.tot_epochs-1}, Validation Loss: {validation_loss[-1]:.4f}, Elapsed time: {time.time() - start:.0f} sec\")\n",
        "\n",
        "                      # Update training time\n",
        "                        epoch_time = int(time.time() - self.tot_time) + self.elapsed_seconds\n",
        "\n",
        "                      #Create checkpoint\n",
        "                        val_dict = {\n",
        "                                  'model_state_dict': self.model.state_dict(),\n",
        "                                  'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                                  'training_loss': training_loss,\n",
        "                                  'validation_loss': validation_loss,\n",
        "                                  'dict_metrics': self.dict_metrics,\n",
        "                                  'elapsed_seconds': epoch_time\n",
        "                                  }\n",
        "                        self.log_checkpoint(val_dict)\n",
        "                    else:\n",
        "                        # Kind of exception, needed to keep the vectors of the same size\n",
        "                        validation_loss.append(np.mean(validation_loss))\n",
        "\n",
        "                        #Log metric\n",
        "                        mlflow.log_metric('val_loss',validation_loss[-1], step=epoch-1)\n",
        "\n",
        "                #### TRAIN ######\n",
        "                train_batch_loss = self.__train(train_loader, epoch)\n",
        "                training_loss.append(np.array(train_batch_loss).mean())\n",
        "\n",
        "                # Log metrics\n",
        "                mlflow.log_metric('train_loss',training_loss[-1], step=epoch) # MLFLOW tracking\n",
        "                print(f\"Epoch: {epoch}/{self.tot_epochs-1}, Loss: {training_loss[-1]:.4f}, Epoch elapsed time: {time.time() - start:.0f} sec \\n\")\n",
        "\n",
        "                #Save model every elapsed epoch\n",
        "                self.elapsed_epochs = epoch\n",
        "                epoch_time = int(time.time() - self.tot_time) + self.elapsed_seconds\n",
        "\n",
        "                train_dict = {\n",
        "                          'model_state_dict': self.model.state_dict(),\n",
        "                          'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                          'training_loss': training_loss,\n",
        "                          'validation_loss': validation_loss,\n",
        "                          'dict_metrics':self.dict_metrics,\n",
        "                          'elapsed_seconds': epoch_time\n",
        "                          }\n",
        "                self.log_checkpoint(train_dict)\n",
        "\n",
        "                #### EVALUATE ######\n",
        "                if(val_loader is not None):\n",
        "\n",
        "                    val_batch_loss = self.__eval(val_loader, epoch)\n",
        "                    validation_loss.append(np.array(val_batch_loss).mean())\n",
        "\n",
        "                    # Log additional metrics\n",
        "                    self.dict_metrics = self.__eval_metrics(val_loader)\n",
        "                    for k in self.dict_metrics.keys():\n",
        "                        mlflow.log_metric(str(k), self.dict_metrics[k], step=epoch)\n",
        "\n",
        "                    # Log metric\n",
        "                    mlflow.log_metric('val_loss',validation_loss[-1], step=epoch)\n",
        "                    print(f\"Epoch: {epoch}/{self.tot_epochs-1}, Validation Loss: {validation_loss[-1]:.4f}, Elapsed time: {time.time() - start:.0f} sec\\n\")\n",
        "\n",
        "                    #Checkpoint\n",
        "                    epoch_time = int(time.time() - self.tot_time) + self.elapsed_seconds\n",
        "                    val_dict = {\n",
        "                              'model_state_dict': self.model.state_dict(),\n",
        "                              'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                              'training_loss': training_loss,\n",
        "                              'validation_loss': validation_loss,\n",
        "                              'dict_metrics':self.dict_metrics,\n",
        "                              'elapsed_seconds': epoch_time\n",
        "                              }\n",
        "                    self.log_checkpoint(val_dict)\n",
        "\n",
        "            print(f\"Total training time: {time.time()-self.tot_time:.0f} sec\")\n",
        "\n",
        "            # Log model --> end run\n",
        "            mlflow.pytorch.log_model(self.model, artifact_path='model')\n",
        "            mlflow.end_run()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Exception thrown in class Wrapper, method train:\")\n",
        "            print(e)\n",
        "            print('\\n')\n",
        "            mlflow.end_run()\n",
        "\n",
        "        return training_loss, validation_loss\n",
        "\n",
        "\n",
        "\n",
        "    def __train(self, train_loader: DataLoader, epoch:int):\n",
        "\n",
        "        \"\"\" Train for an epoch \"\"\"\n",
        "\n",
        "        self.model.train()\n",
        "        train_batch_loss = []\n",
        "        train_steps = len(train_loader)\n",
        "\n",
        "        if(self.isgnn):\n",
        "\n",
        "            metrics = np.zeros((len(train_loader),3))\n",
        "            i = 0\n",
        "\n",
        "            for batch_mris, batch_graphs, batch_features, batch_labels in train_loader:\n",
        "\n",
        "                batch_graphs = batch_graphs.to(self.device)\n",
        "                batch_features = batch_features.to(self.device)\n",
        "                batch_labels = batch_labels.to(self.device)\n",
        "\n",
        "                logits = self.model(batch_graphs,batch_features)\n",
        "                logits = logits.to(self.device)\n",
        "                #_, predicted_classes = torch.max(logits, dim=1)\n",
        "                predicted_classes = logits.argmax(1)\n",
        "\n",
        "                loss = self.loss_fn(logits, batch_labels)\n",
        "                train_batch_loss.append(loss.detach().item())\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                metrics[i][:] = evaluation.calculate_node_dices(predicted_classes.detach().cpu().numpy(),\n",
        "                                                          batch_labels.detach().cpu().numpy())\n",
        "\n",
        "                i = i+1\n",
        "                out = f\"Epoch: {epoch}/{self.tot_epochs-1}, Step: {i+1}/{train_steps}, Training Loss: {loss.item():.4f}, Elapsed time: {time.time() - self.tot_time:.0f} sec \"\n",
        "                sys.stdout.write(\"\\r\" + out)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "\n",
        "            self.scheduler.step()\n",
        "            avg_metrics = np.mean(metrics,axis=0)\n",
        "\n",
        "            print(f\"train_wt_dice: {np.round(avg_metrics[0],4)}\")\n",
        "            print(f\"train_ct_dice: {np.round(avg_metrics[1],4)}\")\n",
        "            print(f\"train_at_dice: {np.round(avg_metrics[2],4)}\")\n",
        "\n",
        "            mlflow.log_metric('train_wt_dice', np.round(avg_metrics[0],4), step = epoch)\n",
        "            mlflow.log_metric('train_ct_dice', np.round(avg_metrics[1],4), step = epoch)\n",
        "            mlflow.log_metric('train_at_dice', np.round(avg_metrics[2],4), step = epoch)\n",
        "\n",
        "        else:\n",
        "\n",
        "            for i, (data,labels) in enumerate(train_loader):\n",
        "\n",
        "                #torch.cuda.empty_cache()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                if self.device == 'cuda':\n",
        "                  data = data.type(torch.cuda.FloatTensor)\n",
        "                else:\n",
        "                  data = data.type(torch.FloatTensor)\n",
        "\n",
        "                data = data.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                outputs = self.model(data)\n",
        "\n",
        "                if(self.supervised):\n",
        "                  loss = self.loss_fn(outputs,labels)\n",
        "                else:\n",
        "                  loss = self.loss_fn(outputs, data)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                train_batch_loss.append(loss.detach().item())\n",
        "\n",
        "                out = f\"Epoch: {epoch}/{self.tot_epochs-1}, Step: {i+1}/{train_steps}, Training Loss: {loss.item():.4f}, Elapsed time: {time.time() - self.tot_time:.0f} sec \"\n",
        "                sys.stdout.write(\"\\r\" + out)\n",
        "                sys.stdout.flush()\n",
        "            torch.cuda.empty_cache()\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        return train_batch_loss\n",
        "\n",
        "\n",
        "    def __eval(self, val_loader: DataLoader, epoch:int):\n",
        "\n",
        "        \"\"\" Evaluate for an epoch \"\"\"\n",
        "\n",
        "        val_steps =  len(val_loader)\n",
        "        self.model.eval()\n",
        "        val_batch_loss = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if(self.isgnn):\n",
        "\n",
        "                #metrics stores loss and node dices\n",
        "                metrics = np.zeros((len(val_loader),3))\n",
        "                i=0\n",
        "\n",
        "                for curr_id,batch_graphs,batch_feats,batch_labels in val_loader:\n",
        "\n",
        "                    batch_graphs = batch_graphs.to(self.device)\n",
        "                    batch_feats = torch.FloatTensor(batch_feats).to(self.device)\n",
        "                    batch_labels = torch.LongTensor(batch_labels).to(self.device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        logits = self.model(batch_graphs,batch_feats)\n",
        "                        val_loss = self.loss_fn(logits, batch_labels)\n",
        "\n",
        "                    val_batch_loss.append(val_loss.detach().item())\n",
        "                    #_, predicted_classes = torch.max(logits, dim=1)\n",
        "                    predicted_classes = logits.argmax(1)\n",
        "                    res = evaluation.calculate_node_dices(predicted_classes.detach().cpu().numpy(),\n",
        "                                                          batch_labels.detach().cpu().numpy())\n",
        "                    metrics[i][:] = res\n",
        "                    i+=1\n",
        "\n",
        "                    out = f\"Epoch: {epoch}/{self.tot_epochs-1}, Validation Step: {i+1}/{val_steps}, Validation Loss: {val_loss.item():.4f}, Elapsed time: {time.time() - self.tot_time:.0f} sec \"\n",
        "                    sys.stdout.write(\"\\r\" + out)\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "                avg_metrics = np.mean(metrics,axis=0)\n",
        "\n",
        "                print(f\"val_wt_dice: {np.round(avg_metrics[0],4)}\")\n",
        "                print(f\"val_ct_dice: {np.round(avg_metrics[1],4)}\")\n",
        "                print(f\"val_at_dice: {np.round(avg_metrics[2],4)}\")\n",
        "\n",
        "                mlflow.log_metric('val_wt_dice', np.round(avg_metrics[0],4), step = epoch)\n",
        "                mlflow.log_metric('val_ct_dice', np.round(avg_metrics[1],4), step = epoch)\n",
        "                mlflow.log_metric('val_at_dice', np.round(avg_metrics[2],4), step = epoch)\n",
        "\n",
        "                return val_batch_loss\n",
        "\n",
        "            else:\n",
        "                for i, (data,labels) in enumerate(val_loader):\n",
        "\n",
        "                  #torch.cuda.empty_cache()\n",
        "                  self.optimizer.zero_grad()\n",
        "\n",
        "                  if self.device == 'cuda':\n",
        "                    data = data.type(torch.cuda.FloatTensor)\n",
        "                  else:\n",
        "                    data = data.type(torch.FloatTensor)\n",
        "\n",
        "                  data = data.to(self.device)\n",
        "                  outputs = self.model(data)\n",
        "\n",
        "                  if(self.supervised):\n",
        "                      val_loss = self.loss_fn(outputs, labels)\n",
        "                  else:\n",
        "                      val_loss = self.loss_fn(outputs, data)\n",
        "\n",
        "                  val_batch_loss.append(val_loss.detach().item())\n",
        "\n",
        "                  out = f\"Epoch: {epoch}/{self.tot_epochs-1}, Validation Step: {i+1}/{val_steps}, Validation Loss: {val_loss.item():.4f}, Elapsed time: {time.time() - self.tot_time:.0f} sec \"\n",
        "                  sys.stdout.write(\"\\r\" + out)\n",
        "                  sys.stdout.flush()\n",
        "\n",
        "                torch.cuda.empty_cache()\n",
        "                time.sleep(.5)\n",
        "\n",
        "        return val_batch_loss\n",
        "\n",
        "\n",
        "    def __eval_metrics(self, data_loader:DataLoader):\n",
        "        \"\"\" Evaluates additional metrics aside the loss function \"\"\"\n",
        "        metrics_dict = {}\n",
        "        try:\n",
        "            self.model.eval()\n",
        "            if(self.eval_metrics is not None):\n",
        "\n",
        "                metrics_dict = {k.__class__.__name__:[] for k in self.eval_metrics}\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for i, (data,labels) in enumerate(data_loader):\n",
        "                        torch.cuda.empty_cache()\n",
        "                        self.optimizer.zero_grad()\n",
        "\n",
        "                        for metric in self.eval_metrics:\n",
        "\n",
        "                            if self.device == 'cuda':\n",
        "                                data = data.type(torch.cuda.FloatTensor)\n",
        "                            else:\n",
        "                                data = data.type(torch.FloatTensor)\n",
        "\n",
        "                            data = data.to(self.device)\n",
        "                            outputs = self.model(data)\n",
        "\n",
        "                            if(self.supervised):\n",
        "                                metric_value = metric(outputs, labels)\n",
        "                            else:\n",
        "                                metric_value = metric(outputs, data)\n",
        "\n",
        "                            metrics_dict[metric.__class__.__name__].append(metric_value.detach().item())\n",
        "\n",
        "                    for k in metrics_dict.keys():\n",
        "                        metrics_dict[k] = np.array(metrics_dict[k]).mean()\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Exception thrown in wrapper.__eval_metrics')\n",
        "            print(e)\n",
        "        finally:\n",
        "            return metrics_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict_graph(self, dataset:ImageGraphDataset, save_preds = True):\n",
        "\n",
        "        self.model.eval()\n",
        "        start = time.time()\n",
        "\n",
        "        predictions = {}\n",
        "        metrics = np.zeros([len(dataset), 3])\n",
        "\n",
        "        if(dataset.read_label):\n",
        "\n",
        "            i = 0\n",
        "\n",
        "            for curr_id,graph,feats,labels in dataset:\n",
        "\n",
        "                graph = graph.to(self.device)\n",
        "                feats = torch.FloatTensor(feats).to(self.device)\n",
        "                labels = torch.LongTensor(labels).to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    logits = self.model(graph,feats)\n",
        "\n",
        "                _, predicted_classes = torch.max(logits, dim=1)\n",
        "                #predicted_classes = logits.argmax(1)\n",
        "                res = evaluation.calculate_node_dices(predicted_classes.detach().cpu().numpy(),\n",
        "                                                          labels.detach().cpu().numpy())\n",
        "                metrics[i] = res\n",
        "                predictions[curr_id] = predicted_classes\n",
        "\n",
        "                i += 1\n",
        "\n",
        "                if(save_preds):\n",
        "                    dataset.save_voxel_preds(curr_id, predicted_classes)\n",
        "\n",
        "                out = f\"Step {i}/{len(dataset)+1} Elapsed time: {time.time() - start:.0f} sec \"\n",
        "                sys.stdout.write(\"\\r\" + out)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            avg_metrics = np.mean(metrics,axis=0)\n",
        "            return predictions, avg_metrics\n",
        "\n",
        "        else:\n",
        "            i = 0\n",
        "            for curr_id,graph,feats in dataset:\n",
        "\n",
        "                graph = graph.to(self.device)\n",
        "                feats = torch.FloatTensor(feats).to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    logits = self.model(graph,feats)\n",
        "\n",
        "                #predicted_classes = logits.argmax(1).detach().cpu().numpy()\n",
        "                _, predicted_classes = torch.max(logits, dim=1)\n",
        "                predictions[curr_id] = predicted_classes\n",
        "\n",
        "                if(save_preds):\n",
        "                    dataset.save_voxel_preds(curr_id, predicted_classes )\n",
        "\n",
        "                i += 1\n",
        "                out = f\"Step {i}/{len(dataset)+1} Elapsed time: {time.time() - start:.0f} sec \"\n",
        "                sys.stdout.write(\"\\r\" + out)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            return predictions\n",
        "\n",
        "\n",
        "\n",
        "    def predict_batch(self, data_loader:DataLoader):\n",
        "\n",
        "        output = []\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "              for i, batch in enumerate(data_loader):\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                if(len(batch) == 1):\n",
        "                  data = batch\n",
        "                else:\n",
        "                  data,label = batch\n",
        "\n",
        "                if self.device == 'cuda':\n",
        "                  data = data.type(torch.cuda.FloatTensor)\n",
        "                else:\n",
        "                  data = data.type(torch.FloatTensor)\n",
        "\n",
        "                data.to(self.device)\n",
        "\n",
        "                out = self.model(data)\n",
        "                out = out.cpu().detach().numpy()\n",
        "                output.append(out)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Exception thrown in class {self.model.__class__.__name__ }, method predict_batch:\")\n",
        "                print(e)\n",
        "                print('\\n')\n",
        "\n",
        "        return np.array(output)\n",
        "\n",
        "\n",
        "    def predict(self, data):\n",
        "\n",
        "        if device == 'cuda':\n",
        "          data = data.type(torch.cuda.FloatTensor)\n",
        "        else:\n",
        "          data = data.type(torch.FloatTensor)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            data.to(device)\n",
        "            output = self.model(data)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he-C8aZXQkry"
      },
      "source": [
        "## Loss 🕳"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pUxP6_HiQiP8"
      },
      "outputs": [],
      "source": [
        "class Loss(nn.Module):\n",
        "    def __init__(self, focal):\n",
        "        super(Loss, self).__init__()\n",
        "        if focal:\n",
        "            self.loss_fn = DiceFocalLoss(\n",
        "                include_background=False, softmax=True, to_onehot_y=True, batch=True, gamma=2.0\n",
        "            )\n",
        "        else:\n",
        "            self.loss_fn = DiceCELoss(include_background=False, softmax=True, to_onehot_y=True, batch=True)\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        return self.loss_fn(y_pred, y_true)\n",
        "\n",
        "\n",
        "class LossBraTS(nn.Module):\n",
        "    def __init__(self, focal):\n",
        "        super(LossBraTS, self).__init__()\n",
        "        self.dice = DiceLoss(sigmoid=True, batch=True)\n",
        "        self.ce = FocalLoss(gamma=2.0, to_onehot_y=False) if focal else nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def _loss(self, p, y):\n",
        "\n",
        "        return self.dice(p, y) + self.ce(p, y.float())\n",
        "\n",
        "    def forward(self, p, y):\n",
        "        print('p '+str(p.size()))\n",
        "        print('y '+str(y.size()))\n",
        "        y_wt, y_tc, y_et = y > 0, ((y == 1) + (y == 3)) > 0, y == 3\n",
        "        p_wt, p_tc, p_et = p[:, 0].unsqueeze(1), p[:, 1].unsqueeze(1), p[:, 2].unsqueeze(1)\n",
        "        l_wt, l_tc, l_et = self._loss(p_wt, y_wt), self._loss(p_tc, y_tc), self._loss(p_et, y_et)\n",
        "        return l_wt + l_tc + l_et"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsRN0nOwDif5"
      },
      "source": [
        "## Build dataset 🏗"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9m9XrfCJDqWy"
      },
      "outputs": [],
      "source": [
        "dotenv_path = \"/content/drive/MyDrive/Lorusso/BraTS/.env\"\n",
        "load_dotenv(dotenv_path)\n",
        "\n",
        "\n",
        "INPUT_PATH = os.getenv(\"INPUT_DATA_DIR\")\n",
        "PROCESSED_PATH = os.getenv('PROCESSED_DATA_DIR')\n",
        "INPUT_PATH_PARENT = '/'.join(INPUT_PATH.split('/')[:-1])\n",
        "\n",
        "TRAIN_PATH = os.path.join(INPUT_PATH_PARENT,'train')\n",
        "VAL_PATH = os.path.join(INPUT_PATH_PARENT,'val')\n",
        "TEST_PATH = os.path.join(INPUT_PATH_PARENT,'test')\n",
        "\n",
        "TAR_PATH = os.path.join(INPUT_PATH_PARENT,'BraTS2021_Training_Data.tar')\n",
        "BUILD_DATASET = False\n",
        "\n",
        "if(BUILD_DATASET):\n",
        "  untar_brats(tar_path = '/content/drive/MyDrive/Lorusso/BraTS/data/raw/BraTS2021_Training_Data.tar', extract_path = INPUT_PATH )\n",
        "  #dataset = DataPreprocessor()\n",
        "  #dataset.split_dataset()\n",
        "\n",
        "#dataset = DataPreprocessor()\n",
        "#train_loader = DataLoader(dataset, sampler = SeqSampler(dataset), batch_size = 1, num_workers = 0)\n",
        "#images, labels= next(iter(train_loader))\n",
        "#plot_brain_sections([images[0], labels[0]])\n",
        "#del images, labels, train_loader, dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59tr9TkL0zF3"
      },
      "source": [
        "# Generate Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZkZZCxM101ZL"
      },
      "outputs": [],
      "source": [
        "#dataset = DataPreprocessor(INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/raw/BraTS2023-Test', label_extension = None,  mri_prefix = 'BraTS-GLI-', modality_extensions = ['-t1c.nii.gz', '-t1n.nii.gz', '-t2f.nii.gz', '-t2w.nii.gz'], force_conversion = False)\n",
        "\n",
        "IMG2GRAPH = False\n",
        "if(IMG2GRAPH):\n",
        "    dataset = DataPreprocessor(INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/raw/brats', force_conversion = True)\n",
        "    #dataset = DataPreprocessor(INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled', force_conversion = True)\n",
        "    print(len(set(dataset.get_status_ids()['Finished'])))\n",
        "    if(not set(dataset.get_status_ids()['Finished']).issuperset(set(dataset.all_ids))):\n",
        "        ids_to_convert = list(set(dataset.all_ids).difference(set(dataset.get_status_ids()['Finished'])))\n",
        "        print('MRIs to convert:'+ str(len(ids_to_convert)))\n",
        "        dataset.all_ids = ids_to_convert\n",
        "        print(f\"Pending conversions: {len(set(dataset.get_status_ids()['Pending']))}\")\n",
        "        dataset.run()\n",
        "    dataset = DataPreprocessor(INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/processed_15000_0.5_0/brats')\n",
        "    dataset.split_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz-6k-tLLDbh"
      },
      "source": [
        "# Train and predict ⌛"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dle5EmF9LXms"
      },
      "source": [
        "## Autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UZ5ItOcISTzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a608974e-dad5-4a1e-f7bd-1dc83052fd65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed epochs: 3\n"
          ]
        }
      ],
      "source": [
        "TRAIN_MODEL = False\n",
        "LOAD_MODEL = True\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "time.sleep(5)\n",
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "  num_workers = 0\n",
        "  batch_size = 1\n",
        "  num_epochs = 1\n",
        "  lr = 0.005\n",
        "  supervised = False\n",
        "\n",
        "  # MONAI AUTOENCODER\n",
        "  autoencoder = AutoEncoder(\n",
        "         spatial_dims=3,\n",
        "         kernel_size = 3,\n",
        "         up_kernel_size = 3,\n",
        "         in_channels=4,\n",
        "         out_channels=4,\n",
        "         channels=(5,),\n",
        "         strides=(2,),\n",
        "         inter_channels=(8, 8, 16),\n",
        "         inter_dilations=(1, 2, 4),\n",
        "         num_inter_units=2\n",
        "     )\n",
        "\n",
        "  optimizer = torch.optim.Adam(autoencoder.parameters(), lr=lr,weight_decay=1e-10)\n",
        "  loss_fn = nn.MSELoss() #SSIMLoss(spatial_dims=3)\n",
        "\n",
        "  wrapper_autoencoder = ModelWrapper(model = autoencoder,\n",
        "                            loss_fn = loss_fn,\n",
        "                            optimizer = optimizer,\n",
        "                            supervised = supervised,\n",
        "                            num_epochs = num_epochs,\n",
        "                            LOAD_MODEL = LOAD_MODEL\n",
        "                        )\n",
        "\n",
        "  dataset = DataPreprocessor(INPUT_PATH = INPUT_PATH)\n",
        "\n",
        "  # Split dataset if it's not\n",
        "  if(not os.path.exists(TRAIN_PATH)):\n",
        "    dataset.split_dataset()\n",
        "\n",
        "\n",
        "  train_dataset = DataPreprocessor(INPUT_PATH = TRAIN_PATH)\n",
        "  val_dataset = DataPreprocessor(INPUT_PATH = VAL_PATH)\n",
        "  test_dataset = DataPreprocessor(INPUT_PATH = TEST_PATH)\n",
        "\n",
        "  train_loader = DataLoader(dataset = train_dataset,\n",
        "                           sampler = SeqSampler(train_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "  val_loader = DataLoader(dataset = val_dataset,\n",
        "                           sampler = SeqSampler(val_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "  test_loader = DataLoader(dataset = test_dataset,\n",
        "                           sampler = SeqSampler(test_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "\n",
        "  print('Elapsed epochs: ' + str(wrapper_autoencoder.elapsed_epochs))\n",
        "\n",
        "if(TRAIN_MODEL):\n",
        "      training_loss, validation_loss = wrapper_autoencoder.train(train_loader = train_loader, val_loader = val_loader )\n",
        "      torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Bo0aQf08rHvn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7b99421e-e558-4eb3-fbc5-361e0e283675"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "  xx = range(1,wrapper_autoencoder.elapsed_epochs+1)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(xx, wrapper_autoencoder.training_loss, '-o', label = 'Train')\n",
        "  plt.plot(xx, wrapper_autoencoder.validation_loss,'-o', label = 'Val')\n",
        "  plt.legend(loc='lower left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DvvSdXEvIw5w"
      },
      "outputs": [],
      "source": [
        "#if(TRAIN_MODEL or LOAD_MODEL):\n",
        "if(False):\n",
        "\n",
        "  slice_index = 100\n",
        "  ax = 0\n",
        "\n",
        "  im_test, lab = next(iter(val_loader))\n",
        "  output = wrapper.predict(data = im_test)\n",
        "  out = output[0]\n",
        "  out_numpy = out.cpu().detach().numpy()\n",
        "  im_test_numpy = im_test.cpu().detach().numpy()\n",
        "  print(np.shape(im_test_numpy))\n",
        "\n",
        "  plot_reconstruction(im_orig = np.sum(im_test_numpy[0], axis=0), im_rec = np.sum(out_numpy, axis = 0), ax = ax, slice_index = slice_index)\n",
        "  plot_brain_sections([im_test[0], lab[0]], ax = ax, slice_index = slice_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ETwFBvf1Zjyp"
      },
      "outputs": [],
      "source": [
        "#if(TRAIN_MODEL or LOAD_MODEL):\n",
        "if(False):\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(ker[0], allkernels=True)\n",
        "\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(torch.sum(ker, dim=(0)), allkernels=True)\n",
        "\n",
        "  kk = torch.sum(ker, dim= 0)\n",
        "  kk = torch.sum(kk, dim = 0)\n",
        "  plt.imshow(kk[0,:,:].cpu().detach().numpy())\n",
        "\n",
        "  # Sum over the channels\n",
        "  #ker =ker.sum(axis=(0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filters extraction"
      ],
      "metadata": {
        "id": "Lndh0zBN3dGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bottleneck = autoencoder.decode[0][0][0].weight.detach().clone()\n",
        "sum_bottleneck = bottleneck.sum(axis=(0,1)).flatten().cpu()\n",
        "A,S,Vt = np.linalg.svd(sum_bottleneck.reshape(3,3,3))\n",
        "\n",
        "quantile = get_quantiles(np.copy(sum_bottleneck))[-1]\n",
        "filter_mean = sum_bottleneck.mean()\n",
        "global_features = []\n",
        "for el in S:\n",
        "    global_features.append(el[0])\n",
        "global_features.append(quantile)\n",
        "global_features.append(filter_mean.detach().clone().item())\n",
        "global_features = np.array(global_features, dtype=np.float32)"
      ],
      "metadata": {
        "id": "7Hx_jatYoMjM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNn9uFyniXlr"
      },
      "source": [
        "## GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xtS7HG4ciZi5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3bd0cc-8377-400b-e8cb-2d8a0c7c844d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1001 MRIs\n",
            "Found 125 MRIs\n",
            "Found 125 MRIs\n",
            "Elapsed epochs: 0\n",
            "Epoch: 1/6, Step: 168/167, Training Loss: 0.7044, Elapsed time: 1869 sec train_wt_dice: 0.4814\n",
            "train_ct_dice: 0.3425\n",
            "train_at_dice: 0.2954\n",
            "Epoch: 1/6, Loss: 2.1489, Epoch elapsed time: 1869 sec \n",
            "\n",
            "Epoch: 1/6, Validation Step: 22/21, Validation Loss: 0.6687, Elapsed time: 2020 sec val_wt_dice: 0.4717\n",
            "val_ct_dice: 0.3956\n",
            "val_at_dice: 0.3293\n",
            "Epoch: 1/6, Validation Loss: 0.5958, Elapsed time: 2021 sec\n",
            "\n",
            "Epoch: 2/6, Step: 168/167, Training Loss: 0.6698, Elapsed time: 3223 sec train_wt_dice: 0.5927\n",
            "train_ct_dice: 0.5063\n",
            "train_at_dice: 0.4801\n",
            "Epoch: 2/6, Loss: 0.5709, Epoch elapsed time: 1203 sec \n",
            "\n",
            "Epoch: 2/6, Validation Step: 22/21, Validation Loss: 0.3911, Elapsed time: 3372 sec val_wt_dice: 0.5868\n",
            "val_ct_dice: 0.6611\n",
            "val_at_dice: 0.611\n",
            "Epoch: 2/6, Validation Loss: 0.4919, Elapsed time: 1352 sec\n",
            "\n",
            "Epoch: 3/6, Step: 168/167, Training Loss: 0.6430, Elapsed time: 4579 sec train_wt_dice: 0.6222\n",
            "train_ct_dice: 0.5623\n",
            "train_at_dice: 0.5427\n",
            "Epoch: 3/6, Loss: 0.5032, Epoch elapsed time: 1206 sec \n",
            "\n",
            "Epoch: 3/6, Validation Step: 22/21, Validation Loss: 0.3137, Elapsed time: 4729 sec val_wt_dice: 0.6253\n",
            "val_ct_dice: 0.7184\n",
            "val_at_dice: 0.6822\n",
            "Epoch: 3/6, Validation Loss: 0.4624, Elapsed time: 1357 sec\n",
            "\n",
            "Epoch: 4/6, Step: 168/167, Training Loss: 0.6117, Elapsed time: 5969 sec train_wt_dice: 0.643\n",
            "train_ct_dice: 0.6006\n",
            "train_at_dice: 0.582\n",
            "Epoch: 4/6, Loss: 0.4565, Epoch elapsed time: 1239 sec \n",
            "\n",
            "Epoch: 4/6, Validation Step: 22/21, Validation Loss: 0.2686, Elapsed time: 6124 sec val_wt_dice: 0.6546\n",
            "val_ct_dice: 0.7537\n",
            "val_at_dice: 0.707\n",
            "Epoch: 4/6, Validation Loss: 0.4404, Elapsed time: 1395 sec\n",
            "\n",
            "Epoch: 5/6, Step: 168/167, Training Loss: 0.5264, Elapsed time: 7372 sec train_wt_dice: 0.6633\n",
            "train_ct_dice: 0.6297\n",
            "train_at_dice: 0.6194\n",
            "Epoch: 5/6, Loss: 0.4196, Epoch elapsed time: 1248 sec \n",
            "\n",
            "Epoch: 5/6, Validation Step: 22/21, Validation Loss: 0.2490, Elapsed time: 7526 sec val_wt_dice: 0.6775\n",
            "val_ct_dice: 0.7734\n",
            "val_at_dice: 0.7291\n",
            "Epoch: 5/6, Validation Loss: 0.4342, Elapsed time: 1402 sec\n",
            "\n",
            "Epoch: 6/6, Step: 168/167, Training Loss: 0.5431, Elapsed time: 8771 sec train_wt_dice: 0.6781\n",
            "train_ct_dice: 0.6517\n",
            "train_at_dice: 0.6392\n",
            "Epoch: 6/6, Loss: 0.3951, Epoch elapsed time: 1244 sec \n",
            "\n",
            "Epoch: 6/6, Validation Step: 22/21, Validation Loss: 0.2282, Elapsed time: 8925 sec val_wt_dice: 0.7114\n",
            "val_ct_dice: 0.7799\n",
            "val_at_dice: 0.7437\n",
            "Epoch: 6/6, Validation Loss: 0.4369, Elapsed time: 1399 sec\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/01/18 15:01:27 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.0+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.1.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training time: 8926 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/01/18 15:01:42 WARNING mlflow.utils.requirements_utils: Found dgl version (2.0.0+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'dgl==2.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2024/01/18 15:01:42 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.0+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.1.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/processed_15000_0.5_0'\n",
        "\n",
        "TRAIN_PATH = os.path.join(INPUT_PATH,'train')\n",
        "VAL_PATH = os.path.join(INPUT_PATH,'val')\n",
        "TEST_PATH = os.path.join(INPUT_PATH,'test') # set to 'test' in production\n",
        "\n",
        "\n",
        "train_dataset = ImageGraphDataset(TRAIN_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True, features=global_features)\n",
        "val_dataset = ImageGraphDataset(VAL_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True, features=global_features)\n",
        "test_dataset = ImageGraphDataset(TEST_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True, features=global_features)\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_MODEL = True\n",
        "LOAD_MODEL = False # resume training\n",
        "\n",
        "supervised = True\n",
        "eval_metrics = None\n",
        "num_workers = 0\n",
        "\n",
        "batch_size = 6\n",
        "num_epochs = 6\n",
        "lr = 5e-4\n",
        "dropout = 0.1\n",
        "input_feats = 25\n",
        "class_weights = compute_average_weights(val_dataset) # torch.Tensor([0.2,1,2,3]).to(device)\n",
        "layer_sizes=[512]*4\n",
        "n_classes=4\n",
        "aggregator_type='pool'\n",
        "\n",
        "heads = [128]*3\n",
        "residuals = [128]*3\n",
        "heads = [8, 8, 8, 8, 8, 8]\n",
        "residuals = [False, True, True, False, True, True]\n",
        "activation = F.relu\n",
        "val_dropout = 0.02\n",
        "val_feat_drop = 0.2\n",
        "val_attn_drop = 0.2\n",
        "\n",
        "\n",
        "dict_params = {k:eval(k) for k in ['dropout','input_feats', 'class_weights', 'layer_sizes', 'n_classes', 'aggregator_type', 'n_classes', 'heads', 'residuals']}\n",
        "model = GraphSage(in_feats=input_feats,\n",
        "                  layer_sizes=layer_sizes,\n",
        "                  n_classes=n_classes,\n",
        "                  aggregator_type=aggregator_type,\n",
        "                  dropout=dropout)\n",
        "\n",
        "#model = GAT(in_feats = input_feats, layer_sizes = layer_sizes, n_classes=n_classes,heads = heads, residuals = residuals, activation = activation,  feat_drop = val_feat_drop, attn_drop = val_attn_drop)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=lr,weight_decay=1e-4)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "#loss_fn = LossBraTS(focal=False)\n",
        "\n",
        "wrapper = ModelWrapper(model = model,\n",
        "                            loss_fn = loss_fn,\n",
        "                            optimizer = optimizer,\n",
        "                            supervised = supervised,\n",
        "                            dict_params = dict_params,\n",
        "                            isgnn = True,\n",
        "                            num_epochs = num_epochs,\n",
        "                            LOAD_MODEL = LOAD_MODEL,\n",
        "                            eval_metrics = eval_metrics\n",
        "                          )\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset = train_dataset,\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers,\n",
        "                              collate_fn=minibatch_graphs)\n",
        "\n",
        "val_loader = DataLoader(dataset = val_dataset,\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers,\n",
        "                              collate_fn=minibatch_graphs)\n",
        "\n",
        "\n",
        "\n",
        "print('Elapsed epochs: ' + str(wrapper.elapsed_epochs))\n",
        "\n",
        "if(TRAIN_MODEL):\n",
        "    training_loss, validation_loss = wrapper.train(train_loader = train_loader, val_loader = val_loader )\n",
        "    #torch.cuda.empty_cache()\n",
        "\n",
        "if(LOAD_MODEL):\n",
        "  xx_train = range(len(wrapper.training_loss))\n",
        "  xx_val = range(len(wrapper.validation_loss))\n",
        "  plt.title('Loss curve')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(xx_train, wrapper.training_loss, '-o', label = 'Train')\n",
        "  plt.plot(xx_val, wrapper.validation_loss,'-o', label = 'Val')\n",
        "  plt.legend(loc='lower left')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testdata2023 = ImageGraphDataset('/content/drive/MyDrive/Lorusso/BraTS/data/processed_15000_0.5_0/BraTS2023-ValidationData','BraTS-GLI-',read_image=False,read_graph=True,read_label=False, features=global_features)\n",
        "#predictions = wrapper.predict_graph(testdata2023)"
      ],
      "metadata": {
        "id": "wVE18k6M5O3b"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seg_to_challenge():\n",
        "    parent = '/content/drive/MyDrive/Lorusso/BraTS/data/processed_15000_0.5_0/BraTS2023-ValidationData'\n",
        "    dst =  '/content/drive/MyDrive/Lorusso/BraTS/data/segmentation/BraTS2023-ValidationData/'\n",
        "\n",
        "    for mri_id in os.listdir(parent):\n",
        "        for el in os.listdir(os.path.join(parent, mri_id)):\n",
        "            if(not (el.endswith('_input.nii.gz') or el.endswith('.json') or el.endswith('_supervoxels.nii.gz') or el.endswith('.pkl')) ):\n",
        "                shutil.copy2(os.path.join(parent,mri_id,el), dst)\n",
        "                os.rename(os.path.join(dst,el), f\"{dst}{os.sep}{mri_id}.nii.gz\")"
      ],
      "metadata": {
        "id": "P5lTKZhKNgMh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seg_to_challenge()"
      ],
      "metadata": {
        "id": "UmwsYRTTzOBn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_J8e-G2cJQxr"
      },
      "outputs": [],
      "source": [
        "def rm_run(run_id, experiment = '134912204135972352',):\n",
        "    rm_path = '/content/drive/MyDrive/Lorusso/BraTS/mlruns/'+experiment+'/'+run_id\n",
        "    shutil.rmtree(rm_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1_TRMmsm6jY"
      },
      "source": [
        "# TESTING SECTION 🚧"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bC-ortIQ3vXX"
      },
      "outputs": [],
      "source": [
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled\n",
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/train\n",
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/test\n",
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/val\n",
        "\n",
        "\n",
        "\n",
        "#trainset = set()\n",
        "#valset = set()\n",
        "#testset = set()\n",
        "#for el in dp.get_status_ids()['Pending']:\n",
        "#    if(el in train_dataset.all_ids):\n",
        "#        trainset.add(el)\n",
        "#    if(el in val_dataset.all_ids):\n",
        "#        valset.add(el)\n",
        "#    if(el in test_dataset.all_ids):\n",
        "#        testset.add(el)\n",
        "#\n",
        "#\n",
        "#for el in valset:\n",
        "#    src = VAL_PATH+'/'+str(el)\n",
        "#    dst = '/content/drive/MyDrive/Lorusso/BraTS/data/processed_5000_0.5_10/brats/'+el\n",
        "#    shutil.copytree(src, dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJUctzmLazd9"
      },
      "source": [
        "## Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eHHzhDDF_3G9"
      },
      "outputs": [],
      "source": [
        "\n",
        "TEST_MODE = False\n",
        "\n",
        "\n",
        "if(TEST_MODE):\n",
        "\n",
        "    INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled'\n",
        "    INPUT_PATH_PARENT = '/'.join(INPUT_PATH.split('/')[:-1])\n",
        "    TRAIN_PATH = os.path.join(INPUT_PATH_PARENT,'train')\n",
        "    VAL_PATH = os.path.join(INPUT_PATH_PARENT,'val')\n",
        "    TEST_PATH = os.path.join(INPUT_PATH_PARENT,'test')\n",
        "\n",
        "    TRAIN_MODEL = False\n",
        "    LOAD_MODEL = True # resume training\n",
        "\n",
        "    num_workers = 0\n",
        "    batch_size = 1\n",
        "    num_epochs = 2\n",
        "    lr = 0.01\n",
        "    supervised = False\n",
        "    eval_metrics = [nn.MSELoss()]\n",
        "\n",
        "  # MONAI AUTOENCODER\n",
        "    model = AutoEncoder(\n",
        "           spatial_dims=3,\n",
        "           kernel_size = 3,\n",
        "           up_kernel_size = 3,\n",
        "           in_channels=4,\n",
        "           out_channels=4,\n",
        "           channels=(5,),\n",
        "           strides=(2,),\n",
        "           inter_channels=(8, 16, 32),\n",
        "           inter_dilations=(1, 2, 4),\n",
        "           num_inter_units=2\n",
        "       )\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-10)\n",
        "    loss_fn = nn.MSELoss() #SSIMLoss(spatial_dims=3)\n",
        "\n",
        "    wrapper = ModelWrapper(model = model,\n",
        "                              loss_fn = loss_fn,\n",
        "                              optimizer = optimizer,\n",
        "                              supervised = supervised,\n",
        "                              num_epochs = num_epochs,\n",
        "                              LOAD_MODEL = LOAD_MODEL,\n",
        "                              eval_metrics = eval_metrics\n",
        "                          )\n",
        "\n",
        "    dataset = DataPreprocessor(INPUT_PATH = INPUT_PATH)\n",
        "\n",
        "    # Split dataset if it's not\n",
        "    if(not os.path.exists(TRAIN_PATH)):\n",
        "      dataset.split_dataset()\n",
        "\n",
        "\n",
        "    train_dataset = DataPreprocessor(INPUT_PATH = TRAIN_PATH)\n",
        "    val_dataset = DataPreprocessor(INPUT_PATH = VAL_PATH)\n",
        "    test_dataset = DataPreprocessor(INPUT_PATH = TEST_PATH)\n",
        "\n",
        "    train_loader = DataLoader(dataset = train_dataset,\n",
        "                             sampler = SeqSampler(train_dataset),\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers)\n",
        "\n",
        "    val_loader = DataLoader(dataset = val_dataset,\n",
        "                             sampler = SeqSampler(val_dataset),\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers)\n",
        "\n",
        "    test_loader = DataLoader(dataset = test_dataset,\n",
        "                             sampler = SeqSampler(test_dataset),\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers)\n",
        "\n",
        "\n",
        "    print('Elapsed epochs: ' + str(wrapper.elapsed_epochs))\n",
        "\n",
        "    if(TRAIN_MODEL):\n",
        "        training_loss, validation_loss = wrapper.train(train_loader = train_loader, val_loader = val_loader, experiment_prefix = 'Test' )\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CcuxwsmW9OYS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cc8b96c6-07b6-48f1-ff1d-402fdf9527c9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "  xx = range(1,wrapper.elapsed_epochs+1)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(xx, wrapper.training_loss, '-o', label = 'Train')\n",
        "  plt.plot(xx, wrapper.validation_loss,'-o', label = 'Val')\n",
        "  plt.legend(loc='lower left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "V3TKi4A9O4NA"
      },
      "outputs": [],
      "source": [
        "if(False):\n",
        "\n",
        "  slice_index = 90\n",
        "  ax = 0\n",
        "\n",
        "  im_test, lab = next(iter(test_loader))\n",
        "  output = wrapper.predict(data = im_test)\n",
        "  out = output[0]\n",
        "  out_numpy = out.cpu().detach().numpy()\n",
        "  im_test_numpy = im_test.cpu().detach().numpy()\n",
        "\n",
        "  plot_reconstruction(im_orig = np.sum(im_test_numpy[0], axis=0), im_rec = np.sum(out_numpy, axis = 0), ax = ax, slice_index = slice_index)\n",
        "  plot_brain_sections([im_test[0], lab[0]], ax = ax, slice_index = slice_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "s5fwjWaWOhXN"
      },
      "outputs": [],
      "source": [
        "if(False):\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(ker[1], allkernels=True)\n",
        "\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(torch.sum(ker, dim=(0)), allkernels=True)\n",
        "\n",
        "  kk = torch.sum(ker, dim= 0)\n",
        "  kk = torch.sum(kk, dim = 0)\n",
        "  plt.imshow(kk[0,:,:].cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGPCqWQcrPqb"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-lpiQp3XNxn"
      },
      "source": [
        "## GNN training ⛽"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wVWsu1gEXV9j"
      },
      "outputs": [],
      "source": [
        "\n",
        "TEST_MODE = False\n",
        "\n",
        "\n",
        "if(TEST_MODE):\n",
        "\n",
        "    INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/processed_5000_0.5_10'\n",
        "    TRAIN_PATH = os.path.join(INPUT_PATH,'train')\n",
        "    VAL_PATH = os.path.join(INPUT_PATH,'val')\n",
        "    TEST_PATH = os.path.join(INPUT_PATH,'val') # set again to 'test' in production\n",
        "\n",
        "    TRAIN_MODEL = True\n",
        "    LOAD_MODEL = False # resume training\n",
        "\n",
        "    num_workers = 0\n",
        "    batch_size = 1\n",
        "    num_epochs = 2\n",
        "    lr = 0.005\n",
        "    supervised = True\n",
        "    eval_metrics = []\n",
        "\n",
        "    dropout = 0\n",
        "    input_feats = 20\n",
        "    class_weights = torch.Tensor([0.1,1,2,2])\n",
        "    layer_sizes=[256]*4\n",
        "    n_classes=4\n",
        "    aggregator_type='pool'\n",
        "\n",
        "    dict_params = {k:eval(k) for k in ['input_feats', 'class_weights', 'layer_sizes', 'n_classes', 'aggregator_type', 'n_classes']}\n",
        "\n",
        "    model = GraphSage(in_feats=input_feats,layer_sizes=layer_sizes,n_classes=n_classes,aggregator_type=aggregator_type,dropout=dropout)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(),lr=lr,weight_decay=1e-10)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    wrapper = ModelWrapper(model = model,\n",
        "                            loss_fn = loss_fn,\n",
        "                            optimizer = optimizer,\n",
        "                            supervised = supervised,\n",
        "                            dict_params = dict_params,\n",
        "                            isgnn = True,\n",
        "                            num_epochs = num_epochs,\n",
        "                            LOAD_MODEL = LOAD_MODEL,\n",
        "                            eval_metrics = eval_metrics\n",
        "                          )\n",
        "\n",
        "\n",
        "    train_dataset = ImageGraphDataset(VAL_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True)\n",
        "    #val_dataset = ImageGraphDataset(VAL_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True)\n",
        "    #test_dataset = ImageGraphDataset(TEST_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True)\n",
        "\n",
        "    train_loader = DataLoader(dataset = train_dataset,\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers,\n",
        "                              collate_fn=minibatch_graphs)\n",
        "\n",
        "\n",
        "    print('Elapsed epochs: ' + str(wrapper.elapsed_epochs))\n",
        "\n",
        "    if(TRAIN_MODEL):\n",
        "        training_loss, validation_loss = wrapper.train(train_loader = train_loader, val_loader = train_loader, experiment_prefix = 'Test_GNN' )\n",
        "        torch.cuda.empty_cache()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mwFJcAuef07T",
        "q7ygOSsiEAWs",
        "aKEY_j_lCzCa",
        "cgINpL8BK3hL",
        "he-C8aZXQkry",
        "vsRN0nOwDif5",
        "59tr9TkL0zF3",
        "dle5EmF9LXms",
        "n1_TRMmsm6jY"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}