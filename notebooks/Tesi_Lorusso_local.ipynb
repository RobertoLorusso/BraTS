{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub1h-Y7DIpYf"
      },
      "source": [
        "## README ‚ùó\n",
        "\n",
        "Set a manual_seed for reproducibility.\n",
        "\n",
        "References:\n",
        "\n",
        "- See [Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwFJcAuef07T"
      },
      "source": [
        "## Environment setup üèõ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45BU03HjgAgT",
        "outputId": "add018d1-6e46-46c7-839c-5a97eb10f619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "#! pip install python-dotenv\n",
        "#! pip install monai\n",
        "#! pip install shutil\n",
        "#! pip install mlflow --quiet\n",
        "#! pip install pyngrok --quiet\n",
        "#! pip install torchmetrics\n",
        "##! pip install dgl\n",
        "#! pip install  dgl -f https://data.dgl.ai/wheels/cu121/repo.html\n",
        "#! pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html\n",
        "#\n",
        "\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "\n",
        "sys.path.append('/Users/Roberto/Desktop/Universit√†/Magistrale/Tesi/BraTS/')\n",
        "\n",
        "from src.preprocess import evaluation\n",
        "from src.preprocess.image_processing import *\n",
        "from src.preprocess.nifti_io import *\n",
        "from src.preprocess.graphgen import *\n",
        "from src.preprocess.graph_io import *\n",
        "\n",
        "from mlflow.models.signature import infer_signature\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from pyngrok import ngrok\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dotenv import load_dotenv\n",
        "import concurrent.futures\n",
        "import tarfile\n",
        "import nibabel as nib\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "from distutils.dir_util import copy_tree\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import SequentialSampler\n",
        "from torchvision import utils\n",
        "\n",
        "from monai.networks.nets import AutoEncoder\n",
        "from monai.losses import DiceCELoss, DiceFocalLoss, DiceLoss, FocalLoss\n",
        "\n",
        "from dgl import from_networkx as to_dgl_graph\n",
        "from dgl import batch as dgl_batch\n",
        "from dgl import to_networkx\n",
        "\n",
        "#torch.use_deterministic_algorithms(True, warn_only=True)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE7OwGBES7sa"
      },
      "source": [
        "## MLFlow server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "M4-kysoZS-i4"
      },
      "outputs": [],
      "source": [
        "mlflow.set_tracking_uri('file:///Users/Roberto/Desktop/Universit√†/Magistrale/Tesi/BraTS/mlruns')\n",
        "# run tracking UI in the background\n",
        "#get_ipython().system_raw(\"mlflow ui --backend-store-uri file:///Users/Roberto/Desktop/Universit√†/Magistrale/Tesi/BraTS/mlruns --port 5000 & \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7ygOSsiEAWs"
      },
      "source": [
        "## Utils üõ†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Lrs_wCJEAJpS"
      },
      "outputs": [],
      "source": [
        "class Logger:\n",
        "    def __init__(self,filename):\n",
        "        self.filename = filename\n",
        "        try:  #try to open file in 'r' (read) mode, if file does not exists the statement will throw an IOexception\n",
        "            log_file = open(self.filename, \"r\")\n",
        "            log_file.close()\n",
        "        except Exception as e: #catch the Exception raised from the block above and create the missing file in the specified path\n",
        "            log_file = open(self.filename, \"w\")\n",
        "            log_file.close()\n",
        "\n",
        "    def log_msg(self,*args):\n",
        "        try:\n",
        "            with open(self.filename, \"a+\") as log_file:\n",
        "                for el in args:\n",
        "                    if(type(el) is list or type(el) is tuple):\n",
        "                        for subel in el:\n",
        "                            log_file.write(subel)\n",
        "                            log_file.write('\\n')\n",
        "                    else:\n",
        "                        log_file.write(el)\n",
        "                        log_file.write('\\n')\n",
        "        except Exception as e:\n",
        "            print('Exception in Logger.log_msg')\n",
        "            print(e)\n",
        "\n",
        "    def read_msg(self):\n",
        "        content = []\n",
        "        try:\n",
        "            with open(self.filename) as file:\n",
        "                for el in file:\n",
        "                    content.append(el)\n",
        "        except Exception as e:\n",
        "            print('Exception in Logger.read_msg')\n",
        "            print(e)\n",
        "        finally:\n",
        "            return content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def untar_brats(tar_path, extract_path):\n",
        "    tar = tarfile.open(tar_path)\n",
        "    tar.extractall(extract_path)\n",
        "    tar.close()\n",
        "\n",
        "def plot_reconstruction(im_orig, im_rec, ax:int = 0, slice_index:int = 100):\n",
        "\n",
        "    f, ax_array = plt.subplots(1,2, figsize=(10,10))\n",
        "    ax_array[0].imshow(np.take(im_orig, indices = slice_index, axis = ax), cmap='gray')\n",
        "    ax_array[1].imshow(np.take( im_rec , indices=slice_index, axis = ax), cmap='gray')\n",
        "\n",
        "def plot_brain_sections(images,ax = 1,slice_index = 90):\n",
        "\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    d1,d2,d3 = np.shape(images[1])\n",
        "    dims = [d1,d2,d3]\n",
        "    dims.pop(ax)\n",
        "    color_segmentation = np.zeros((dims[0],dims[1],3))\n",
        "\n",
        "    gray_segmentation = np.take(images[1],slice_index,axis = ax)\n",
        "    color_segmentation[gray_segmentation == 1] = [255,0,0] # Red (necrotic tumor core)\n",
        "    color_segmentation[gray_segmentation == 2] = [0,255,0] # Green (peritumoral edematous/invaded tissue)\n",
        "    color_segmentation[gray_segmentation == 4] = [0,0,255] # Blue (enhancing tumor)\n",
        "\n",
        "    t1 = images[0][0]\n",
        "    flair = images[0][1]\n",
        "    t2 = images[0][2]\n",
        "    t1ce = images[0][3]\n",
        "\n",
        "    image = t1+t2+flair+t1ce\n",
        "\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.subplot(1,4,1)\n",
        "    plt.imshow(np.take(image,slice_index,axis = 0),cmap='gray')\n",
        "\n",
        "    plt.subplot(1,4,2)\n",
        "    plt.imshow(np.take(image,slice_index,axis = 1),cmap='gray')\n",
        "\n",
        "    plt.subplot(1,4,3)\n",
        "    plt.imshow(np.take(image,slice_index,axis = 2),cmap='gray')\n",
        "\n",
        "    plt.subplot(1,4,4)\n",
        "    plt.imshow(color_segmentation,cmap='gray')\n",
        "    plt.xlabel('Segmentation')\n",
        "\n",
        "\n",
        "def visTensor(tensor, ch=0, allkernels=False, nrow=8, padding=1):\n",
        "    n,c,w,h = tensor.shape\n",
        "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
        "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
        "    rows = np.min((tensor.shape[0] // nrow + 1, 64))\n",
        "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
        "    plt.figure( figsize=(nrow,rows) )\n",
        "    plt.imshow(grid.cpu().permute((1, 2, 0)))\n",
        "\n",
        "\n",
        "def count_labels(triple_list):\n",
        "    label_counts = {}\n",
        "    labels = [triple[3] for triple in triple_list]\n",
        "\n",
        "    # Concatenate all the arrays into one\n",
        "    all_labels = np.concatenate(labels)\n",
        "\n",
        "    # Count the occurrences of each label\n",
        "    counter = Counter(all_labels)\n",
        "\n",
        "    # Create a dict with the counts for labels 1 to 4\n",
        "    counts_dict = {i: counter[i] for i in counter.keys()}\n",
        "    return counts_dict\n",
        "\n",
        "\n",
        "def class_weights_tensor(label_weights):\n",
        "    num_classes = max(label_weights.keys())+1\n",
        "    weight_tensor = torch.zeros(num_classes, dtype=torch.float32)\n",
        "    # Sort the dictionary by keys (labels)\n",
        "    sorted_label_weights = sorted(label_weights.items(), key=lambda x: x[0])\n",
        "    for label, weight in sorted_label_weights:\n",
        "        weight_tensor[label] = weight  # Subtract 1 from label if your labels start from 1\n",
        "    return weight_tensor\n",
        "\n",
        "\n",
        "def compute_average_weights(graphs):\n",
        "    label_counts = count_labels(graphs)\n",
        "    total_count = sum(label_counts.values())\n",
        "    class_weights = {label: total_count / count for label, count in label_counts.items()}\n",
        "    weight_tensor = class_weights_tensor(class_weights)\n",
        "    return weight_tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKEY_j_lCzCa"
      },
      "source": [
        "## Dataset class  üíæ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-Pa_KNzpRi0d"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Preprocessing script to convert from data provided by BraTS to data used by our model. Should be the first thing you run.\n",
        "Fulfills the following four functions:\n",
        "1. Normalize and standardize each image of each MRI modality\n",
        "2. Combine multiple MRI modalitities into one image array\n",
        "3. Swap labels from BraTS order (0,2,1,4) to more intuitive order (0,1,2,3)\n",
        "4. Convert image into a graph\n",
        "    Using Simple Linear Iterative Clustering algorithm\n",
        "    Parameters passed on command line\n",
        "\n",
        "If no labels are present (e.g. at test time, in deployment) can also build graph without labels.\n",
        "\n",
        "Saves the following in the specified output directory for each sample\n",
        "MRI_ID/\n",
        "    _input.nii.gz (processed and combined modalities for a sample as nifti file)\n",
        "    _label.nii.gz\n",
        "    _nxgraph.json (networkx graph containing both graph topography and features and labels for each node)\n",
        "    _supervoxels.nii.gz (supervoxel partitioning produced by SLIC)\n",
        "    _crop.npy (optionally the crop of the processed data relative to the original data) (crops out empty image planes)\n",
        "'''\n",
        "\n",
        "LABEL_MAP = {4: 3, 2: 1, 1: 2}\n",
        "\n",
        "def swap_labels_from_brats(label_data):\n",
        "    uniques = np.unique(label_data)\n",
        "    for u in uniques:\n",
        "        if u not in [0, 1, 2, 4]:\n",
        "            raise RuntimeError('unexpected label')\n",
        "    new_label_data = np.zeros_like(label_data, dtype=np.int16)\n",
        "    new_label_data[label_data == 4] = LABEL_MAP[4]\n",
        "    new_label_data[label_data == 2] = LABEL_MAP[2]\n",
        "    new_label_data[label_data == 1] = LABEL_MAP[1]\n",
        "    return new_label_data\n",
        "\n",
        "def swap_labels_to_brats(label_data):\n",
        "    uniques = np.unique(label_data)\n",
        "    for u in uniques:\n",
        "        if u not in [0, 1, 2, 3]:\n",
        "            raise RuntimeError('unexpected label')\n",
        "    new_label_data = np.zeros_like(label_data, dtype=np.int16)\n",
        "    new_label_data[label_data == LABEL_MAP[4]] = 4\n",
        "    new_label_data[label_data == LABEL_MAP[2]] = 2\n",
        "    new_label_data[label_data == LABEL_MAP[1]] = 1\n",
        "    return new_label_data\n",
        "\n",
        "\n",
        "class DataPreprocessor(Dataset):\n",
        "    def __init__(self, dotenv_path = \"/content/drive/MyDrive/Lorusso/BraTS/.env\", transform:bool = True, INPUT_PATH = None,\n",
        "                 num_nodes = 15000, boxiness_coef = 0.5, num_neighbors = 0, **kwargs):\n",
        "\n",
        "        load_dotenv(dotenv_path)\n",
        "        # Data mean and variance\n",
        "        data_stats = ([0.4645, 0.6625, 0.4064, 0.3648],\n",
        "                      [0.1593, 0.1703, 0.1216, 0.1627])\n",
        "        self.N_THREADS = 8\n",
        "        self.num_nodes = num_nodes\n",
        "        self.boxiness_coef = boxiness_coef\n",
        "        self.num_neighbors = num_neighbors\n",
        "\n",
        "        if(INPUT_PATH is not None and os.path.exists(INPUT_PATH)):\n",
        "            self.data_dir = INPUT_PATH\n",
        "        else:\n",
        "            self.data_dir = os.getenv('INPUT_DATA_DIR')\n",
        "\n",
        "        self.output_dir = os.getenv('PROCESSED_DATA_DIR')\n",
        "        self.graph_dir = f\"{self.output_dir}_{self.num_nodes}_{self.boxiness_coef}_{self.num_neighbors}{os.sep}{os.path.basename(self.data_dir)}\"\n",
        "        self.logger = Logger(filename=os.path.join(os.getenv('LOG_PATH'), os.path.basename(self.data_dir)+'_logs.txt'))\n",
        "\n",
        "        self.mri_prefix = 'BraTS2021_'\n",
        "        self.modality_extensions = [\"_flair.nii.gz\", \"_t1.nii.gz\", \"_t1ce.nii.gz\", \"_t2.nii.gz\"]\n",
        "        self.label_extension = \"_seg.nii.gz\"\n",
        "        self.dataset_mean = np.array(data_stats[0], dtype=np.float32)\n",
        "        self.dataset_std = np.array(data_stats[1], dtype=np.float32)\n",
        "        self.transform = transform\n",
        "        self.force_conversion = False\n",
        "\n",
        "        # Set or overwrite additional attributes\n",
        "        for el in kwargs.keys():\n",
        "            setattr(self,str(el),kwargs[el])\n",
        "\n",
        "        self.include_labels = self.label_extension is not None\n",
        "        self.all_ids, self.id_to_fp = self.get_all_mris_in_dataset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_ids)\n",
        "\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        # DO NOT APPLY PADDING!! See this issue:\n",
        "        # https://github.com/RobertoLorusso/BraTS/issues/10#issue-2065431255\n",
        "        #imstack, labels = self.padding(imstack, labels)\n",
        "\n",
        "        imstack = self.read_in_patient_sample(idx)\n",
        "        if(self.include_labels):\n",
        "            labels = self.read_in_labels(idx)\n",
        "        else:\n",
        "            labels = None\n",
        "        crop_idxs = None\n",
        "\n",
        "        if (self.transform):\n",
        "            imstack,labels,crop_idxs = self.get_standardized_image(imstack, labels)\n",
        "        return imstack, labels, crop_idxs\n",
        "\n",
        "\n",
        "    def read_in_patient_sample(self, mri_id):\n",
        "\n",
        "        scan_dir = self.data_dir + os.sep + mri_id\n",
        "        modality_exts = self.modality_extensions\n",
        "        num_modalities=len(modality_exts)\n",
        "        modality_imgs = []\n",
        "        for root, _, files in os.walk(scan_dir):\n",
        "            for ext in modality_exts:\n",
        "                for filename in files:\n",
        "                    if filename.endswith(ext):\n",
        "                        filepath = os.path.join(root, filename)\n",
        "                        mod_img = nib.load(filepath)\n",
        "                        #data is actually stored as int16\n",
        "                        img_data = np.array(mod_img.dataobj,dtype=np.float32)\n",
        "                        modality_imgs.append(img_data)\n",
        "        #check that all the modalities were present in the folder\n",
        "        assert(len(modality_imgs)==num_modalities)\n",
        "\n",
        "        patient_sample = np.stack(modality_imgs,3) if num_modalities>1 else modality_imgs[0]\n",
        "        return patient_sample\n",
        "\n",
        "\n",
        "    def read_in_labels(self,mri_id):\n",
        "        scan_dir = self.data_dir + os.sep + mri_id\n",
        "        for filename in os.listdir(scan_dir):\n",
        "            if filename.endswith(self.label_extension):\n",
        "                label_nib = nib.load(scan_dir+os.sep+filename)\n",
        "                #potentially also return affine if they are different between images (which they are not for brats)\n",
        "                return np.array(label_nib.dataobj,dtype=np.int16)\n",
        "        raise FileNotFoundError(f\"Label image not found in folder: {scan_dir}\")\n",
        "\n",
        "\n",
        "    def get_all_mris_in_dataset(self):\n",
        "        mri_folders = glob.glob(f\"{self.data_dir}**/{self.mri_prefix}*/\",\n",
        "                                recursive=True)\n",
        "        mri_folders = self.remove_incomplete_mris(mri_folders)\n",
        "        scan_dic = {os.path.split(fp)[0].split(\"/\")[-1]: fp for fp in mri_folders}\n",
        "        if(len(mri_folders) == 0):\n",
        "            print(\"No MRI found at \" + self.data_dir)\n",
        "        return list(scan_dic.keys()), scan_dic\n",
        "\n",
        "\n",
        "    def remove_incomplete_mris(self, mri_folders):\n",
        "        # if there are any you want to ignore just add them to this list\n",
        "        removed_mris = []\n",
        "        return [fp for fp in mri_folders if fp.split(\"/\")[-2] not in removed_mris]\n",
        "\n",
        "    def padding(self,image, labels):\n",
        "        n_channels = np.shape(image)[0]\n",
        "        max_val = max(np.shape(image))\n",
        "        pad_list = np.zeros([n_channels,max_val,max_val,max_val],dtype=np.float32)\n",
        "\n",
        "        for channel in range(0, n_channels): # pad every channel\n",
        "            pad_list[channel] = np.pad(image[channel],[(42,43),(0,0),(0,0)],'constant')\n",
        "        labels = np.pad(labels, [(42,43),(0,0),(0,0)],'constant')\n",
        "\n",
        "        return pad_list, labels\n",
        "\n",
        "\n",
        "    def get_standardized_image(self, image_data, label_data):\n",
        "\n",
        "        #standardized_labels = self.swap_labels_from_brats(label_data)\n",
        "\n",
        "        crop_idxs = self.determine_brain_crop(image_data)\n",
        "        cropped_data = image_data[crop_idxs]\n",
        "        if(self.include_labels):\n",
        "            cropped_labels = label_data[crop_idxs]\n",
        "            standardized_labels= swap_labels_from_brats(cropped_labels)\n",
        "        else:\n",
        "            standardized_labels = None\n",
        "\n",
        "        normalized_data = self.normalize_img_quantile(cropped_data)\n",
        "        standardized_data = self.standardize_img(normalized_data)\n",
        "        return standardized_data,standardized_labels,crop_idxs\n",
        "\n",
        "\n",
        "    def determine_brain_crop(self,multi_modal_data):\n",
        "        if(len(multi_modal_data.shape)==4):\n",
        "            max_intensity_vals = np.amax(multi_modal_data,axis=3)\n",
        "        elif(len(multi_modal_data.shape)==3):\n",
        "            max_intensity_vals = multi_modal_data\n",
        "        else:\n",
        "            raise Exception(f\"Expected input shape of either nxmxr or nxmxrxC. Instead got {multi_modal_data.shape}\")\n",
        "        mask = max_intensity_vals>0.01\n",
        "        ix = np.ix_(mask.any(axis=(1,2)),mask.any(axis=(0,2)),mask.any(axis=(0,1)))\n",
        "\n",
        "        return ix\n",
        "\n",
        "    def normalize_img(self, img_array):\n",
        "        new_image = np.zeros(img_array.shape, dtype=np.float32)\n",
        "        n_channel = img_array.shape[3] # channel-first images\n",
        "\n",
        "        for channel in range(0, n_channel): # normalize every channel\n",
        "\n",
        "            maxval, minval= np.max(img_array[channel]), np.min(img_array[channel])\n",
        "            new_image[channel] = (img_array[channel] - minval)/(maxval-minval)\n",
        "        return new_image\n",
        "\n",
        "\n",
        "    def normalize_img_quantile(self, img_array):\n",
        "        # Exclude the channel axis, for channel-first images is zero\n",
        "        quantile = np.quantile(img_array, 0.995, axis = (0,1,2) ).astype(np.float32)\n",
        "        img_array = img_array/quantile\n",
        "        return img_array\n",
        "\n",
        "\n",
        "\n",
        "    def standardize_img(self,img_array):\n",
        "        centered = img_array-self.dataset_mean\n",
        "        standardized = centered/self.dataset_std\n",
        "        return standardized\n",
        "\n",
        "\n",
        "    def get_status_ids(self):\n",
        "\n",
        "        common_ids = set()\n",
        "        converting_ids = set()\n",
        "        finished_ids = set()\n",
        "\n",
        "        finished = []\n",
        "        pending = []\n",
        "\n",
        "        try:\n",
        "            regex = r'(Converting|Finished) ' + self.mri_prefix + '(\\d+)([-_]\\d+)?'\n",
        "\n",
        "            content = self.logger.read_msg()\n",
        "            \n",
        "            # Define the regular expression pattern\n",
        "            pattern = re.compile(regex)\n",
        "            # Find all occurrences in the list\n",
        "            matches = [match.groups() for s in content if (match := pattern.match(s))]\n",
        "\n",
        "            for action, brats_id, suffix in matches:\n",
        "                if action == 'Converting':\n",
        "                    if(suffix is not None):\n",
        "                        converting_ids.add(self.mri_prefix + brats_id + suffix)\n",
        "                    else:\n",
        "                        converting_ids.add(self.mri_prefix + brats_id)\n",
        "                elif action == 'Finished':\n",
        "                    if(suffix is not None):\n",
        "                        finished_ids.add(self.mri_prefix + brats_id + suffix)\n",
        "                    else:\n",
        "                        finished_ids.add(self.mri_prefix + brats_id)\n",
        "            # Find finished and pending conversions\n",
        "            common_ids = converting_ids.intersection(finished_ids)\n",
        "            finished = [el for el in common_ids]\n",
        "            pending = [el for el in converting_ids.difference(common_ids)]\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Exception in DataPreprocessor.get_status_ids')\n",
        "            print(e)\n",
        "\n",
        "        return {'Finished':finished, 'Pending':pending}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def remove_pending_graphs(self):\n",
        "\n",
        "        pending = self.get_status_ids()['Pending']\n",
        "\n",
        "        for mri_id in pending:\n",
        "            remove_path = f\"{self.graph_dir}{os.sep}{mri_id}\"\n",
        "            try:\n",
        "                print('Removing pending graph: ' + remove_path)\n",
        "                shutil.rmtree(remove_path)\n",
        "            except Exception as e:\n",
        "                print('Exception in DataPreprocessor.remove_pending_graphs:')\n",
        "                print(e)\n",
        "\n",
        "\n",
        "\n",
        "    def split_dataset(self, fixed = (1001, 125, 125),seed = 42):\n",
        "\n",
        "        random.seed(seed)\n",
        "        pos = random.sample(range(0,len(self.all_ids)), len(self.all_ids))\n",
        "\n",
        "        if fixed:\n",
        "            if(np.sum(fixed) != len(self.all_ids)):\n",
        "                print(\"Error: fixed ratio does not sum up to one.\\nSwitching to default (1001,125,125))\")\n",
        "                fixed = (1001,125,125)\n",
        "\n",
        "            train_length = fixed[0]\n",
        "            val_length = fixed[1]\n",
        "            test_length = fixed[2]\n",
        "\n",
        "\n",
        "        split_dict = {\n",
        "            'train': [self.all_ids[i] for i in pos[:train_length]],\n",
        "            'val': [self.all_ids[i] for i in pos[train_length :train_length + val_length]],\n",
        "            'test': [self.all_ids[i] for i in pos[train_length + val_length:]]\n",
        "        }\n",
        "\n",
        "        for k in split_dict.keys():\n",
        "            parent = '/'.join(self.data_dir.split('/')[:-1])\n",
        "            dst = os.path.join(parent,k)\n",
        "\n",
        "            try:\n",
        "              # create train,val,test dirs\n",
        "              if(not os.path.exists(dst)):\n",
        "                os.mkdir(dst)\n",
        "\n",
        "              # copy splitted data inside folders\n",
        "              for id in split_dict[k]:\n",
        "                if(not os.path.exists(os.path.join(dst,id))):\n",
        "                   os.mkdir(os.path.join(dst,id))\n",
        "                copy_tree(self.id_to_fp[id],os.path.join(dst,id))\n",
        "\n",
        "            except Exception as e:\n",
        "              print(f\"Exception thrown in class {self.__class__.__name__ }, method split_dataset\")\n",
        "              print(e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def image_to_graph(self, mri_id):\n",
        "\n",
        "\n",
        "        save_path = f\"{self.graph_dir}{os.sep}{mri_id}\"\n",
        "        finished = self.get_status_ids()['Finished']\n",
        "\n",
        "        if(mri_id not in finished):\n",
        "            self.logger.log_msg('Converting ' + str(mri_id))\n",
        "        print('Converting ' + str(mri_id))\n",
        "\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "\n",
        "        if(not os.path.exists(f\"{save_path}{os.sep}{mri_id}_input.nii.gz\") or self.force_conversion == True):\n",
        "            imstack, labels, crop_idxs = self.__getitem__(mri_id)\n",
        "        else:\n",
        "            imstack, labels = self.get_image(mri_id)\n",
        "\n",
        "        # Load supervoxels, if already exist, and save computational time avoiding the runnning of slic\n",
        "        sv_partitioning = None\n",
        "        if(os.path.exists(f\"{save_path}{os.sep}{mri_id}_supervoxels.nii.gz\")):\n",
        "            sv_partitioning = self.get_supervoxel_partitioning(mri_id)\n",
        "            print('Loading supervoxels...')\n",
        "\n",
        "        nx_graph,node_feats,region_img = img2graph(imstack,labels,sv_partitioning,self.num_nodes,self.boxiness_coef,self.num_neighbors)\n",
        "\n",
        "        save_networkx_graph(nx_graph, f\"{save_path}{os.sep}{mri_id}_nxgraph.json\")\n",
        "        save_as_nifti(imstack,f\"{save_path}{os.sep}{mri_id}_input.nii.gz\")\n",
        "        if(self.include_labels):\n",
        "            save_as_nifti(labels,f\"{save_path}{os.sep}{mri_id}_label.nii.gz\")\n",
        "        save_as_nifti(region_img,f\"{save_path}{os.sep}{mri_id}_supervoxels.nii.gz\")\n",
        "        \n",
        "        with open(f\"{save_path}{os.sep}{mri_id}_crop.pkl\", \"wb\") as f:\n",
        "            pickle.dump(crop_idxs, f)\n",
        "\n",
        "        return mri_id\n",
        "\n",
        "    def get_voxel_labels(self,mri_id):\n",
        "        fp=f\"{self.graph_dir}{os.sep}{mri_id}{os.sep}{mri_id}_label.nii.gz\"\n",
        "        return read_nifti(fp,np.int16)\n",
        "\n",
        "    def get_image(self,mri_id):\n",
        "        fp = f\"{self.graph_dir}{os.sep}{mri_id}{os.sep}{mri_id}_input.nii.gz\"\n",
        "        img = read_nifti(fp,np.float32)\n",
        "        return img,self.get_voxel_labels(mri_id)\n",
        "\n",
        "\n",
        "    def get_supervoxel_partitioning(self,mri_id):\n",
        "        fp=f\"{self.graph_dir}{os.sep}{mri_id}{os.sep}{mri_id}_supervoxels.nii.gz\"\n",
        "        return read_nifti(fp,np.int16)\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        self.remove_pending_graphs()\n",
        "\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.N_THREADS) as executor:\n",
        "            futures = [executor.submit(self.image_to_graph, mri_id) for mri_id in self.all_ids]\n",
        "            print(\"Set up Threads, starting execution\")\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                try:\n",
        "                    mri_id = future.result()\n",
        "                except Exception as exc:\n",
        "                    print(\"Exception caught in DataPreprocessor.run\")\n",
        "                    print(f\"{exc}\")\n",
        "                else:\n",
        "                    if(mri_id not in self.get_status_ids()['Finished']):\n",
        "                        # Log message\n",
        "                        self.logger.log_msg('Finished ' + str(mri_id))\n",
        "                    print(\"Finished \"+ str(mri_id))\n",
        "\n",
        "\n",
        "'''\n",
        "A Dataset similar to a torch dataset which iterates over all samples in a directory and returns the contents as numpy arrays.\n",
        "Expects to receive a filepath to the output of the preprocess script which should have the following:\n",
        "1.processed image (nifti)\n",
        "2.label image (nifti)\n",
        "3.networkx graph (json)\n",
        "4.supervoxel partitioning (nifti)\n",
        "5. (optionally) a .npy file containing the crop of the processed image relative to the original image\n",
        "\n",
        "\n",
        "#Input#\n",
        "dataset_root_dir: filepath to preprocessed dataset (generated by running preprocess script)\n",
        "mri_start_string: a prefix that every image folder starts with (can be empty string)\n",
        "read_image: whether to read in and return preprocessed images for each sample (only necessary for CNN model)\n",
        "read_graph: whether to return graphs for each sample (for training GNN)\n",
        "read_label: whether to read in labels. Will be returned in vector form (one label per node if )\n",
        "\n",
        "#Output#\n",
        "\n",
        "If graph:\n",
        "Returns a DGL Graph, features for each node, and (optionally) labels for each node\n",
        "If image:\n",
        "Returns a numpy image array and (optionally) a numpy label array\n",
        "\n",
        "'''\n",
        "\n",
        "class ImageGraphDataset(Dataset):\n",
        "    def __init__(self, dataset_root_dir,mri_start_string,read_image=True,read_graph=True,read_label=True, features = None):\n",
        "        self.dataset_root_dir=dataset_root_dir\n",
        "        self.all_ids = self.get_all_mris_in_dataset(dataset_root_dir,mri_start_string)\n",
        "        self.read_image=read_image\n",
        "        self.read_graph=read_graph\n",
        "        self.read_label = read_label\n",
        "        self.features = features\n",
        "        assert(self.read_graph or self.read_image)\n",
        "\n",
        "    def get_all_mris_in_dataset(self,dataset_root_dir,mri_start_string):\n",
        "        mri_folders = glob.glob(f\"{dataset_root_dir}**/{mri_start_string}*/\",recursive=True)\n",
        "        mri_ids = [fp.split(os.sep)[-2] for fp in mri_folders]\n",
        "        print(f\"Found {len(mri_folders)} MRIs\")\n",
        "        return mri_ids\n",
        "\n",
        "    def get_one(self,mri_id):\n",
        "        if(self.read_graph and not self.read_image):\n",
        "            return (mri_id, *self.get_graph(mri_id))\n",
        "        elif(self.read_image  and not self.read_graph):\n",
        "            return (mri_id, *self.get_image(mri_id))\n",
        "        elif(self.read_image and self.read_graph):\n",
        "            return (mri_id, *self.get_graph(mri_id), *self.get_image(mri_id))\n",
        "        else:\n",
        "            print(\"Invalid combination of flags\")\n",
        "    \n",
        "    def add_features(self,features):\n",
        "        feats_len = len(features)\n",
        "        new_features = np.zeros([feats_len + len(self.features)])\n",
        "        new_features[:feats_len] = features\n",
        "        \n",
        "        new_features[:feats_len] = np.copy(features)\n",
        "        new_features[feats_len:] = np.copy(self.features)\n",
        "\n",
        "        return np.array(new_features)\n",
        "\n",
        "    '''\n",
        "    Reads in the saved networkx graph, converts it to a DGLGraph, normalizes the graph (not actually sure how useful this is),\n",
        "    and returns the DGLGraph as well as a vector of node features and optionally labels.\n",
        "    '''\n",
        "    def get_graph(self,mri_id):\n",
        "        nx_graph = load_networkx_graph(f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_nxgraph.json\")\n",
        "        #features = np.array([nx_graph.nodes[n]['features'] for n in nx_graph.nodes])\n",
        "        features = []\n",
        "        for n in nx_graph.nodes:\n",
        "            node_features = np.array(nx_graph.nodes[n]['features'])\n",
        "            if(self.features is not None):\n",
        "                node_features = self.add_features(node_features)\n",
        "            features.append(node_features)\n",
        "        features = np.array(features, dtype=np.float32)\n",
        "\n",
        "        if(self.read_label):\n",
        "            labels = np.array([nx_graph.nodes[n]['label'] for n in nx_graph.nodes])\n",
        "\n",
        "        G = to_dgl_graph(nx_graph)\n",
        "        n_edges = G.number_of_edges()\n",
        "        # normalization\n",
        "        degs = G.in_degrees().float()\n",
        "        norm = torch.pow(degs, -0.5)\n",
        "        norm[torch.isinf(norm)] = 0\n",
        "        G.ndata['norm'] = norm.unsqueeze(1)\n",
        "        #G.ndata['feat'] = features\n",
        "        if(self.read_label):\n",
        "            #G.ndata['label'] = labels\n",
        "            return G, features, labels\n",
        "        return G, features\n",
        "\n",
        "    def get_voxel_labels(self,mri_id):\n",
        "        fp=f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_label.nii.gz\"\n",
        "        return read_nifti(fp,np.int16)\n",
        "\n",
        "    def get_image(self,mri_id):\n",
        "        fp = f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_input.nii.gz\"\n",
        "        img = read_nifti(fp,np.float32)\n",
        "        if(self.read_label):\n",
        "            return img,self.get_voxel_labels(mri_id)\n",
        "        else:\n",
        "            return (img,)\n",
        "\n",
        "    def get_supervoxel_partitioning(self,mri_id):\n",
        "        fp=f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_supervoxels.nii.gz\"\n",
        "        return read_nifti(fp,np.int16)\n",
        "\n",
        "    def get_crop(self,mri_id):\n",
        "        fp=f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_crop.pkl\"\n",
        "        return tuple(np.load(fp,allow_pickle=True))\n",
        "\n",
        "    def save_voxel_preds(self, mri_id, predicted_nodes):\n",
        "        supervoxel_partitioning = self.get_supervoxel_partitioning(mri_id)\n",
        "        raw_data_crop = self.get_crop(mri_id)\n",
        "        predicted_voxels = project_nodes_to_img(supervoxel_partitioning,predicted_nodes)\n",
        "        predicted_voxels = uncrop_to_brats_size(raw_data_crop,predicted_voxels)\n",
        "        predicted_voxels = swap_labels_to_brats(predicted_voxels)\n",
        "        save_as_nifti(predicted_voxels,f\"{self.dataset_root_dir}{os.sep}{mri_id}{os.sep}{mri_id}_seg.nii.gz\")\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        for mri_id in self.all_ids:\n",
        "            yield self.get_one(mri_id)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        mri_id = self.all_ids[index]\n",
        "        #print(index)\n",
        "        #mri_id = [el for el in self.all_ids if el == index][0]\n",
        "        return self.get_one(mri_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_ids)\n",
        "\n",
        "\n",
        "\n",
        "class SeqSampler(SequentialSampler):\n",
        "    \"\"\"Samples elements sequentially, always in the same order.\n",
        "\n",
        "    Args:\n",
        "        data_source (Dataset): dataset to sample from\n",
        "    \"\"\"\n",
        "    def __init__(self, data_source:Dataset):\n",
        "        self.data_source = data_source\n",
        "        self.indexDict = [id for id in data_source.all_ids]\n",
        "    def __iter__(self):\n",
        "        return iter(self.indexDict)\n",
        "    def __len__(self):\n",
        "        return len(self.indexDict)\n",
        "\n",
        "\n",
        "def minibatch_graphs(samples):\n",
        "    mri_ids,graphs,features, labels = map(list, zip(*samples))\n",
        "    #print(\"Batch Mri Ids:\",mri_ids)\n",
        "    batched_graph = dgl_batch(graphs)\n",
        "    return mri_ids,batched_graph, torch.FloatTensor(np.concatenate(features)), torch.LongTensor(np.concatenate(labels))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgINpL8BK3hL"
      },
      "source": [
        "## Models üì™\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "78WIjwSZRkGN"
      },
      "outputs": [],
      "source": [
        "\n",
        "from dgl.nn.pytorch import GATConv, GraphConv\n",
        "from dgl.nn.pytorch.conv import SAGEConv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "'''\n",
        "Contains the actual neural network architectures.\n",
        "Supports GraphSAGE with either the pool,mean,gcn, or lstm aggregator as well as GAT.\n",
        "The input, output, and intermediate layer sizes can all be specified.\n",
        "Typically will call init_graph_net and pass along the desired model and hyperparameters.\n",
        "\n",
        "Also contains the CNN Refinement net which is a very simple 2 layer 3D convolutional neural network.\n",
        "As input, it expects 8 channels, which are the concatenated 4 input modalities and 4 output logits of the GNN predictions.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "class GraphSage(nn.Module):\n",
        "    def __init__(self,in_feats,layer_sizes,n_classes,aggregator_type,dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        # input layer\n",
        "        self.layers.append(SAGEConv(in_feats, layer_sizes[0], aggregator_type, feat_drop=dropout, activation=F.relu))\n",
        "        # hidden layers\n",
        "        for i in range(1,len(layer_sizes)):\n",
        "            self.layers.append(SAGEConv(layer_sizes[i-1], layer_sizes[i], aggregator_type, feat_drop=dropout, activation=F.relu))\n",
        "        # output layer\n",
        "        self.layers.append(SAGEConv(layer_sizes[-1], n_classes, aggregator_type, feat_drop=0, activation=None))\n",
        "\n",
        "    def forward(self,graph,features):\n",
        "        h = features\n",
        "        for layer in self.layers:\n",
        "            h = layer(graph, h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self,in_feats,layer_sizes,n_classes,heads,residuals,\n",
        "                activation=F.elu,feat_drop=0,attn_drop=0,negative_slope=0.2):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        # input projection (no residual)\n",
        "        self.layers.append(GATConv(\n",
        "            in_feats, layer_sizes[0], heads[0],\n",
        "            feat_drop, attn_drop, negative_slope, False, self.activation))\n",
        "        # hidden layers\n",
        "        for i in range(1, len(layer_sizes)):\n",
        "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "            self.layers.append(GATConv(\n",
        "                layer_sizes[i-1] * heads[i-1], layer_sizes[i], heads[i],\n",
        "                feat_drop, attn_drop, negative_slope, residuals[i], self.activation))\n",
        "        # output projection\n",
        "        self.layers.append(GATConv(\n",
        "            layer_sizes[-1] * heads[-1], n_classes, 1,\n",
        "            feat_drop, attn_drop, negative_slope, False, None))\n",
        "\n",
        "    def forward(self,g, inputs):\n",
        "        h = inputs\n",
        "        for l in range(len(self.layers)-1):\n",
        "            h = self.layers[l](g, h).flatten(1)\n",
        "        # output projection\n",
        "        logits = self.layers[-1](g, h).mean(1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class GIN(nn.Module):\n",
        "    def __init__(self, in_feats, layer_sizes, n_classes, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # input layer\n",
        "        self.layers.append(GINConv(apply_func=nn.Linear(in_feats, layer_sizes[0]), aggregator_type='sum'))\n",
        "        # hidden layers\n",
        "        for i in range(1, len(layer_sizes)):\n",
        "            self.layers.append(GINConv(apply_func=nn.Linear(layer_sizes[i-1], layer_sizes[i]), aggregator_type='sum'))\n",
        "        # output layer\n",
        "        self.layers.append(GINConv(apply_func=nn.Linear(layer_sizes[-1], n_classes), aggregator_type='sum'))\n",
        "\n",
        "    def forward(self, g, feat):\n",
        "        h = feat\n",
        "        for layer in self.layers[:-1]:\n",
        "            h = layer(g, h)\n",
        "            h = F.relu(h)\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = self.layers[-1](g, h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class ChebNet(nn.Module):\n",
        "    def __init__(self, in_feats, layer_sizes, n_classes, k, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Input layer\n",
        "        self.layers.append(ChebConv(in_feats, layer_sizes[0], k))\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(layer_sizes)):\n",
        "            self.layers.append(ChebConv(layer_sizes[i-1], layer_sizes[i], k))\n",
        "\n",
        "        # Output layer\n",
        "        self.layers.append(ChebConv(layer_sizes[-1], n_classes, k))\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            h = layer(g, h)\n",
        "            if i != len(self.layers) - 1: # No activation and dropout on the output layer\n",
        "                h = F.relu(h)\n",
        "                h = self.dropout(h)\n",
        "        return h\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u3q4OhjYyGq"
      },
      "source": [
        "## Model Wrapper üì®\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "biiQzha9JP8G"
      },
      "outputs": [],
      "source": [
        "class ModelWrapper():\n",
        "    \"\"\"\n",
        "    Allows train, evaluation, prediction and I/O operations on generic PyTorch models\n",
        "    The model is saved at every epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, optimizer: nn.Module, loss_fn: nn.Module,\n",
        "                 num_epochs: int, supervised: bool = True, dict_params:dict = {}, eval_metrics = None, isgnn:bool = True, LOAD_MODEL: bool = False,\n",
        "                 model_path: str  = '/content/drive/MyDrive/Lorusso/models',):\n",
        "\n",
        "        self.device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "        self.model = model\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model = self.model.to(torch.float)\n",
        "        self.num_epochs = num_epochs\n",
        "        self.loss_fn = loss_fn\n",
        "        self.loss_fn = self.loss_fn.to(self.device)\n",
        "        self.eval_metrics = eval_metrics\n",
        "        self.optimizer = optimizer\n",
        "        self.model_path = model_path\n",
        "        self.save_path = self.model_path + '/' + self.model.__class__.__name__ + '/model.pt'\n",
        "\n",
        "        self.LOAD_MODEL = LOAD_MODEL\n",
        "        self.isgnn = isgnn\n",
        "        self.supervised = supervised\n",
        "        self.training_loss = []\n",
        "        self.validation_loss = []\n",
        "        self.dict_metrics = {}\n",
        "        self.elapsed_epochs = 0\n",
        "        self.elapsed_seconds = 0\n",
        "\n",
        "        self.dict_params = dict_params\n",
        "        self.update_params({'loss_fn':self.loss_fn.__class__.__name__})\n",
        "        self.update_params({'optimizer':self.optimizer.__class__.__name__})\n",
        "        self.update_params({'learning_rate':self.optimizer.state_dict()['param_groups'][0]['lr']})\n",
        "        self.update_params({'weight_decay':self.optimizer.state_dict()['param_groups'][0]['weight_decay']})\n",
        "\n",
        "        if(self.LOAD_MODEL):\n",
        "          self.load_checkpoint()\n",
        "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma = 0.98)\n",
        "        # Create directory for model loading\n",
        "        #try:\n",
        "        #  if(not os.path.exists(self.model_path + '/' + self.model.__class__.__name__)):\n",
        "        #    os.mkdir(self.model_path + '/' + self.model.__class__.__name__)\n",
        "        #except Exception as e:\n",
        "        #  print(f\"Exception thrown in class {self.__class__.__name__ }, method __init__\")\n",
        "        #  print(e)\n",
        "        #  print('\\n')\n",
        "\n",
        "\n",
        "    def update_params(self, new_params):\n",
        "        try:\n",
        "            self.dict_params.update(new_params)\n",
        "        except Exception as e:\n",
        "            print('Exception raised in WrapperModel.update_params')\n",
        "            print(e)\n",
        "\n",
        "    def log_checkpoint(self, info: dict):\n",
        "        mlflow.pytorch.log_state_dict(info, artifact_path='checkpoint')\n",
        "        #torch.save(info, self.save_path)\n",
        "\n",
        "    def load_checkpoint(self, run_id = None):\n",
        "        \"\"\" Loads the last checkpoint for the given model \"\"\"\n",
        "        try:\n",
        "            if(run_id is None):\n",
        "                run_id = mlflow.search_runs(experiment_names=['BraTS_'+type(self.model).__name__],\n",
        "                                        order_by=[\"start_time DESC\"]).iloc[0].run_id\n",
        "\n",
        "            checkpoint = mlflow.pytorch.load_state_dict('runs:/'+run_id+'/checkpoint',\n",
        "                                                        map_location=torch.device(self.device))\n",
        "\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.elapsed_epochs = len(checkpoint['training_loss'])\n",
        "            self.training_loss = checkpoint['training_loss']\n",
        "            self.validation_loss = checkpoint['validation_loss']\n",
        "            self.dict_metrics = checkpoint['dict_metrics']\n",
        "            self.elapsed_seconds = checkpoint['elapsed_seconds']\n",
        "            #print(self.elapsed_epochs)\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Exception thrown in class {self.model.__class__.__name__ }, method load_checkpoint\")\n",
        "            print(e)\n",
        "            print('\\n')\n",
        "            if(mlflow.active_run()):\n",
        "                mlflow.end_run()\n",
        "\n",
        "\n",
        "    #Calculates a slew of different metrics that might be interesting such as the number of nodes of each label and voxel and node Dice scores\n",
        "    def calculate_all_metrics_for_brain(self,mri_id,dataset,node_preds,node_labels):\n",
        "        label_counts = np.concatenate([evaluation.count_node_labels(node_preds),evaluation.count_node_labels(node_labels)])\n",
        "        node_dices = evaluation.calculate_node_dices(node_preds,node_labels)\n",
        "        #read in voxel_labels and supervoxel mapping to compute the image metrics\n",
        "        sv_partitioning = dataset.get_supervoxel_partitioning(mri_id)\n",
        "        true_voxels = dataset.get_voxel_labels(mri_id)\n",
        "        pred_voxels = project_nodes_to_img(sv_partitioning,node_preds)\n",
        "        voxel_metrics = evaluation.calculate_brats_metrics(pred_voxels,true_voxels)\n",
        "        return label_counts,np.concatenate([node_dices,voxel_metrics])\n",
        "\n",
        "\n",
        "    def train(self, train_loader, val_loader = None,  experiment_prefix = ''):\n",
        "\n",
        "        try:\n",
        "            # Set MLFlow experiment\n",
        "            if(experiment_prefix):\n",
        "                mlflow.set_experiment(experiment_prefix + self.model.__class__.__name__)\n",
        "            else:\n",
        "                mlflow.set_experiment('BraTS_'+self.model.__class__.__name__)\n",
        "\n",
        "            # Start a new run if the model wasn't loaded\n",
        "            if(not mlflow.active_run()):\n",
        "                # Track metrics in the current run\n",
        "                mlflow.start_run()\n",
        "            elif(self.LOAD_MODEL == False and mlflow.active_run()):\n",
        "                mlflow.end_run()\n",
        "                mlflow.start_run()\n",
        "\n",
        "            for i in range(len(self.training_loss)):\n",
        "                mlflow.log_metric('train_loss', self.training_loss[i], step=i)\n",
        "\n",
        "            for i in range(len(self.validation_loss)):\n",
        "                mlflow.log_metric('val_loss', self.validation_loss[i], step=i)\n",
        "\n",
        "            self.update_params({'batch_size':train_loader.batch_size})\n",
        "            mlflow.log_params(self.dict_params)\n",
        "\n",
        "            training_loss = self.training_loss\n",
        "            validation_loss = self.validation_loss\n",
        "\n",
        "            self.tot_epochs = self.elapsed_epochs + self.num_epochs+1\n",
        "            self.tot_time = time.time()\n",
        "\n",
        "            # Train\n",
        "            for epoch in range(self.elapsed_epochs+1, self.tot_epochs):\n",
        "                start = time.time() # track time\n",
        "\n",
        "              # Evaluate first if loaded model missed the evaluation during an epoch\n",
        "                if(len(training_loss) > len(validation_loss)):\n",
        "\n",
        "                    # COMPLETE EVALUATION OF PREVIOUS EPOCH\n",
        "                    # NB: epoch = epoch - 1\n",
        "                    if(val_loader is not None):\n",
        "\n",
        "                        val_batch_loss = self.__eval(val_loader, (epoch-1) )\n",
        "                        validation_loss.append(np.array(val_batch_loss).mean())\n",
        "\n",
        "                        # Log additional metrics\n",
        "                        self.dict_metrics = self.__eval_metrics(val_loader)\n",
        "                        for k in self.dict_metrics.keys():\n",
        "                            mlflow.log_metric(str(k), self.dict_metrics[k], step=epoch-1)\n",
        "\n",
        "                        mlflow.log_metric('val_loss',validation_loss[-1], step=epoch-1)\n",
        "\n",
        "                        print(f\"Epoch: {epoch-1}/{self.tot_epochs-1}, Validation Loss: {validation_loss[-1]:.4f}, Elapsed time: {time.time() - start:.0f} sec\")\n",
        "\n",
        "                      # Update training time\n",
        "                        epoch_time = int(time.time() - self.tot_time) + self.elapsed_seconds\n",
        "\n",
        "                      #Create checkpoint\n",
        "                        val_dict = {\n",
        "                                  'model_state_dict': self.model.state_dict(),\n",
        "                                  'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                                  'training_loss': training_loss,\n",
        "                                  'validation_loss': validation_loss,\n",
        "                                  'dict_metrics': self.dict_metrics,\n",
        "                                  'elapsed_seconds': epoch_time\n",
        "                                  }\n",
        "                        self.log_checkpoint(val_dict)\n",
        "                    else:\n",
        "                        # Kind of exception, needed to keep the vectors of the same size\n",
        "                        validation_loss.append(np.mean(validation_loss))\n",
        "\n",
        "                        #Log metric\n",
        "                        mlflow.log_metric('val_loss',validation_loss[-1], step=epoch-1)\n",
        "\n",
        "                #### TRAIN ######\n",
        "                train_batch_loss = self.__train(train_loader, epoch)\n",
        "                training_loss.append(np.array(train_batch_loss).mean())\n",
        "\n",
        "                # Log metrics\n",
        "                mlflow.log_metric('train_loss',training_loss[-1], step=epoch) # MLFLOW tracking\n",
        "                print(f\"Epoch: {epoch}/{self.tot_epochs-1}, Loss: {training_loss[-1]:.4f}, Epoch elapsed time: {time.time() - start:.0f} sec \\n\")\n",
        "\n",
        "                #Save model every elapsed epoch\n",
        "                self.elapsed_epochs = epoch\n",
        "                epoch_time = int(time.time() - self.tot_time) + self.elapsed_seconds\n",
        "\n",
        "                train_dict = {\n",
        "                          'model_state_dict': self.model.state_dict(),\n",
        "                          'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                          'training_loss': training_loss,\n",
        "                          'validation_loss': validation_loss,\n",
        "                          'dict_metrics':self.dict_metrics,\n",
        "                          'elapsed_seconds': epoch_time\n",
        "                          }\n",
        "                self.log_checkpoint(train_dict)\n",
        "\n",
        "                #### EVALUATE ######\n",
        "                if(val_loader is not None):\n",
        "\n",
        "                    val_batch_loss = self.__eval(val_loader, epoch)\n",
        "                    validation_loss.append(np.array(val_batch_loss).mean())\n",
        "\n",
        "                    # Log additional metrics\n",
        "                    self.dict_metrics = self.__eval_metrics(val_loader)\n",
        "                    for k in self.dict_metrics.keys():\n",
        "                        mlflow.log_metric(str(k), self.dict_metrics[k], step=epoch)\n",
        "\n",
        "                    # Log metric\n",
        "                    mlflow.log_metric('val_loss',validation_loss[-1], step=epoch)\n",
        "                    print(f\"Epoch: {epoch}/{self.tot_epochs-1}, Validation Loss: {validation_loss[-1]:.4f}, Elapsed time: {time.time() - start:.0f} sec\\n\")\n",
        "\n",
        "                    #Checkpoint\n",
        "                    epoch_time = int(time.time() - self.tot_time) + self.elapsed_seconds\n",
        "                    val_dict = {\n",
        "                              'model_state_dict': self.model.state_dict(),\n",
        "                              'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                              'training_loss': training_loss,\n",
        "                              'validation_loss': validation_loss,\n",
        "                              'dict_metrics':self.dict_metrics,\n",
        "                              'elapsed_seconds': epoch_time\n",
        "                              }\n",
        "                    self.log_checkpoint(val_dict)\n",
        "\n",
        "            print(f\"Total training time: {time.time()-self.tot_time:.0f} sec\")\n",
        "\n",
        "            # Log model --> end run\n",
        "            mlflow.pytorch.log_model(self.model, artifact_path='model')\n",
        "            mlflow.end_run()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Exception thrown in class Wrapper, method train:\")\n",
        "            print(e)\n",
        "            print('\\n')\n",
        "            mlflow.end_run()\n",
        "\n",
        "        return training_loss, validation_loss\n",
        "\n",
        "\n",
        "\n",
        "    def __train(self, train_loader: DataLoader, epoch:int):\n",
        "\n",
        "        \"\"\" Train for an epoch \"\"\"\n",
        "\n",
        "        self.model.train()\n",
        "        train_batch_loss = []\n",
        "        train_steps = len(train_loader)\n",
        "\n",
        "        if(self.isgnn):\n",
        "\n",
        "            metrics = np.zeros((len(train_loader),3))\n",
        "            i = 0\n",
        "\n",
        "            for batch_mris, batch_graphs, batch_features, batch_labels in train_loader:\n",
        "\n",
        "                batch_graphs = batch_graphs.to(self.device)\n",
        "                batch_features = batch_features.to(self.device)\n",
        "                batch_labels = batch_labels.to(self.device)\n",
        "\n",
        "                logits = self.model(batch_graphs,batch_features)\n",
        "                logits = logits.to(self.device)\n",
        "                #_, predicted_classes = torch.max(logits, dim=1)\n",
        "                predicted_classes = logits.argmax(1)\n",
        "\n",
        "                loss = self.loss_fn(logits, batch_labels)\n",
        "                train_batch_loss.append(loss.detach().item())\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                metrics[i][:] = evaluation.calculate_node_dices(predicted_classes.detach().cpu().numpy(),\n",
        "                                                          batch_labels.detach().cpu().numpy())\n",
        "\n",
        "                i = i+1\n",
        "                out = f\"Epoch: {epoch}/{self.tot_epochs-1}, Step: {i+1}/{train_steps}, Training Loss: {loss.item():.4f}, Elapsed time: {time.time() - self.tot_time:.0f} sec \"\n",
        "                sys.stdout.write(\"\\r\" + out)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "\n",
        "            self.scheduler.step()\n",
        "            avg_metrics = np.mean(metrics,axis=0)\n",
        "\n",
        "            print(f\"train_wt_dice: {np.round(avg_metrics[0],4)}\")\n",
        "            print(f\"train_ct_dice: {np.round(avg_metrics[1],4)}\")\n",
        "            print(f\"train_at_dice: {np.round(avg_metrics[2],4)}\")\n",
        "\n",
        "            mlflow.log_metric('train_wt_dice', np.round(avg_metrics[0],4), step = epoch)\n",
        "            mlflow.log_metric('train_ct_dice', np.round(avg_metrics[1],4), step = epoch)\n",
        "            mlflow.log_metric('train_at_dice', np.round(avg_metrics[2],4), step = epoch)\n",
        "\n",
        "        else:\n",
        "\n",
        "            for i, (data,labels) in enumerate(train_loader):\n",
        "\n",
        "                #torch.cuda.empty_cache()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                if self.device == 'cuda':\n",
        "                  data = data.type(torch.cuda.FloatTensor)\n",
        "                else:\n",
        "                  data = data.type(torch.FloatTensor)\n",
        "\n",
        "                data = data.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                outputs = self.model(data)\n",
        "\n",
        "                if(self.supervised):\n",
        "                  loss = self.loss_fn(outputs,labels)\n",
        "                else:\n",
        "                  loss = self.loss_fn(outputs, data)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                train_batch_loss.append(loss.detach().item())\n",
        "\n",
        "                out = f\"Epoch: {epoch}/{self.tot_epochs-1}, Step: {i+1}/{train_steps}, Training Loss: {loss.item():.4f}, Elapsed time: {time.time() - self.tot_time:.0f} sec \"\n",
        "                sys.stdout.write(\"\\r\" + out)\n",
        "                sys.stdout.flush()\n",
        "            torch.cuda.empty_cache()\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        return train_batch_loss\n",
        "\n",
        "\n",
        "    def __eval(self, val_loader: DataLoader, epoch:int):\n",
        "\n",
        "        \"\"\" Evaluate for an epoch \"\"\"\n",
        "\n",
        "        val_steps =  len(val_loader)\n",
        "        self.model.eval()\n",
        "        val_batch_loss = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if(self.isgnn):\n",
        "\n",
        "                #metrics stores loss and node dices\n",
        "                metrics = np.zeros((len(val_loader),3))\n",
        "                i=0\n",
        "\n",
        "                for curr_id,batch_graphs,batch_feats,batch_labels in val_loader:\n",
        "\n",
        "                    batch_graphs = batch_graphs.to(self.device)\n",
        "                    batch_feats = torch.FloatTensor(batch_feats).to(self.device)\n",
        "                    batch_labels = torch.LongTensor(batch_labels).to(self.device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        logits = self.model(batch_graphs,batch_feats)\n",
        "                        val_loss = self.loss_fn(logits, batch_labels)\n",
        "\n",
        "                    val_batch_loss.append(val_loss.detach().item())\n",
        "                    #_, predicted_classes = torch.max(logits, dim=1)\n",
        "                    predicted_classes = logits.argmax(1)\n",
        "                    res = evaluation.calculate_node_dices(predicted_classes.detach().cpu().numpy(),\n",
        "                                                          batch_labels.detach().cpu().numpy())\n",
        "                    metrics[i][:] = res\n",
        "                    i+=1\n",
        "                    \n",
        "                    out = f\"Epoch: {epoch}/{self.tot_epochs-1}, Validation Step: {i+1}/{val_steps}, Validation Loss: {val_loss.item():.4f}, Elapsed time: {time.time() - self.tot_time:.0f} sec \"\n",
        "                    sys.stdout.write(\"\\r\" + out)\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "                avg_metrics = np.mean(metrics,axis=0)\n",
        "\n",
        "                print(f\"val_wt_dice: {np.round(avg_metrics[0],4)}\")\n",
        "                print(f\"val_ct_dice: {np.round(avg_metrics[1],4)}\")\n",
        "                print(f\"val_at_dice: {np.round(avg_metrics[2],4)}\")\n",
        "\n",
        "                mlflow.log_metric('val_wt_dice', np.round(avg_metrics[0],4), step = epoch)\n",
        "                mlflow.log_metric('val_ct_dice', np.round(avg_metrics[1],4), step = epoch)\n",
        "                mlflow.log_metric('val_at_dice', np.round(avg_metrics[2],4), step = epoch)\n",
        "\n",
        "                return val_batch_loss\n",
        "\n",
        "            else:\n",
        "                for i, (data,labels) in enumerate(val_loader):\n",
        "\n",
        "                  #torch.cuda.empty_cache()\n",
        "                  self.optimizer.zero_grad()\n",
        "\n",
        "                  if self.device == 'cuda':\n",
        "                    data = data.type(torch.cuda.FloatTensor)\n",
        "                  else:\n",
        "                    data = data.type(torch.FloatTensor)\n",
        "\n",
        "                  data = data.to(self.device)\n",
        "                  outputs = self.model(data)\n",
        "\n",
        "                  if(self.supervised):\n",
        "                      val_loss = self.loss_fn(outputs, labels)\n",
        "                  else:\n",
        "                      val_loss = self.loss_fn(outputs, data)\n",
        "\n",
        "                  val_batch_loss.append(val_loss.detach().item())\n",
        "\n",
        "                  out = f\"Epoch: {epoch}/{self.tot_epochs-1}, Validation Step: {i+1}/{val_steps}, Validation Loss: {val_loss.item():.4f}, Elapsed time: {time.time() - self.tot_time:.0f} sec \"\n",
        "                  sys.stdout.write(\"\\r\" + out)\n",
        "                  sys.stdout.flush()\n",
        "\n",
        "                torch.cuda.empty_cache()\n",
        "                time.sleep(.5)\n",
        "\n",
        "        return val_batch_loss\n",
        "\n",
        "\n",
        "    def __eval_metrics(self, data_loader:DataLoader):\n",
        "        \"\"\" Evaluates additional metrics aside the loss function \"\"\"\n",
        "        metrics_dict = {}\n",
        "        try:\n",
        "            self.model.eval()\n",
        "            if(self.eval_metrics is not None):\n",
        "\n",
        "                metrics_dict = {k.__class__.__name__:[] for k in self.eval_metrics}\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for i, (data,labels) in enumerate(data_loader):\n",
        "                        torch.cuda.empty_cache()\n",
        "                        self.optimizer.zero_grad()\n",
        "\n",
        "                        for metric in self.eval_metrics:\n",
        "\n",
        "                            if self.device == 'cuda':\n",
        "                                data = data.type(torch.cuda.FloatTensor)\n",
        "                            else:\n",
        "                                data = data.type(torch.FloatTensor)\n",
        "\n",
        "                            data = data.to(self.device)\n",
        "                            outputs = self.model(data)\n",
        "\n",
        "                            if(self.supervised):\n",
        "                                metric_value = metric(outputs, labels)\n",
        "                            else:\n",
        "                                metric_value = metric(outputs, data)\n",
        "\n",
        "                            metrics_dict[metric.__class__.__name__].append(metric_value.detach().item())\n",
        "\n",
        "                    for k in metrics_dict.keys():\n",
        "                        metrics_dict[k] = np.array(metrics_dict[k]).mean()\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Exception thrown in wrapper.__eval_metrics')\n",
        "            print(e)\n",
        "        finally:\n",
        "            return metrics_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict_graph(self, dataset:ImageGraphDataset, save_preds = True):\n",
        "\n",
        "        self.model.eval()\n",
        "        start = time.time()\n",
        "    \n",
        "        predictions = {}\n",
        "        metrics = np.zeros([len(dataset), 3])\n",
        "\n",
        "        if(dataset.read_label):\n",
        "\n",
        "            i = 0\n",
        "\n",
        "            for curr_id,graph,feats,labels in dataset:\n",
        "\n",
        "                graph = graph.to(self.device)\n",
        "                feats = torch.FloatTensor(feats).to(self.device)\n",
        "                labels = torch.LongTensor(labels).to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    logits = self.model(graph,feats)\n",
        "\n",
        "                #_, predicted_classes = torch.max(logits, dim=1)\n",
        "                predicted_classes = logits.argmax(1)\n",
        "                res = evaluation.calculate_node_dices(predicted_classes.detach().cpu().numpy(),\n",
        "                                                          labels.detach().cpu().numpy())\n",
        "                metrics[i] = res\n",
        "                predictions[curr_id] = predicted_classes\n",
        "                i += 1\n",
        "\n",
        "                if(save_preds):\n",
        "                    dataset.save_voxel_preds(curr_id, predicted_classes)\n",
        "\n",
        "                out = f\"Step {i}/{len(dataset)+1} Elapsed time: {time.time() - start:.0f} sec \"\n",
        "                sys.stdout.write(\"\\r\" + out)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            avg_metrics = np.mean(metrics,axis=0)\n",
        "            return predictions, avg_metrics\n",
        "            \n",
        "        else:\n",
        "            i = 0\n",
        "            for curr_id,graph,feats in dataset:\n",
        "\n",
        "                graph = graph.to(self.device)\n",
        "                feats = torch.FloatTensor(feats).to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    logits = self.model(graph,feats)\n",
        "\n",
        "                predicted_classes = logits.argmax(1).detach().cpu().numpy()\n",
        "                predictions[curr_id] = predicted_classes\n",
        "\n",
        "                if(save_preds):\n",
        "                    dataset.save_voxel_preds(curr_id, predicted_classes)\n",
        "                \n",
        "                i += 1\n",
        "                out = f\"Step {i}/{len(dataset)+1} Elapsed time: {time.time() - start:.0f} sec \"\n",
        "                sys.stdout.write(\"\\r\" + out)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            return predictions\n",
        "\n",
        "        \n",
        "\n",
        "    def predict_batch(self, data_loader:DataLoader):\n",
        "\n",
        "        output = []\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "              for i, batch in enumerate(data_loader):\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                if(len(batch) == 1):\n",
        "                  data = batch\n",
        "                else:\n",
        "                  data,label = batch\n",
        "\n",
        "                if self.device == 'cuda':\n",
        "                  data = data.type(torch.cuda.FloatTensor)\n",
        "                else:\n",
        "                  data = data.type(torch.FloatTensor)\n",
        "\n",
        "                data.to(self.device)\n",
        "\n",
        "                out = self.model(data)\n",
        "                out = out.cpu().detach().numpy()\n",
        "                output.append(out)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Exception thrown in class {self.model.__class__.__name__ }, method predict_batch:\")\n",
        "                print(e)\n",
        "                print('\\n')\n",
        "\n",
        "        return np.array(output)\n",
        "\n",
        "\n",
        "    def predict(self, data):\n",
        "\n",
        "        if device == 'cuda':\n",
        "          data = data.type(torch.cuda.FloatTensor)\n",
        "        else:\n",
        "          data = data.type(torch.FloatTensor)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            data.to(device)\n",
        "            output = self.model(data)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he-C8aZXQkry"
      },
      "source": [
        "## Loss üï≥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pUxP6_HiQiP8"
      },
      "outputs": [],
      "source": [
        "class Loss(nn.Module):\n",
        "    def __init__(self, focal):\n",
        "        super(Loss, self).__init__()\n",
        "        if focal:\n",
        "            self.loss_fn = DiceFocalLoss(\n",
        "                include_background=False, softmax=True, to_onehot_y=True, batch=True, gamma=2.0\n",
        "            )\n",
        "        else:\n",
        "            self.loss_fn = DiceCELoss(include_background=False, softmax=True, to_onehot_y=True, batch=True)\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        return self.loss_fn(y_pred, y_true)\n",
        "\n",
        "\n",
        "class LossBraTS(nn.Module):\n",
        "    def __init__(self, focal):\n",
        "        super(LossBraTS, self).__init__()\n",
        "        self.dice = DiceLoss(sigmoid=True, batch=True)\n",
        "        self.ce = FocalLoss(gamma=2.0, to_onehot_y=False) if focal else nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def _loss(self, p, y):\n",
        "\n",
        "        return self.dice(p, y) + self.ce(p, y.float())\n",
        "\n",
        "    def forward(self, p, y):\n",
        "        print('p '+str(p.size()))\n",
        "        print('y '+str(y.size()))\n",
        "        y_wt, y_tc, y_et = y > 0, ((y == 1) + (y == 3)) > 0, y == 3\n",
        "        p_wt, p_tc, p_et = p[:, 0].unsqueeze(1), p[:, 1].unsqueeze(1), p[:, 2].unsqueeze(1)\n",
        "        l_wt, l_tc, l_et = self._loss(p_wt, y_wt), self._loss(p_tc, y_tc), self._loss(p_et, y_et)\n",
        "        return l_wt + l_tc + l_et"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsRN0nOwDif5"
      },
      "source": [
        "## Build dataset üèó"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9m9XrfCJDqWy"
      },
      "outputs": [],
      "source": [
        "dotenv_path = \"/Users/Roberto/Desktop/Universit√†/Magistrale/Tesi/BraTS/.env\"\n",
        "load_dotenv(dotenv_path)\n",
        "\n",
        "\n",
        "INPUT_PATH = os.getenv(\"INPUT_DATA_DIR\")\n",
        "PROCESSED_PATH = os.getenv('PROCESSED_DATA_DIR')\n",
        "INPUT_PATH_PARENT = '/'.join(INPUT_PATH.split('/')[:-1])\n",
        "\n",
        "TRAIN_PATH = os.path.join(INPUT_PATH_PARENT,'train')\n",
        "VAL_PATH = os.path.join(INPUT_PATH_PARENT,'val')\n",
        "TEST_PATH = os.path.join(INPUT_PATH_PARENT,'test')\n",
        "\n",
        "TAR_PATH = os.path.join(INPUT_PATH_PARENT,'BraTS2021_Training_Data.tar')\n",
        "BUILD_DATASET = False\n",
        "\n",
        "if(BUILD_DATASET):\n",
        "  untar_brats(tar_path = '/content/drive/MyDrive/Lorusso/BraTS/data/raw/BraTS2021_Training_Data.tar', extract_path = INPUT_PATH )\n",
        "  #dataset = DataPreprocessor()\n",
        "  #dataset.split_dataset()\n",
        "\n",
        "#dataset = DataPreprocessor()\n",
        "#train_loader = DataLoader(dataset, sampler = SeqSampler(dataset), batch_size = 1, num_workers = 0)\n",
        "#images, labels= next(iter(train_loader))\n",
        "#plot_brain_sections([images[0], labels[0]])\n",
        "#del images, labels, train_loader, dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59tr9TkL0zF3"
      },
      "source": [
        "# Generate Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkZZCxM101ZL",
        "outputId": "1fd0fcfb-38f9-4e2e-bc1b-47332991a47e"
      },
      "outputs": [],
      "source": [
        "IMG2GRAPH = False\n",
        "if(IMG2GRAPH):\n",
        "    dataset = DataPreprocessor(INPUT_PATH = '/Users/Roberto/Desktop/Universit√†/Magistrale/Tesi/BraTS/data/raw/BraTS2023-ValidationData', label_extension = None,  mri_prefix = 'BraTS-GLI-', modality_extensions = ['-t1c.nii.gz', '-t1n.nii.gz', '-t2f.nii.gz', '-t2w.nii.gz'], force_conversion = True)\n",
        "    #dataset = DataPreprocessor(INPUT_PATH = '/Users/Roberto/Desktop/Universit√†/Magistrale/Tesi/BraTS/data/raw/sampled', force_conversion = True)\n",
        "    if(not set(dataset.get_status_ids()['Finished']).issuperset(set(dataset.all_ids))):\n",
        "        ids_to_convert = list(set(dataset.all_ids).difference(set(dataset.get_status_ids()['Finished'])))\n",
        "        print('MRIs to convert:'+ str(len(ids_to_convert)))\n",
        "        dataset.all_ids = ids_to_convert\n",
        "        print(f\"Pending conversions: {len(set(dataset.get_status_ids()['Pending']))}\")\n",
        "        dataset.run()\n",
        "        #dataset = DataPreprocessor(INPUT_PATH = '/Users/Roberto/Desktop/Universit√†/Magistrale/Tesi/BraTS/data/processed_15000_0.5_0/sampled', force_conversion = True)\n",
        "        ##dataset.split_dataset(fixed=(200,100,0))\n",
        "        #print(len(dataset.all_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz-6k-tLLDbh"
      },
      "source": [
        "# Train and predict ‚åõ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dle5EmF9LXms"
      },
      "source": [
        "## Autoencoder v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "UZ5ItOcISTzW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No MRI found at /Users/Roberto/Desktop/Universit√†/Magistrale/Tesi/BraTS/data/raw/brats\n",
            "Error: fixed ratio does not sum up to one.\n",
            "Switching to default (1001,125,125))\n",
            "No MRI found at /Users/Roberto/Desktop/Universit√†/Magistrale/Tesi/BraTS/data/raw/train\n",
            "No MRI found at /Users/Roberto/Desktop/Universit√†/Magistrale/Tesi/BraTS/data/raw/val\n",
            "No MRI found at /Users/Roberto/Desktop/Universit√†/Magistrale/Tesi/BraTS/data/raw/test\n",
            "Elapsed epochs: 3\n"
          ]
        }
      ],
      "source": [
        "TRAIN_MODEL = False\n",
        "LOAD_MODEL = True\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "time.sleep(5)\n",
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "  num_workers = 0\n",
        "  batch_size = 1\n",
        "  num_epochs = 1\n",
        "  lr = 0.005\n",
        "  supervised = False\n",
        "\n",
        "  # MONAI AUTOENCODER\n",
        "  autoencoder = AutoEncoder(\n",
        "         spatial_dims=3,\n",
        "         kernel_size = 3,\n",
        "         up_kernel_size = 3,\n",
        "         in_channels=4,\n",
        "         out_channels=4,\n",
        "         channels=(5,),\n",
        "         strides=(2,),\n",
        "         inter_channels=(8,8, 16),\n",
        "         inter_dilations=(1, 2, 4),\n",
        "         num_inter_units=2\n",
        "     )\n",
        "\n",
        "  optimizer = torch.optim.Adam(autoencoder.parameters(), lr=lr,weight_decay=1e-10)\n",
        "  loss_fn = nn.MSELoss() #SSIMLoss(spatial_dims=3)\n",
        "\n",
        "  wrapper = ModelWrapper(model = autoencoder,\n",
        "                            loss_fn = loss_fn,\n",
        "                            optimizer = optimizer,\n",
        "                            supervised = supervised,\n",
        "                            num_epochs = num_epochs,\n",
        "                            LOAD_MODEL = LOAD_MODEL\n",
        "                        )\n",
        "\n",
        "  dataset = DataPreprocessor(INPUT_PATH = INPUT_PATH)\n",
        "\n",
        "  # Split dataset if it's not\n",
        "  if(not os.path.exists(TRAIN_PATH)):\n",
        "    dataset.split_dataset()\n",
        "\n",
        "\n",
        "  train_dataset = DataPreprocessor(INPUT_PATH = TRAIN_PATH)\n",
        "  val_dataset = DataPreprocessor(INPUT_PATH = VAL_PATH)\n",
        "  test_dataset = DataPreprocessor(INPUT_PATH = TEST_PATH)\n",
        "\n",
        "  train_loader = DataLoader(dataset = train_dataset,\n",
        "                           sampler = SeqSampler(train_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "  val_loader = DataLoader(dataset = val_dataset,\n",
        "                           sampler = SeqSampler(val_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "  test_loader = DataLoader(dataset = test_dataset,\n",
        "                           sampler = SeqSampler(test_dataset),\n",
        "                           batch_size = batch_size,\n",
        "                           num_workers = num_workers)\n",
        "\n",
        "\n",
        "  print('Elapsed epochs: ' + str(wrapper.elapsed_epochs))\n",
        "\n",
        "if(TRAIN_MODEL):\n",
        "      training_loss, validation_loss = wrapper.train(train_loader = train_loader, val_loader = val_loader )\n",
        "      torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Bo0aQf08rHvn"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "  xx = range(1,wrapper.elapsed_epochs+1)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(xx, wrapper.training_loss, '-o', label = 'Train')\n",
        "  plt.plot(xx, wrapper.validation_loss,'-o', label = 'Val')\n",
        "  plt.legend(loc='lower left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DvvSdXEvIw5w"
      },
      "outputs": [],
      "source": [
        "if(False):\n",
        "\n",
        "  slice_index = 100\n",
        "  ax = 0\n",
        "\n",
        "  im_test, lab = next(iter(val_loader))\n",
        "  output = wrapper.predict(data = im_test)\n",
        "  out = output[0]\n",
        "  out_numpy = out.cpu().detach().numpy()\n",
        "  im_test_numpy = im_test.cpu().detach().numpy()\n",
        "  print(np.shape(im_test_numpy))\n",
        "\n",
        "  plot_reconstruction(im_orig = np.sum(im_test_numpy[0], axis=0), im_rec = np.sum(out_numpy, axis = 0), ax = ax, slice_index = slice_index)\n",
        "  plot_brain_sections([im_test[0], lab[0]], ax = ax, slice_index = slice_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ETwFBvf1Zjyp"
      },
      "outputs": [],
      "source": [
        "if(False):\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(ker[0], allkernels=True)\n",
        "\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(torch.sum(ker, dim=(0)), allkernels=True)\n",
        "\n",
        "  kk = torch.sum(ker, dim= 0)\n",
        "  kk = torch.sum(kk, dim = 0)\n",
        "  plt.imshow(kk[0,:,:].cpu().detach().numpy())\n",
        "\n",
        "  # Sum over the channels\n",
        "  #ker =ker.sum(axis=(0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "CScy9Z1UXCD2"
      },
      "outputs": [],
      "source": [
        "#import torch.nn.functional as F\n",
        "#im_test, lab = next(iter(test_loader))\n",
        "#ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "#print(ker.size(), im_test.size())\n",
        "##ker =ker.sum(axis=0)\n",
        "##im_test = im_test.sum(axis=0)\n",
        "#result = F.conv3d(im_test,ker)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "bottleneck = autoencoder.decode[0][0][0].weight.detach().clone()\n",
        "sum_bottleneck = bottleneck.sum(axis=(0,1)).flatten().cpu()\n",
        "A,S,Vt = np.linalg.svd(sum_bottleneck.reshape(3,3,3))\n",
        "\n",
        "quantile = get_quantiles(sum_bottleneck)[-1]\n",
        "filter_mean = sum_bottleneck.mean()\n",
        "new_features = []\n",
        "for el in S:\n",
        "    new_features.append(el[0])\n",
        "new_features.append(quantile)\n",
        "new_features.append(filter_mean.detach().clone().item())\n",
        "new_features = np.array(new_features, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNn9uFyniXlr"
      },
      "source": [
        "## GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtS7HG4ciZi5",
        "outputId": "68b0ae4c-eb97-44c9-b107-4191e4189ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 100 MRIs\n",
            "Found 60 MRIs\n",
            "Found 0 MRIs\n",
            "Elapsed epochs: 10\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "INPUT_PATH = '/Users/Roberto/Desktop/Universit√†/Magistrale/Tesi/BraTS/data/processed_15000_0.5_0'\n",
        "\n",
        "TRAIN_PATH = os.path.join(INPUT_PATH,'train')\n",
        "VAL_PATH = os.path.join(INPUT_PATH,'val')\n",
        "TEST_PATH = os.path.join(INPUT_PATH,'test') # set to 'test' in production\n",
        "\n",
        "\n",
        "train_dataset = ImageGraphDataset(TRAIN_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True, features=new_features)\n",
        "val_dataset = ImageGraphDataset(VAL_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True, features=new_features)\n",
        "test_dataset = ImageGraphDataset(TEST_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True, features=new_features)\n",
        "\n",
        "\n",
        "TRAIN_MODEL = False\n",
        "LOAD_MODEL = True # resume training\n",
        "\n",
        "supervised = True\n",
        "eval_metrics = None\n",
        "num_workers = 0\n",
        "\n",
        "batch_size = 3\n",
        "num_epochs = 5\n",
        "lr = 5e-4\n",
        "dropout = 0\n",
        "input_feats = 25\n",
        "class_weights = torch.Tensor([0.2,1,2,3]).to(device) #compute_average_weights(val_dataset) #\n",
        "layer_sizes=[256]*4\n",
        "n_classes=4\n",
        "aggregator_type='pool'\n",
        "\n",
        "#heads = [128]*3\n",
        "#residuals = [128]*3\n",
        "heads = [8, 8, 8, 8, 8, 8]\n",
        "residuals = [False, True, True, False, True, True]\n",
        "activation = F.relu\n",
        "val_dropout = 0.02\n",
        "val_feat_drop = 0.2\n",
        "val_attn_drop = 0.2\n",
        "\n",
        "\n",
        "dict_params = {k:eval(k) for k in ['dropout','input_feats', 'class_weights', 'layer_sizes', 'n_classes', 'aggregator_type', 'n_classes', 'heads', 'residuals']}\n",
        "model = GraphSage(in_feats=input_feats,\n",
        "                  layer_sizes=layer_sizes,\n",
        "                  n_classes=n_classes,\n",
        "                  aggregator_type=aggregator_type,\n",
        "                  dropout=dropout)\n",
        "\n",
        "#model = GAT(in_feats = input_feats, layer_sizes = layer_sizes, n_classes=n_classes,heads = heads, residuals = residuals, activation = activation,  feat_drop = val_feat_drop, attn_drop = val_attn_drop)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=lr,weight_decay=1e-4)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "#loss_fn = LossBraTS(focal=False)\n",
        "\n",
        "wrapper = ModelWrapper(model = model,\n",
        "                            loss_fn = loss_fn,\n",
        "                            optimizer = optimizer,\n",
        "                            supervised = supervised,\n",
        "                            dict_params = dict_params,\n",
        "                            isgnn = True,\n",
        "                            num_epochs = num_epochs,\n",
        "                            LOAD_MODEL = LOAD_MODEL,\n",
        "                            eval_metrics = eval_metrics\n",
        "                          )\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset = train_dataset,\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers,\n",
        "                              collate_fn=minibatch_graphs)\n",
        "\n",
        "val_loader = DataLoader(dataset = val_dataset,\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers,\n",
        "                              collate_fn=minibatch_graphs)\n",
        "\n",
        "\n",
        "\n",
        "print('Elapsed epochs: ' + str(wrapper.elapsed_epochs))\n",
        "\n",
        "if(TRAIN_MODEL):\n",
        "    training_loss, validation_loss = wrapper.train(train_loader = train_loader, val_loader = val_loader )\n",
        "    #torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3 MRIs\n",
            "Step 3/4 Elapsed time: 6 sec "
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'BraTS-GLI-00013-000': array([0, 0, 0, ..., 0, 0, 0]),\n",
              " 'BraTS-GLI-00001-001': array([0, 0, 0, ..., 0, 0, 0]),\n",
              " 'BraTS-GLI-00001-000': array([0, 0, 0, ..., 0, 0, 0])}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testdata2023 = ImageGraphDataset('/Users/Roberto/Desktop/UniversitaÃÄ/Magistrale/Tesi/BraTS/data/processed_15000_0.5_0/BraTS2023-Test','BraTS-GLI-',read_image=False,read_graph=True,read_label=False, features=new_features)\n",
        "wrapper.predict_graph(testdata2023)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J8e-G2cJQxr"
      },
      "outputs": [],
      "source": [
        "def rm_run(run_id, experiment = '134912204135972352',):\n",
        "    rm_path = '/content/drive/MyDrive/Lorusso/BraTS/mlruns/'+experiment+'/'+run_id\n",
        "    shutil.rmtree(rm_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1_TRMmsm6jY"
      },
      "source": [
        "# TESTING SECTION üöß"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC-ortIQ3vXX"
      },
      "outputs": [],
      "source": [
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled\n",
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/train\n",
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/test\n",
        "#! rm -r /content/drive/MyDrive/Lorusso/BraTS/data/interim/val\n",
        "\n",
        "\n",
        "\n",
        "#trainset = set()\n",
        "#valset = set()\n",
        "#testset = set()\n",
        "#for el in dp.get_status_ids()['Pending']:\n",
        "#    if(el in train_dataset.all_ids):\n",
        "#        trainset.add(el)\n",
        "#    if(el in val_dataset.all_ids):\n",
        "#        valset.add(el)\n",
        "#    if(el in test_dataset.all_ids):\n",
        "#        testset.add(el)\n",
        "#\n",
        "#\n",
        "#for el in valset:\n",
        "#    src = VAL_PATH+'/'+str(el)\n",
        "#    dst = '/content/drive/MyDrive/Lorusso/BraTS/data/processed_5000_0.5_10/brats/'+el\n",
        "#    shutil.copytree(src, dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJUctzmLazd9"
      },
      "source": [
        "## Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHHzhDDF_3G9"
      },
      "outputs": [],
      "source": [
        "\n",
        "TEST_MODE = False\n",
        "\n",
        "\n",
        "if(TEST_MODE):\n",
        "\n",
        "    INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/interim/sampled'\n",
        "    INPUT_PATH_PARENT = '/'.join(INPUT_PATH.split('/')[:-1])\n",
        "    TRAIN_PATH = os.path.join(INPUT_PATH_PARENT,'train')\n",
        "    VAL_PATH = os.path.join(INPUT_PATH_PARENT,'val')\n",
        "    TEST_PATH = os.path.join(INPUT_PATH_PARENT,'test')\n",
        "\n",
        "    TRAIN_MODEL = False\n",
        "    LOAD_MODEL = True # resume training\n",
        "\n",
        "    num_workers = 0\n",
        "    batch_size = 1\n",
        "    num_epochs = 2\n",
        "    lr = 0.01\n",
        "    supervised = False\n",
        "    eval_metrics = [nn.MSELoss()]\n",
        "\n",
        "  # MONAI AUTOENCODER\n",
        "    model = AutoEncoder(\n",
        "           spatial_dims=3,\n",
        "           kernel_size = 3,\n",
        "           up_kernel_size = 3,\n",
        "           in_channels=4,\n",
        "           out_channels=4,\n",
        "           channels=(5,),\n",
        "           strides=(2,),\n",
        "           inter_channels=(8, 16, 32),\n",
        "           inter_dilations=(1, 2, 4),\n",
        "           num_inter_units=2\n",
        "       )\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-10)\n",
        "    loss_fn = nn.MSELoss() #SSIMLoss(spatial_dims=3)\n",
        "\n",
        "    wrapper = ModelWrapper(model = model,\n",
        "                              loss_fn = loss_fn,\n",
        "                              optimizer = optimizer,\n",
        "                              supervised = supervised,\n",
        "                              num_epochs = num_epochs,\n",
        "                              LOAD_MODEL = LOAD_MODEL,\n",
        "                              eval_metrics = eval_metrics\n",
        "                          )\n",
        "\n",
        "    dataset = DataPreprocessor(INPUT_PATH = INPUT_PATH)\n",
        "\n",
        "    # Split dataset if it's not\n",
        "    if(not os.path.exists(TRAIN_PATH)):\n",
        "      dataset.split_dataset()\n",
        "\n",
        "\n",
        "    train_dataset = DataPreprocessor(INPUT_PATH = TRAIN_PATH)\n",
        "    val_dataset = DataPreprocessor(INPUT_PATH = VAL_PATH)\n",
        "    test_dataset = DataPreprocessor(INPUT_PATH = TEST_PATH)\n",
        "\n",
        "    train_loader = DataLoader(dataset = train_dataset,\n",
        "                             sampler = SeqSampler(train_dataset),\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers)\n",
        "\n",
        "    val_loader = DataLoader(dataset = val_dataset,\n",
        "                             sampler = SeqSampler(val_dataset),\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers)\n",
        "\n",
        "    test_loader = DataLoader(dataset = test_dataset,\n",
        "                             sampler = SeqSampler(test_dataset),\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers)\n",
        "\n",
        "\n",
        "    print('Elapsed epochs: ' + str(wrapper.elapsed_epochs))\n",
        "\n",
        "    if(TRAIN_MODEL):\n",
        "        training_loss, validation_loss = wrapper.train(train_loader = train_loader, val_loader = val_loader, experiment_prefix = 'Test' )\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcuxwsmW9OYS"
      },
      "outputs": [],
      "source": [
        "if(TRAIN_MODEL or LOAD_MODEL):\n",
        "  xx = range(1,wrapper.elapsed_epochs+1)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(xx, wrapper.training_loss, '-o', label = 'Train')\n",
        "  plt.plot(xx, wrapper.validation_loss,'-o', label = 'Val')\n",
        "  plt.legend(loc='lower left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3TKi4A9O4NA"
      },
      "outputs": [],
      "source": [
        "if(False):\n",
        "\n",
        "  slice_index = 90\n",
        "  ax = 0\n",
        "\n",
        "  im_test, lab = next(iter(test_loader))\n",
        "  output = wrapper.predict(data = im_test)\n",
        "  out = output[0]\n",
        "  out_numpy = out.cpu().detach().numpy()\n",
        "  im_test_numpy = im_test.cpu().detach().numpy()\n",
        "\n",
        "  plot_reconstruction(im_orig = np.sum(im_test_numpy[0], axis=0), im_rec = np.sum(out_numpy, axis = 0), ax = ax, slice_index = slice_index)\n",
        "  plot_brain_sections([im_test[0], lab[0]], ax = ax, slice_index = slice_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5fwjWaWOhXN"
      },
      "outputs": [],
      "source": [
        "if(False):\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(ker[1], allkernels=True)\n",
        "\n",
        "  ker = wrapper.model.decode[0][0][0].weight.detach().clone()\n",
        "  print(ker.size())\n",
        "  #\n",
        "  visTensor(torch.sum(ker, dim=(0)), allkernels=True)\n",
        "\n",
        "  kk = torch.sum(ker, dim= 0)\n",
        "  kk = torch.sum(kk, dim = 0)\n",
        "  plt.imshow(kk[0,:,:].cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGPCqWQcrPqb"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-lpiQp3XNxn"
      },
      "source": [
        "## GNN training ‚õΩ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVWsu1gEXV9j"
      },
      "outputs": [],
      "source": [
        "\n",
        "TEST_MODE = False\n",
        "\n",
        "\n",
        "if(TEST_MODE):\n",
        "\n",
        "    INPUT_PATH = '/content/drive/MyDrive/Lorusso/BraTS/data/processed_5000_0.5_10'\n",
        "    TRAIN_PATH = os.path.join(INPUT_PATH,'train')\n",
        "    VAL_PATH = os.path.join(INPUT_PATH,'val')\n",
        "    TEST_PATH = os.path.join(INPUT_PATH,'val') # set again to 'test' in production\n",
        "\n",
        "    TRAIN_MODEL = True\n",
        "    LOAD_MODEL = False # resume training\n",
        "\n",
        "    num_workers = 0\n",
        "    batch_size = 1\n",
        "    num_epochs = 2\n",
        "    lr = 0.005\n",
        "    supervised = True\n",
        "    eval_metrics = []\n",
        "\n",
        "    dropout = 0\n",
        "    input_feats = 20\n",
        "    class_weights = torch.Tensor([0.1,1,2,2])\n",
        "    layer_sizes=[256]*4\n",
        "    n_classes=4\n",
        "    aggregator_type='pool'\n",
        "\n",
        "    dict_params = {k:eval(k) for k in ['input_feats', 'class_weights', 'layer_sizes', 'n_classes', 'aggregator_type', 'n_classes']}\n",
        "\n",
        "    model = GraphSage(in_feats=input_feats,layer_sizes=layer_sizes,n_classes=n_classes,aggregator_type=aggregator_type,dropout=dropout)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(),lr=lr,weight_decay=1e-10)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    wrapper = ModelWrapper(model = model,\n",
        "                            loss_fn = loss_fn,\n",
        "                            optimizer = optimizer,\n",
        "                            supervised = supervised,\n",
        "                            dict_params = dict_params,\n",
        "                            isgnn = True,\n",
        "                            num_epochs = num_epochs,\n",
        "                            LOAD_MODEL = LOAD_MODEL,\n",
        "                            eval_metrics = eval_metrics\n",
        "                          )\n",
        "\n",
        "\n",
        "    train_dataset = ImageGraphDataset(VAL_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True)\n",
        "    #val_dataset = ImageGraphDataset(VAL_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True)\n",
        "    #test_dataset = ImageGraphDataset(TEST_PATH,'BraTS2021',read_image=False,read_graph=True,read_label=True)\n",
        "\n",
        "    train_loader = DataLoader(dataset = train_dataset,\n",
        "                             batch_size = batch_size,\n",
        "                             num_workers = num_workers,\n",
        "                              collate_fn=minibatch_graphs)\n",
        "\n",
        "\n",
        "    print('Elapsed epochs: ' + str(wrapper.elapsed_epochs))\n",
        "\n",
        "    if(TRAIN_MODEL):\n",
        "        training_loss, validation_loss = wrapper.train(train_loader = train_loader, val_loader = train_loader, experiment_prefix = 'Test_GNN' )\n",
        "        torch.cuda.empty_cache()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mwFJcAuef07T",
        "wE7OwGBES7sa",
        "q7ygOSsiEAWs",
        "aKEY_j_lCzCa",
        "cgINpL8BK3hL",
        "0u3q4OhjYyGq",
        "he-C8aZXQkry",
        "vsRN0nOwDif5",
        "pwDBKoXvkno1",
        "dle5EmF9LXms",
        "n1_TRMmsm6jY",
        "T-lpiQp3XNxn",
        "kVH1teeglI4t"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
